Hello, everybody. I am Evan Hubinger. I am a research fellow at MIRI. I used to work at OpenAI. I've done a bunch of stuff with Paul Cresciano. I also wrote this paper that I'm going to be talking about today. Let me very quickly raise hands if you have had any experience with the paper I'm going to be talking about. Let's see. OK. OK, pretty good. Sweet. OK. Potentially, I could give a follow-up talk that covers more advanced stuff. But OK, I think that was not quite enough hands. So I think I'm going to be doing this talk. And hopefully, it'll help people at least work through the material and understand stuff better. I think you will probably understand it better from having seen the talk, at the very least. Maybe at some other point, if I stick around, I can give another talk. OK. So yes, let's get started. So let's see. About me, this talk was made when I was at OpenAI. I'm currently at MIRI. I did other stuff before that. I've given this talk at a bunch of places. It's based on this paper. You know it, I think, some of you. OK, what are we talking about? OK, so I want to start with just going back and trying to understand what machine learning does and how it works, because it's very important to understand how machine learning works if we're going to try to talk about it. So what does machine learning do? So fundamentally, any machine learning training process, it has some model space, some large space of algorithms, and then does some really big search over that algorithm space to find some algorithm which performs well empirically on some set of data, on some loss function over that data. OK, this is what essentially everything looks like. This is what RL looks like. This is what supervised learning looks like. This is what fine tuning looks like. Everything looks essentially like we have some really big parameterized space. We do a big search over it. We find an algorithm which does well on some loss over some data. OK, so I think that when a lot of people like to think about this process, there is an abstraction that is really common in thinking about it, and an abstraction that I think can be very useful. And I call that abstraction the does the right thing abstraction. So well, when we trained this model, when we produced this particular parameterization, this particular algorithm over this particular data, well, we selected that algorithm to do a good job on this loss on that data. And so we sort of want to conceptually think about the algorithm as trying to minimize the loss. You know, if you're like, how is this algorithm going to generalize? You can sort of think, well, you know, what would it do that would be loss minimizing? What would be the sort of loss minimizing behavior? But of course, this is not literally true. We did not literally produce an algorithm that is, in fact, minimizing the loss on all off-distribution points. What we have produced is we produce an algorithm that empirically was observed to minimize the loss on the training data. But in fact, when we move it into new situations, it could do anything. But well, we selected it to minimize the loss, and so we'd like to think probably it's going to do something like the loss minimizing behavior. OK. So this abstraction, like all abstractions that are not trivial, are leaky. And we are going to be talking about ways in which this abstraction is leaky. OK. So I'm going to be talking about a very specific situation where I think this abstraction can be leaky in a way that I think is problematic. So that situation is a situation where you have a algorithm itself is doing some sort of optimization. So what do I mean by that? So we're going to say that a system is an optimizer if it is internally searching through some space of possible plans, strategies, actions, whatever, for those that score highly on some criteria. So maybe it's looking for actions that would get low loss. Maybe it is looking for actions that would get it gold coins. Maybe it is looking for actions that would do a good job of predicting the future, whatever. It's looking for things that do a good job on some criteria. OK. So just really quickly, gradient descent is an optimizer because it looks through possible algorithms to find those that do a good job empirically on the loss. A minimax algorithm is an optimizer. It looks for moves that do a good job at playing the game. Humans are optimizers. We look for plans, strategies that accomplish our goals. Things that are not optimizers, a bottle cap is not an optimizer. This is a classic example. You can take a bottle of water, and it's got the water in the bottle. And it's really good at keeping the water in the bottle. You can turn the bottle over, and the water stays in. But that's not because the bottle cap is doing some sort of optimization procedure. It's because we did an optimization procedure for deduce a bottle, and that bottle is then really good. In the same way that a gradient descent process, by default, does an optimization procedure over the space of algorithms and produces a neural network that is not necessarily itself doing optimization. It may just be like the bottle cap. It was a thing that was optimized to do something, but it's not necessarily doing any optimization itself. Certainly, if I just randomly initialize a neural network, it is definitely not going to be doing any optimization unless I get really, really unlucky or lucky. OK. All right. So that is what an optimizer is. So what we want to talk about, and I alluded to this previously, is a situation where the model itself, the algorithm that you found when you did this big search, is itself an optimizer. It is doing some optimization internally inside of your neural network. So I'm going to give names to things in that particular situation where the thing is an optimizer. In that situation, we're going to call the gradient descent process that did a bunch of optimization on top to produce the model. We're going to call it a base optimizer. And we're going to call the thing that it optimized, the algorithm that it found, the gradient descent found, that was, in fact, doing some optimization, we're going to call it a Mesa optimizer. What does that mean? So Mesa is sort of the opposite of meta. Oftentimes, you may be familiar with meta learning. We have an optimization process, and then we put a little meta learning optimization process on top of it that searches over possible ways the learning process could go. And we're like, well, that's sort of what happened here. You had this gradient descent process, and your gradient descent process turned into a meta learning process. Now, instead of searching over a whole space of algorithms, it's sort of specifically searching over learning algorithms. And so we're going to say, well, essentially, the relationship to the base optimizer to the model is very similar to the relationship between a meta optimizer and sort of the thing that meta optimizer is optimizing. So we're going to say, well, it's one meta level below. It's a Mesa optimizer. OK. All right. So some things that people often misunderstand here. So when I say Mesa optimizer, I don't mean some subsystem or some component of the model. I just mean a model. It's a neural network, and it's doing some stuff. And in particular, we're referring to a situation where it is doing some sort of search. It's searching over some space of possible plans, strategies, whatever, for something which does a good job on some criteria. So it's the whole trained model. OK. So I talked a little bit about the relationship with meta learning. But essentially, you can think about this as spontaneous meta learning. You didn't think you were going to be doing meta learning, that you were doing optimization over learning processes. But in fact, the algorithm that your gradient descent process found that was doing a good job on the task was itself a learning algorithm. And so now you're in the business of doing meta learning, because that's what algorithm you found. OK, and so the difficulty is, in this situation, controlling what we're going to meta learn. OK. So right. So we have this does the right thing abstraction, that we said, well, it's leaky sometimes, but it at least helps us reason about models in other times. So we can ask, well, what does the right thing abstraction in the situation where you have a model that is itself doing optimization in the situation where you have a MACE optimizer? Well, so in that situation, it should say, if the model is doing the right thing, which is it's actually out of distribution going to try to minimize the loss, then it should be whatever the thing that this MACE optimizer is searching for, whatever optimization it's doing, that optimization should be directed towards the goal of minimizing the loss. OK, so this is one abstraction that you could use to try to think about this process. But of course, as we stated, this is not the actual process that we are doing. So we don't know whether this abstraction actually holds. And so what we want to know is what happens when this abstraction is leaky. OK. So when it's leaky, we're sort of going to have two alignment problems. We're going to have an outer alignment problem, which is like make it so that the base objective, whatever the loss function is, that's the sort of thing that we would want. And then an inner alignment problem, which is make it so that the MACE objective, the thing that the thing is actually pursuing, is, in fact, the thing that we wanted it to pursue. One caveat that I will say here. So I think that this sort of traditional setup really mostly applies in situations where the sort of goal that you have for how you're going to align the system is we're going to first specify some loss function. And then we're going to make the does the right thing abstraction hold. We're going to specify some loss function that we like. And then we're going to get the model to optimize it. I will say that's not the only way that you could produce an aligned system. So an alternative procedure would be specify some loss function that you know is bad. If the thing actually optimized for that loss function, you know that it would end up producing bad outcomes. And then also get it to not optimize for that. Get it to do some totally different thing that nevertheless results in good behavior. And in fact, I think that's actually a pretty viable strategy and one that we may want to pursue. But for the purposes of this talk, I'm going to be assuming that's not the strategy that we're going with. But the strategy we're going with is we're going to write down something that specifies some loss or reward that actually reflects what we care about. And then we're going to try to get a model to actually care about that thing too. But I just want to preface with that's only one strategy and not even necessarily the best one. But it is one that I think is at least easy to conceptualize and that we can understand what's going on if that's a strategy we're pursuing. Question? It's like bad objective is something like avoiding deception. Or the other example you're talking about, like train it to not do a bad thing, a thing that we don't want it to do and then train it to not do that. Yeah, so here's a really simple example. Here's a really simple example. This is not something I think we should literally do. But here's a simple example that might illustrate the point. So let's say what we do is we have an environment. And we're going to do RL in that environment. And we're going to set up the environment to have incentives and structures that are similar to the human ancestral environment. And then we're like, well, if it's similar to the human ancestral environment, maybe it'll produce agents that operate similarly to humans. And so they'll be aligned. That's a hope that you could have. And if that's your strategy, then the thing that you're doing is you are saying, I'm going to specify some incentives that I know are not aligned, like reproduction, natural selection. That's not what I want. That's not what I care about. But I think that those incentives will nevertheless produce agents which operate in a way that is the thing that I want. So you could have that as your strategy. We're not talking about that sort of a strategy, though, right now in this talk. But I don't want to rule it out. I don't want to say that you could never do that. In fact, I think a lot of my favorite strategies for alignment do look like that, though they don't look like the one I just described. But they do have the general structure of we don't necessarily write down a loss function that literally captures what we want. But supposing we do, we're going to try to explore what this looks like. And in fact, as we'll see, basically all of this is going to apply to that situation, too. Because the problem that we're going to end up talking about is just this central problem of what happens when you have some loss, and then you want to understand what thing will your model, if it is itself an optimizer, end up optimizing for. OK, is that clear? Does that make sense? OK, great. OK, so you've probably heard stories of outer alignment failures, what they might look like. Classical examples are things like paperclip maximizers. You're like, I want to make the most paperclips in my paperclip factory. And then it just destroys the world, because that's the best way to make paperclips. That is a classic example of an outer alignment failure. What I would like to provide is more classic examples of inner alignment failures, so we can start to get a sense of what it might look like if inner alignment, in the sense that we talked about previously, where the model ends up optimizing for a thing that is different than the thing that you wanted to, that you specified, might look like. So what does that look like? Well, so when inner alignment fails, it looks like a situation that we're going to call capability generalization without objective generalization. So what does that mean? So let's say I have a training environment, and it looks like this brown maze on the left. And I've got some arrows that mark what's going on. And the loss function, the reward function that I use, we're going to do RL in this environment. I'm going to say, reward the agent for finishing the maze. I want it to get to the end. And then I'm going to deploy to this other environment, where now I've got a bigger maze. It's blue instead of brown. And I have this green arrow at this random position. And now I want to know what happens. So here are some things that could happen in this new environment. And we ask, what is the generalization behavior of this agent? So here's one thing that could happen. It could look at this massive blue maze and just not have any idea how to do anything. So it just randomly goes off and does nothing useful. That would be a situation where we're going to say, its capabilities did not generalize. It did not learn a general-purpose-enough algorithm for solving mazes that that algorithm was able to generalize this new environment. OK, but it might. So let's say its capabilities did generalize. It actually did learn some sort of general-purpose maze solving algorithm. It knows how to operate in this environment. Well, there's two things that it could potentially learn, at least two. It could learn to use those maze-solving capabilities to get to the end of the maze, which is the thing that we were rewarding it for previously. Or it could use the maze-solving capabilities to get to the green arrow. And if the strategy it was pursuing, if the generalization it learned was always go to the green arrow versus always go to the end of the maze, in training, those were indistinguishable. But now in this deployment environment, they're quite distinct. And so let's say what we really wanted. We really wanted it to go to the end. But this green arrow just got moved. And so we're like, OK, so what happens here? Why is this bad that we could end up in a situation where the model generalizes in such a way that it goes to the green arrow instead? Well, what's happened is we are now in a situation where your model has learned a very powerful general-purpose capability, which is the capability to solve mazes. And that capability is misdirected. You have deployed a model that is in a situation where it is very powerful and is actively using those capabilities not to do the thing that you wanted it to do. So that's bad. That's dangerous. We do not want to be in situations where we are deploying powerful, capable models in situations where they are using their capabilities where they don't actually generalize and they don't actually match up with what we wanted them to use those capabilities for. OK, so that's concerning. And that's the thing that we want to avoid. That's sort of what it looks like when we have this sort of inner alignment failure. OK, so some more words. We're going to say that the situation where the model generalizes correctly on the sort of reward loss objective that we want, that it actually is trying to finish the maze even out of distribution, we're going to say it's robustly aligned. It's a situation where its alignment, it's actually going to pursue the correct thing, is robust to out-of-distributional shifts, whatever. And we're going to say that if the model sort of looks aligned over the training data, but actually we can find situations where the model would not be aligned, then we're going to say it's pseudo-aligned. OK, great. All right, yes. Robust over the whole data means the test data? The test data. We're going to say, well, any data that it might, in fact, encounter in the real world. If the thing is actually robust in all situations, it always pursues the right thing, then we're going to say it's robustly aligned. Importantly, we're only talking about the alignment here. What we don't care about is if the model, like in some situation, its capabilities don't generalize. If it ends up in a situation where it's just like, I don't know what's going on, and then it doesn't do anything, we're fine with that. That's still robust alignment, potentially. We're sort of factoring the capabilities in the alignment here. We're saying that if it is capable of pursuing some complex thing, it is always pursuing the thing that we wanted it to pursue. And here we're saying, it only looks like it's pursuing the right thing on training, but out of distribution, it might end up pursuing the wrong thing capably. Just to clarify, robust alignment is defined with respect to a particular data distribution. You can't speak about it in isolation. No. Pseudo-alignment is defined specifically relative to a data distribution, because it's defined relative to the training data distribution. But robust alignment is not. Robust alignment is defined relative to any data it might ever encounter. OK. All right. Thank you. OK. Great. All right. So we have two problems. Yes? If capabilities don't generalize, are you doomed to have a pseudo-alignment? Because in cases where it doesn't know what to do, it will do things that are unintended. I'm OK if it does things that are unintended. We're worried if the optimization is directed in an unintended place, that it does powerful optimization towards the wrong goal. So if it just, in the new maze, it just falls over and does random stuff, we don't care. We're like, OK, whatever. The problem is a situation where it does know how to solve complex mazes. It actually does have the capability. And then it actively optimizes for something we don't want it to be optimizing for. OK. So even if we had the maze solver and it was robustly aligned, we would call it robustly aligned even if we gave it a massive maze and then it tried to take over on bits of computational power to solve this maze because it was still actually applying. Right. So in this situation, just going back to here, we're assuming that the loss function actually reflects what we care about, right? And we're like, what would it look like if the loss function does reflect what we care about to have an inter-alignment failure? So in that situation, you would be like, well, your strategy of write down a loss function that reflects everything I care about and then get all this optimized for it, you failed at the point where the loss function you wrote down that you thought reflected everything you care about just said optimize for mazes. That was bad. You shouldn't have done that. That was actually not everything you cared about. And so we're like, that was where you went wrong here. I think that, like I said, though, you might not have this strategy. But right now, at least, we're assuming that is your strategy. And we're going to say, actually, when you wrote down this loss function, you were at least trying to get it to capture everything that you wanted. OK. OK. Yes? Do you draw any distinction between a model specifically optimizing for some internal goal versus just statistically preferring something? Yes. Yes, we absolutely are drawing a distinction there. We are saying that the thing is an optimizer if it is, in fact, has some search process inside of it that is searching for things that do well according to some objective. This is the difference between the bottle cap, which, in fact, systematically prefers the water to be in a bottle, but isn't, in fact, doing optimization for it. And the reason that we're making this distinction is because we're going to say, well, we're actually a lot more concerned about situations where the model is actively doing optimization that is misdirected than situations where it has a bunch of heuristics that were selected in a problematic way. And the reason we're more concerned about that is because misdirected optimization has the ability to be a lot worse out of distribution, because it can competently optimize for things that we don't want it to optimize for, even in situations where we didn't select it to be able to do that. Sorry to follow up. Do you have an idea in what situations you're more likely to see a model actually be an optimizer? That is a great question, and that is what we're going to be talking about next. OK, so yeah, we're going to be talking about two problems here. One is unintended optimization, which is maybe you didn't want it to be an optimizer. Maybe you just wanted it to be a bottle cap-like. You wanted it to be a selection of heuristics. You didn't want to be doing any optimization. Why might you get optimization anyway? And then the second question is this inter-alignment question, which is, OK, if it is an optimizer, how do you get it to optimize for what you want? OK, let's talk about it. OK, what sort of machine learning systems are more or less likely to find optimizers? That's the question we're starting with. OK, so here's the first thing, is, look, search algorithms are really good at generalizing. So let's say we're in a situation where we want our algorithm to play Go. We want to do a really good job at Go playing. Well, there is a systematic similarity between all good Go moves. The systematic similarity between all good Go moves is that they are moves which have the property that, if I do some look ahead and try to see how good is this move, they end up being really good. And so if I want to encode an algorithm which systematically produces good Go moves, encoding search is a really simple way to do that. And it's a way that generalizes in a lot of situations. It's a way that if I go into some new situation where I don't necessarily have a bunch of heuristics that are well-optimized for that situation, I nevertheless have the ability to figure out what's going to do well. The Go engine, which has a bunch of ability to do search, can find an absolutely crazy Go board that it's never seen before and still do 10 steps of look ahead and come out with some ideas of what things are going to be good in that situation. Obviously, in fact, we explicitly program search in when we make Go bots. AlphaGo actually has this MCTS process. So there are situations where if you have a really diverse environment, you have a lot of situations you need to be able to handle. Being able to, in each situation, be able to search for what the correct thing to do in that situation is gives the ability to generalize better. And then similarly, also, it's a form of compression. So I said that, well, what is the regularity between all good Go moves? Well, the regularity between them is when I do look ahead, they end up looking good. And so if you were asking, well, what is the easiest way to compress the set of all good Go moves? Well, the way to compress the set of all good Go moves is an optimizer that searches and does look ahead to find good Go moves. And so if we expect that your model is going to be biased towards simplicity, which I think we should, if you expect that, then you should expect that it's going to find these sorts of compressions, ways of taking your data and compressing it into a simple form that can still be able to recapitulate it. And in the situation where the thing that you're doing is some situation where you're trying to train it to solve some complex task, a good way to compress a lot of complex tasks is to be able to do search. Why is there simplicity bias? There's a bunch of stuff on this. I think it's a little bit tricky. Maybe a really sort of simple version of this, this is not something I have up here, but maybe probably the best result that I think argues for simplicity is probably the Mingard result, which shows that if you do sampling with replacement from the Gaussian initialization prior, so this is a little bit of a weird thing to do because it's extremely uncompetitive. It would take years and years if you wanted to literally just initialize a neural network over and over again until you got a neural network which actually had good performance. But you could suppose you did this thing. If you're just like, you keep initializing it every single time, you never train it, and you hope that eventually the initialization actually just does well on your data. If you did this, in fact, the prior ends up being extremely similar to the prior you get from gradient descent, which is showing that actually what gradient descent is doing, it is doing something like finding the minimal norm in Gaussian norm space for solution that is able to solve the problem. That sort of maps onto simplicity in various ways. There's a lot more to say here. Yes? You said prior twice. The prior, when drawing this, is the same as the posterior. After gradient descent. OK. I mean, it is also posterior in this case because it is a posterior is like the prior is Gaussian initialization. And the posterior is take the prior update on, in fact, when I initialize and sample from the initialization, I get a model which has good performance on my data. And then I'm like, that posterior is extremely similar to the posterior of take a model, actually train it, and then see what I get. OK, there's a lot more to say here, but I'm not going to say too much more. You can ask me more later if you want to know more about inductive biases. But whatever. The point is there's a lot of evidence, I think, that is biased towards simplicity. OK, but this is not the only situation where we might want, where we sort of might expect optimization. Another situation is, and this is pretty common, there's a lot of situations where we like to train machine learning models to mimic humans. OK, what is a property of humans that we established previously? Well, humans are optimizers. There's lots of situations where if you're going to do a good job at mimicking humans, you better be able to do a good job at predicting what we're going to do when we have a goal and we're trying to optimize for that goal. Being able to understand, if a human was trying to do x, how would they accomplish x? Being able to sort of work backwards and be like, OK, what would be necessary for the human to be able to accomplish their goal, and how would they achieve that, is something that's pretty important if you want to be able to predict human behavior. And so for predicting humans, we should also expect that our models are at least going to develop the capability to do optimization. OK. So some things here that are going to push you in various different directions. Do you have a question? Oh, no. Sorry. Oh, OK. So some things that are going to push you towards MACE optimization here. Well, so large model capacity, just having the ability to implement complex optimization algorithms, simplicity bias, either explicitly because you have some regularization, implicitly just because gradient descent is regularized in this way, as I was talking about previously. Other things, I didn't talk about this, but statefulness, having the ability to store state is the thing that really improves your ability to do optimization over time, because you can store the results of optimization and then continue iterating. So iterative, stateful procedures are a lot easier to produce optimization in. Things that might disincentivize optimization, if you just give the thing optimization. So I think if you have MCTS as part of your architecture, you're a little bit less likely to just learn optimization in the non-MCTS parts, because it's already doing some optimization. So it has less need to implement it. That's not 100% true, especially because, in fact, when we do MCTS, we usually distill the MCTS into the policy network. And so you're actively training it to mimic a search process. But at least the actual thing of giving it access to optimization seems like it should reduce the probability that it develops itself. But it still could, maybe just because the optimization that it needs is different than the optimization you gave it. Time complexity bias, I didn't mention this. But you could be in a situation where, if you really incentivize the thing to perform its computation very quickly, it's a lot harder to do optimization. Optimization, while it's a very simple algorithm, is one that can take many steps of look ahead to actually produce some results. And so if we bias the thing on the amount of time it takes and its speed, that disincentivizes optimization. And also, if we just give it really simple tasks. I think an example of this might be, we just give it a task that is literally just pre-training on web text. We just give it a task that is, just predict the next token. Really simple tasks that aren't very complex, that don't have a bunch of moving pieces, in terms of what we're trying to train the thing to do. Then it potentially needs less optimization. And probably still, at least if you're trying to predict humans, like I said, it probably still needs to have the capability to do optimization. But it doesn't necessarily need to direct that optimization towards some objective that it's optimizing for. So these are some things that I think might push you in one direction or the other. I think it's a little bit tricky. There's a lot of things pushing you in different directions here. I think my guess is that at some point, just for capabilities reasons, people are going to want to build agents. And they're going to want to build agents that have the ability to competently take actions and do things in the world. And I think when they do that, the sorts of procedures that we are building for doing these sorts of things are moving in the direction of the things that are incentivizing optimization. So why is that? Well, we're moving towards doing more imitation of humans. We're moving towards bigger models. We're moving towards more simplicity bias. This is actually a little bit tricky. I didn't mention this previously. But larger models have a stronger simplicity bias than smaller models. That might be a little bit counterintuitive. But it is true. One way to see this is the double descent curve. Maybe I'll just go back really quickly. If I have the size of my model and train the model over time, you're probably familiar with the standard thing that you get out of learning theory, which is your train loss goes down. And then your test loss, first you underfit. And then you go up. And then you overfit. But in fact, if you actually increase the size of the model far past the point at which you normally would in standard learning theory, your test loss actually goes down again. And it goes lower than you can get at any point in the classical regime. What's happening here is that the only way that this works is if the implicit biases that you get by getting a larger model size are actually selecting amongst the larger space of models for models that actually generalize better. And the models that generalize better are the simpler models. And so as we increase the size of the model, we're actually increasing the amount of simplicity bias that we're applying to get an even simpler and simpler and simpler solution, which is able to fit the problem. OK. Yes? Is that the same thing as saying the larger models have the freedom in their weights to explore these simpler models and then end up finding them? Whereas in a smaller model, they're too compressed to be able to explore that? Yes. So it's a little bit counterintuitive. But in fact, the larger models have the ability to implement simpler models. So if you think about it like this, there are algorithms which are very simple. I can write search and two lines of Python or whatever. But in fact, influencing in a small network might be really difficult. And so as the network gets larger, there's still a bias towards simple algorithms. But it gets the ability to implement more of the possible simple algorithms that are available. But then still, it selects a simple one. OK. There's a lot more to be said here. Yes? So I take it, is there a version of this plot where instead of just complexity of h, it somehow incorporates whatever that bias is you talked about? And then? This is traditionally model size. So just number of parameters. I guess the point is. But you can actually do this with a bunch. You see double descent graphs with all sorts of things on the x-axis. But traditionally, at least, it's with model size. So I'm trying to wrap my head around what's unintuitive about the right side and why it shouldn't actually be unintuitive, I guess. And it seems like the x-axis is just the complexity of our model class. So it's not accounting for how our training procedure interacts with that model class. Is that right? No, no, no. This is after training. So this is literally just, sorry, just cover this up and replace with number of parameters. Right, but that's my point. The number of. And then this is after training. We're keeping training constant. We train to convergence at every point on this graph. Right, but the point is, the x-axis only. I'm maybe not making a very interesting point. But I'm just trying to understand better. So the x-axis here only measures the complexity of our function class. It does not account for how that interacts with our training procedure. It doesn't. Yeah, it doesn't account for the biases. So we have some. This is just the size of the model class. But then on top of that model class, we actually have some prior that selects out the model that we actually like out of that model class. And we're saying that prior, the prior of gradient descent, of machine learning, that selects out a simple. It actually selects out the simple algorithms out of that large model class. So as we increase the model class, we get simpler and simpler algorithms out of it. And so if we made a new plot where the x-axis encapsulated both the complexity of the class and how our training procedure interacts with it, we would not get. Yes, so in fact, one thing that I will say here is that what's going on is that there's a disconnect between the order in which new models get added to the model class and the criteria that gradient descent uses to select from amongst those models. It is not the case that what this tells us is it is not the case that new models get added to the model class of what algorithms are available to be learned in the same order in which gradient descent would prefer them. It doesn't just start with the most preferred algorithm, then the second most preferred algorithm. It's just randomly chucking them in there. And then gradient descent is doing some very special thing to actually pick out the best one from amongst them. If it were the case that the order in which algorithms were added to the support of your prior were actually in simplicity order, then I think you would not see this. Does that mean are you, sorry, go for it. Oh, are you basically arguing in favor of the lottery ticket hypothesis here that, or no, not here? No, and in fact, I think lottery ticket hypothesis, people get very confused about. I think it doesn't say people think it says. I think it's not super relevant here. We could talk about it later, maybe. Yes. You noted that in a neural network, encoding search requires a large enough model. You can't do it in a two-layer network. It's impossible. Seems pretty hard to do in a two-layer network, yeah. So even though we regard search to be simple, should we be conditioning our simplicity based on the architecture of the model? Well, the sort of simplicity that I care about is the simplicity of, if I have a really, really large model and the gradient descent can select a huge amount of possible algorithms from amongst that set, what sort of ones does it select for, right? I'm not as much interested in the sort of simplicity that is like, if I have a tiny model, what are the algorithms that I can implement, right? And so when I say simplicity, the place where I think simplicity bias occurs in machine learning is not the like, oh, if I have a really small model, I just can't implement some things. The place where I think the simplicity bias occurs is if I have a really large model and I can implement a ton of things, which ones does gradient descent actually end up selecting? That's what I think is simplicity bias, not the other one. Couldn't, like, sorry. And in fact, I think that the architectural bias you get from having a small model is more like speed bias. It actually probably disincentivizes optimization, like we're saying, because it just doesn't have enough serial computation time to do it. Couldn't it be, though, that given that it's just quite impossible to do it in a small model, there is less of a simplicity bias and more just, once you reach the required size, then that's an available method to implement? And then at that point, it just varies a tiny bit based on simplicity and generalization? I mean, I think that the claim I'm making is that actually some models are really vastly simpler, but they only become accessible to be implemented by gradient descent once you have a larger model. But we can talk about this later. OK, let's keep going. OK, great. OK, so that's part one, which is like, OK, here are some things that would push you in directions for or against learning optimization. I think my take is it's going to happen. We're going to at least be able to have models which are capable of doing optimization. It seems like all of the ways in which we want to build powerful models and have models that can generalize in new situations, they're going to be able to do optimization. OK, so if that's going to happen, will it be doing the right thing? OK, so first, what does it look like for a model to have an objective for a MACE optimizer, to have a MACE objective that is misaligned with not the same as the loss function, right? Doesn't obey this, does the right thing abstraction. OK, so fundamentally, I think the most basic case, the one that we're primarily going to be talking about is a proxy pseudoalignment, which is a situation where there is some correlation. There's some correlational structure between a proxy that the model is trying to optimize for and the actual objective that we want it to optimize for. In a causal graph language, you can think about them as having some common ancestor. If I am trying to optimize for this and there's some common ancestor between the base objective and me, well, then I need to make this x large because that is how I make my thing large. And that will, as a side effect, make this thing large as well. And so if we're in a situation like this, then we're going to be in a situation where it could be the case that anything which has this correlational structure to the thing we care about could be the thing that your model learns. So for example, we had that situation where it learned to go to the green arrow rather than the end of the maze, because in training, those two things were highly correlated. There are other things as well. I'll just mention really briefly. You could also have something like suboptimality pseudoalignment, where the reason the thing looks aligned is because it's actually doing optimization in some really bad way. And so in fact, its objective is just some crazy thing. An example I like of this is you're trying to make a cleaning robot. And the cleaning robot believes it has an objective of minimizing the amount of atoms in existence. And it mistakenly believes that when it vacuums up the dust, the dust is just annihilated. And so it's like, oh, this is great. But then later, it discovers that, in fact, the dust does not get annihilated. It just goes into the vacuum. And it stops being aligned. So that would be a situation where it actually had some insane objective. But that objective, in fact, exhibited aligned behavior in training because it also had some insane beliefs. So you can also get some weird cancellations and stuff like this. But we're mostly not going to be talking about this. We're mostly going to be focusing on the proxy case. It was just like, well, you know, it kind of understands what's going on. And it just cares about something in the environment, which is correlated, but not the same as the thing we want. OK, so that's sort of what pseudoalignment looks like. Why would you get it? All right, so the most fundamental thing is just unidentifiability. Look, any complex environment, it just has a shitload of proxies. There's just a ton of things in that environment which one could pay attention to and which would be correlated with the thing you care about. And so as you train agents in very complex environments, you're just going to get a bunch of possible things that it could learn. And so by default, if it just learns anything which is correlated, it'd be really unlikely and weird if it learned exactly the thing that you wanted. Because all of these things, to the extent that they're correlated with the thing you're trying to train it on, will still have good training performance. OK, so just a priori, probably some of these proxies are likely to be simpler and easier to find than the one that you actually want, unless you have some really good reason to believe that the one you actually want is very simple and easy to find. OK, in addition, I think the situation is worse than that because actually some proxies can be a lot faster than others. So what do I mean by that? So we have an example here. So we're like, look, a classic example that people like to talk about in terms of this sort of thing is evolution. Evolution also was sort of like an optimization process, selecting over humans, and selecting the humans to do a good job on natural selection, pass on your DNA into the next generation. OK, so let's suppose in an alternate reality that natural selection, evolution, actually produced humans that really just care about how much their alleles are represented in future generations. That's the only thing that the humans care about. OK, but here's the problem with this, is that this is just a terrible optimization task. You have a baby, and the baby stubs their toe. And they're like, fuck, what do I do about this? How is this going to influence my chances of in the future being able to mate and then pass my DNA down? And hmm, what is this? But no, that's terrible. Instead, a much simpler and faster to compute, easier strategy for the baby to implement is just have a pain mechanism that is like, OK, we've developed this heuristic that pain is always going to be bad. And so just shunt that to the negative reward system. And so we're like, OK. So we actually, if we're in a situation where we're training on something which is relatively complex and difficult to specify in terms of the agent's input, something like DNA, we're trying to train on, pass your DNA onto the next generation, it's not going to learn to care about passing the DNA onto the next generation. It's going to learn to care about reducing pain, because reducing pain is something that is much more easily accessible to the baby. And it's something which the baby can actually implement effectively and doesn't rely on it doing some really complex, difficult optimization process to be able to determine how to optimize for that thing. So we shouldn't expect to get some really complex, difficult to optimize for thing. We should expect to get these sorts of faster, easier to optimize for proxies. And also, some proxies are simpler. So I mentioned this. Pain also has this property that it's extremely accessible in terms of the input data of the baby. So the baby can just detect, oh, man, something has happened to my leg. That's bad. And it doesn't have to do some complex thing that's like, OK, I can infer that probably there is DNA inside of my cells. But I'm not 100% sure. I should probably go get a microscope, and then I can figure it out. It doesn't have to do that. It just is extremely accessible, the thing that it cares about in terms of its input data and the data that's available to it. And so we should expect that, again, the sort of proxies you're going to be learning are the sorts of proxies that are accessible and simple in terms of the data that the model has access to. Yes? Yeah, I was going to ask, is it somewhat of a disanalogy? Because when you're considering the case of an artificial neural network, the amount of efforts required is just one forward pass for a simple proxy or a complicated thought process that leads to an output? So yeah, so the way I think you should think about this is that, well, technically, the amount of computation it performs in any forward pass is fixed. But it has to ration that computation, right? It's not the case that it gets infinite computation. In fact, it gets, like you said, a finite amount of computation. And so it needs to ration that computation to do the things that are most useful for it, right? And so if it's wasting a bunch of the computation, doing all of this reasoning about how to get the most DNA, it's not going to have room to, for example, just do the look-ahead five steps more, right? It's not going to have the capacity left to do other things which will help it with performance more. And so because it is computation limited, we should expect that that computation will be rationed in ways that result in simple, fast proxies. OK, great. So all right, so it seems like we're probably going to get proxies that are simpler and faster than other things that we might want. OK, so what are the things, again, push you in different directions here, right? So we have time complexity bias. If you're trying to limit the amount of computation, you're going to be biased towards fast proxies. If you try to have a simplicity bias, you're going to be biased towards having simple proxies that are easy to specify. If you have a really complex environment that just has a lot of proxies and a lot of possible things the thing could learn, then you're going to get more likely that you're going to learn the wrong thing. And if the thing you're trying to get it to learn is really complex, if the objective that you're trying to optimize, that you want it to optimize for is really complex, then it's going to be harder for it to find that one. Things that might help here, so there's things that we can do. We can try to do stuff like adversarial training. We sort of find situations where the model's proxy might come apart from the thing we actually want to do, situations where maybe we actually train on the big blue maze that has the green arrow in the end of the maze in different spots. We can try to do transparency, look inside the model, see what proxies it's learned. We could try to give it optimization explicitly and maybe control that optimization more. So there are things that we can do. OK, so what I want to talk about now at this juncture is adversarial training specifically. And I want to sort of imagine, I think that in some sense, adversarial training is sort of like the default way to solve this problem. You're like, well, OK, look, this maze example, Evan, that you gave me at the beginning. Well, something was obviously wrong here, which was like, you train on the big maze, right? We had access to it. We could have trained on it. Why don't we just do that? I think that's a good response. I think the question that I want to ask next is, what happens if we do this? Because in some sense, we will probably. It's not that hard. We can just generate a bunch of adversarial examples. What happens if we generate a ton of adversarial examples and we try to use those to get to a situation where actually we force it to learn to care about the correct thing that we really wanted to care about? What happens? OK, and unfortunately, I think the situation is not great, actually. So what happens? So OK, now we're going to talk about deceptive alignment. OK, so first, I want to take a digression. This is not directly related to the thing I just said. I just want to establish it as a background fact. So let's suppose that you're in a situation. You know, you're this robot. You're in this situation where there's this training phase, and then there's this deployment phase. And the deployment is a little bit longer. You know, there's more things happening in deployment. And you have some proxy that you care about. You know, you care about the proxy B. And you know, though, that the training process, the loss function, the thing it's trying to get you to do is to care about A. You've discovered this fact about the training process, let's suppose. And you're this robot. What is the optimal strategy for the robot to get the most proxy across time that it can possibly get? You know, we're going to suppose, just in this hypothetical, that it wants to get as much B as it can over all, you know, circumstances that it's in. And we're like, OK, what is the strategy that the robot could implement that would allow it to get the most proxy as possible? Well, the strategy in this situation, you know, it's a naive strategy. Let's say it just goes to B. You know, it likes B. That's what it wants. That's the proxy that it cares about. So it just goes to B whenever it sees a B. Well, the problem with this is that it goes to B. And then the training process is like, hey, that's not. That was bad. I now have an adversarial example. Like, I have an actual data point where you did the wrong thing. And I can back propagate through the, you know, example. And like, you know, now change you to not do that anymore. Because that was bad. And you got low reward for doing it. And so now I've produced a new model that doesn't go to B anymore. And it goes to A. And so, you know, the total amount of B that you got by doing this was 1B. You know, you got to go to B once. And then the training process modified you to not go to B anymore. OK, so here's an alternative strategy. You go to A in training. And you go to B in deployment. What happens if you do this? Well, you go to A in training. And the training process, you know, it looks at, you know, we're just like, you know, computing a, you know, simple, you know, RL reward. We're like, well, you got really high reward. And so whatever behavior just caused you to get high reward, we're going to incentivize that and, you know, increase, you know, that behavior. When we backprop through, you know, we're going to whatever behavior caused you to go to A, you know, it's going to get reinforced. And, you know, but in this situation, the behavior that caused you to go to A was the behavior of actually I'm going to plot to only go to A in training and B in deployment. And so we incentivize the strategy of go to A in training and go to B in deployment. How many Bs does it get? Well, it gets three Bs because we're assuming deployment is, you know, three and training is two in this situation. And so, you know, that's a lot more than 1B. And so, you know, now we have a situation where this robot, theoretically, the best strategy for it is, you know, if it has some proxy and that proxy is different than the thing that we want it to do, the best possible strategy available is to pretend to do the thing that we want in training and then later in deployment defect against us and do some other thing once it has the ability to sort of evade the deployment signal. You can also, you know, we don't necessarily have to have an extremely sharp distinction between training and deployment here. I think you should sort of just conceptualize this as well. At some point, there becomes a point where whatever training processes we had that were able to oversee the agent and sort of correct it if it did the wrong thing, at some point, probably, you know, we're going to have to deploy agents in situations where a single action, you know, on behalf of that agent could potentially be catastrophic. We actually have to trust our agents. We have to actually have to trust our models and we can't just rely on some oversight process. If you believe that we're eventually actually going to have to do that, that we won't just always be able to have some corrective procedure that can sort of post, you know, hawk come back and sort of change the model and make it do the right thing, then, you know, there will eventually be some situation that we can call deployment, where the deployment situation is a situation where eventually, you know, it becomes the case that if the model decides to go to B, we can't stop it anymore. Will the simplicity bias help prevent it from doing different things in training and deployment? Maybe, maybe, maybe. We will talk about that in just a second. I'm not right now talking at all about the likelihood of this happening. I'm just trying to explain what it would look like if it did and why it is, in some sense, theoretically optimal for at least some sorts of agents. Yes. So does this require that the agent has some sort of episodic memory to track the path? It does not require episodic memory. It does require some things. It does require that it have a sort of cross-episodic goal. So it has to care about B, not just like I want to go to B every time I see it. It has to care about going to B as many times as possible. So it does require that. One thing it's, at least, so one thing this picture requires specifically is it requires you to know when it's in training or deployment, to actually have some mechanism being able to determine, oh, I'm now in a situation where I can, I no longer have an oversight process on top of me. As I will talk about later, though, I think that's actually not necessary. I think you don't need that. But, at least for this picture, you do. But I actually think it's not necessary for you to get this option. But the cross-episodic goal absolutely is necessary. We will talk about that. You need some idea of how much training versus deployment for that cross-episodic goal? So you do need to understand that there is training and that there is deployment. You have to understand that those things exist and that they have this sort of structure. You don't necessarily have to understand exactly what the ratio is. You probably have to understand, at least, either the ability to detect when you're in deployment or have some understanding of the ratio, as we'll talk about later. Okay, so this is a thing that could happen. Yes? I'm just a bit confused about the second picture in the sequence. After the first step, you got penalized for going to the wrong thing, right? Yes. Like, now the Mesa objective points to A, but then, like, why does it say- Oh, that's a good question. So this is all from the perspective, when I say the Mesa objective state, this is all from the perspective of the original robot. Because we're sort of imagining, you know, if I'm the robot at the very beginning of this, how, and I have some cross-episodic goal, I care about going to B as many times as possible, how would I plan, you know, for doing things? And it's like, yes, technically, in some sense, the robot here is just like a different robot because now they care about a different thing. And so their, you know, new Mesa objective would be to go to A. But we're like, you know, suppose just right now, from the perspective of the original robot, you know, what does the original robot want to do? And the original robot wants to go to B as many times. And so if they get changed into a robot that goes to A, you know, they're very unhappy. Okay, all right. So why would this happen? Let's talk about it. Okay. Okay, so first. So there's two, so at some point, you know, you do enough adversarial training. At some point, your model is going to have to learn to understand the thing that you are trying to get it to do. You're trying to get it to do something. And at some point, if you've put it in enough situations where, you know, it has to be able to figure out what's happening in all of those situations, at some point, it is going to have to learn what it is that you are trying to get it to do. But I think there's fundamentally two routes via which information about the thing you are trying to get it to do could enter into the model. There's sort of two ways that that information could get in the model. One way is via the gradient updates. You know, the gradient updates provide information to the model about the objective because they, you know, can enforce objectives that in fact result in good behavior and, you know, disincentivize objectives that result in bad behavior. And so that can sort of push the model towards having an objective that corresponds to the correct thing. But there is another path via which information about the thing that you are trying to get it to do can enter into the model, and that's through the model's input. So, you know, if, for example, the model gets to see a bunch of, you know, it gets to read all of Wikipedia, right? You know, there are a bunch of gradient descent update steps that occur in, you know, understanding Wikipedia and, like, predicting it, for example, that encode information about what is in the Wikipedia. And they're not necessarily gradient update steps that are directly about the model's objective. They're just gradient descent update steps that are about encoding and understanding what exists on Wikipedia. Wikipedia actually has a ton of information about the sorts of things that we might want to get models to do. And so there's a mechanism via which information about the sort of training process and about the thing that we are trying to get the model to do can enter into the model that doesn't go directly through a, like, back propagation through, did that objective actually result in good behavior? So there's another mechanism, which is just, like, the model has to learn a bunch of facts about the world via which the model can learn facts about what you're trying to get it to do, what the training process is, and how it works. Okay, so there's these two pathways. So the sort of internalization pathway is pretty straightforward. If this is the way in which it learns about the base objective primarily, it's, you know, hopefully gonna end up with the correct objective. You know, we just reinforce good objectives and, you know, we, you know, disincentivize bad objectives. We should end up, you know, gradient dissenting towards the good one. But there are a couple of ways in which you could instead use the modeling information to learn about the base objective instead. Yes? How do you tell the two are different because when, like, all of Wikipedia's put in the model, it's put in it through training, right? So it's still having gradient effects. That's correct. Let's suppose, so maybe a really simple way to imagine this is let's say we do pre-training and then we do fine-tuning in some RL environments. We pre-train the language model and then we do fine-tuning to try to get that language model to do a good job at, you know, optimizing for something in some environment. In that situation, there was a, there were gradient descent update steps where those update steps were just encoding information. And there were gradient descent update steps where those update steps were encoding, were incentivizing, you know, actions, right? We're directly encoding, like, the sorts of things we wanted to do. And we're saying, well, you know, one, it could end up that most of the information about what we wanted to do comes through, like, you know, the first type or it could come through the second type, right? And if I think it's gonna look different, what you get, depending on where most of the information about the objective is coming from. Is it coming from the sort of just like steps that are just going towards modeling information or is it coming from steps that are going towards, you know, incentivizing or disincentivizing particular objectives? Yes. Is the first one focused on, like, in a world where you have interpretability and you understand exactly what it's doing and you say, don't do that, do this other thing, even though it's achieving the same thing? No, I'm just imagining that you incentivize behavior that you like and you disincentivize behavior you don't like. If you see it do the wrong thing in your environment, then, you know, you get a gradient descent signal to not do that and do something else instead. But that's different than, like, if, you know, giving it information, you know, gradient descent update steps that are just about, you know, giving it information. Okay. Okay, so if the information is mostly coming through, you know, part two, then there's sort of things that are available to gradient descent that can sort of replace step one, right? So in some sense, you know, let's say we do this pre-training and then we do this fine-tuning, right? You know, we could, you know, in the fine-tuning process, just like spend a ton of gradient descent update steps carefully encoding this complex objective that we want the thing to care about. Or instead, we could just make use of the existing knowledge that the model already had in some way that, you know, also results in good performance. So how could you do that? If you're gradient descent and you have this model, and the model knows a ton of facts about the world, it just has a ton of information, how do you use that information to result in the model having good performance in practice on my, you know, training environment? Well, here are two ways. Way number one is deception. You can just sort of take the, you know, you can give it some simple long-term proxy, right? It has to be long-term, as we talked about. But if gradient descent takes the model that knows a bunch of facts, and in particular, if it takes a model that knows a bunch of facts about the training process and gives that model a long-term proxy to optimize, you know, takes the model and says, okay, now what you should do is optimize over the long-term for, you know, number of gold coins or whatever, number of paper clips, whatever. It just gives it some long-term proxy. Then the model has enough information to deduce that given that what I care about is, you know, gold coins of the long-term, I should optimize for, you know, whatever the training process cares about right now so that I can eventually, you know, try to get the gold coins later on. And so as soon as gradient descent makes the modification that takes the model and turns it into a model that cares about, you know, something over the long-term, it gets to make use of all of this information that entered into the model about the training process immediately, right? All of that stuff that the model knew about, like, oh, you know, I know all these facts about the training process. I know how it works. I know what these humans care about. But, and it gets to take all of those facts and instrumentally use them to result in as good performance essentially as possible given the model's understanding of the world because it'll just start optimizing for what you want it to optimize for. And so there's a modification available to gradient descent, which is a relatively simple modification in the situation where most of the information entered through these sort of modeling, which is just, you know, make it so that you care about some long-term thing. Okay, that's option one. Option two is this corrigible thing. So instead of being deceptive, you can instead, you know, you'd be like, well, I have a bunch of information about, you know, what the thing, what the training process is and how it works and what it cares about. I could instead, gradient descent could, take the model and give it a proxy objective that is a pointer to something in that world model. You know, there's something in the world model that corresponds to, you know, the base objective. It corresponds to like what humans care about, what we're trying to train it for, whatever. What you could do is gradient descent could just like, be like, okay, optimize for that thing in the world model here. The difficulty with this though is that you have to actually get that pointer right. So there's like, you know, a lot of difficulty potentially in getting the pointer that actually specifies exactly the correct thing. Whereas with the deception, you know, you sort of don't have to get it right. Like essentially any objective, if you care about it over the long-term will result in good performance because the model will instrumentally reason, oh, you know, the best way to get good performance is just to play along. Okay, so these are two, these are, well, I guess, you know, we have three ways that gradient descent could take a model and produce a model that actually has good performance on like all the adversarial examples in, you know, in a situation we're trying to get it to accomplish some tasks. Okay, so let's talk a little bit about like, you know, how likely each of these are. So I already said some words, but modeling, you know, why do I think modeling is simpler? Well, you know, essentially, right, it's just like, look, you know, it's really, you know, the situation where I think basically in any situation, it's really surprising for your model to actually spend a ton of, you know, for gradient descent spend a ton of update steps just directly encoding for some objective where that objective doesn't make any reference to the like information that the model has. If you have a model and it has a ton of facts in there about the world, and then you, you know, try to get it to optimize for some, you know, for some objective, it's gonna make use of those facts. You know, you shouldn't expect that those facts, you know, are just gonna be, you know, put aside and it's gonna learn some totally separate objective that isn't at all related to the facts that it knows. It's gonna use them both, you know, it's just a simpler, more compressed way of doing things to make use of the information that the model has. And so we shouldn't expect, I think in basically any situation that we end up with an objective that is like separately encoded from the information. It's gonna be encoding in terms of the information in some respect. Okay, so how might it be encoded in terms of the information? Well, you know, one thing is you could have this quadruple thing, right? So the problem is I think the deception, you know, actually beats up the quadruple thing. So why is that? Well, so with the quadruple thing, you know, you're learning some pointer. You're learning some mechanism of pointing to the thing that you, you know, in the world model that corresponds to the thing that you care about. Well, you know, what is that mechanism? Well, essentially, you know, you have to learn some ground truth. You have to learn some way to sort of like, you know, reason about what it is that you care about. Here's an example. Let's say you have a duck and, you know, natural selection evolution wants to get ducks that care about their mothers, right? You want ducks and they want their, you know, they want them to follow their mothers, okay? So what is the mechanism that evolution uses to, you know, get that? Well, you know, it uses this sort of imprinting mechanism, right, where the ducks, you know, they're born and then they see their mother and they're like, oh, that's my mother. I'm going to follow this thing. But, you know, sometimes this fails. You know, the ducks don't see the mother when they're born because they were born, you know, by humans and they see the human and they're like, that's my mother, right? And, you know, that's a problem because the way in which, you know, the pointer was encoded was not actually very robust. And so if you want to solve that problem, you actually have to encode a really robust pointer. You know, you have to have a way of encoding, you know, searching for, you know, okay, I have this understanding of the world that I develop. How do I pick out which thing in that corresponds to my mother, right? Well, it's not the first thing I see, though it is sometimes the first thing I see. Seeing it first is evidence in favor of it being the actual mother, but it's not necessarily the actual mother, right? You have to encode a bunch of detailed information about how to actually pick out your mother from the world model. And the deceptive agent doesn't have to do that, right? The deceptive agent just has to encode, you know, okay, here's the world model. Here is some complex, you know, some extremely simple objective that cares about the long-term, you know, get as many paperclips as possible. And then you just do optimization. You're just like, okay, given that I know these facts about how the world works, and I know that I care about maximizing paperclips, what is the best strategy for me to get the most paperclips? And it's just like, okay, let's do some search. Let's try a bunch of different possible options and see which options would get me the most paperclips. And it's like, oh, the option that gets me those paperclips is play along with this training objective right now and do whatever it wants. And then eventually I'll do something later and I'll get the most paperclips that way. And so you don't have to encode for some really complex way of picking out, you know, what your mother is if you end up doing the deceptive route. Okay, so that's what deception might look like. So what are the sort of, you know, if you ended up in this situation, you know, what would this look like? You know, putting this all together, if you're going to end up in a situation where you develop deception, what are the learning dynamics going to be? Well, you're going to start with some pseudo-alignment. You're gonna get some proxy, you know, to start with as you sort of, you know, start training on some objective. At some point, the model is going to learn information about the training process from its input data. And then at some point, you know, gradient descent is going to take the model and it's going to modify it to, you become deceptive, you know, give it some long-term objective to care about. And then the model is going to start instrumentally authorizing for the base objective, not because it cares about it, but because, you know, for the purpose of staying around so that it can eventually get the most of whatever proxy it wants on its long-term objective that gradient descent gave it. And then at this point, whatever objective is there in the model that gradient descent has given it, some long-term objective, that objective essentially crystallizes. That objective, you know, no longer is relevant performance at all, you know? Whatever that objective is, it does not matter what it is because whatever it is, it's going to result in the same performance because the reason the model has good performance in training has nothing to do with the model's objective and everything to do with the fact that the model is deceptive. And so it no longer matters essentially what the MACE objectives, what the MACE optimizer's objective is. It's now, you know, crystallized, it's frozen, and for all intents and purposes, it's not going to change because it's no longer performance relevant. Okay. It's possible you could get drift towards simpler things, but you're not going to get an incentive towards getting a thing which is more aligned. Okay, and so at this point, gradient descent essentially stops improving the MACE objective, and the only thing that is performance relevant now is just making it so that it's better at being deceptive, you know, giving it a better world model, giving it a better understanding of the training process, helping it be deceptive more effectively because that is the thing that is performance relevant at this point. I don't totally follow why it's more complicated to, like, don't you have to get, it's not necessarily a pointer in the deceptive case, but you still need to form just as good a world model of the MACE objective in both cases. You do. However, you don't have to do that in your weights. You have to do so in the activations, right? So way to think about this, right? So remember we were talking previously about the distinction between simplicity bias and speed bias, right? So the deceptive model, it doesn't have to encode some pointer to, like, how to figure out what its mother is. All it has to encode is, like, care about, you know, over the longterm, care about, you know, coins or whatever. And then it does reasoning on top of that to figure out, oh, I should, you know, optimize, you know, for this particular base objective thing. Those reasoning steps do take extra time, but they don't take extra complexity because it's not the case that you had to encode for that information in the actual structure of the model. It only occurred, it sort of figures it out at runtime, right? It just, like, does some reasoning and realizes what the base objective model is when it just sort of, you know, runs, you know, forward at runtime. And so it costs extra speed to be deceptive, but it takes less complexity to be deceptive. And so if you expect you're going to get more simplicity bias, then you should expect you're going to get more deception. And if you expect you're going to get more speed bias, you should expect you're going to get less deception. But if you can have that, like, sort of, that to-be-evaluated thunk of what your objective is, I think I'm so missing why the courageably aligned one can't do the same. So the courageably and the deceptively aligned one both end up at the same spot. But the question is, how long do they take to end up at the same spot? And how much information do we have to specify to get them to the same spot, right? So how much information do we have to specify to get them to the same spot? Well, the courageably aligned one, we have to specify this complex pointer. And the deceptively aligned one, we have to specify some objective over the longterm. How long do they get there? Well, the courageably aligned one probably gets there first because it has to do fewer steps of reasoning. It has to reason, well, you know, what I have to do is, you know, just follow this pointer. The deceptively aligned one has to do a couple more steps. It has to be like, well, you know, what I want is these coins, but I know there's this training process. And so I know this training process cares about this thing. And so I want to do that. So the deceptively aligned one takes a couple more steps to get there, but the information necessary to encode in the model to get there is smaller. And so it's more compressed and it's simpler, but it does take a couple of extra steps of reasoning at runtime. So the deceptively aligned one, you know, if you're really like, for example, if you really force like short, small models, then you're going to disincentivize deception because it won't have enough time to actually, you know, do the computation. Alternatively, if you have really large models and you have a ton of simplicity bias, then you're going to incentivize deception because you're going to be in a situation where it wants to encode things and as compressed away as possible. So if the base objective is encoded pretty easily in terms of the activations, then the additional complexity isn't that large, right? It's just something like log of the model size in terms of bits. Are you, is that what you- Like for the pointer size? Yeah, for the pointer. Is that what you mean when you talk about sort of the additional complexity or are you imagining that it's actually some complicated function of the activations? So I think that like, I think that's like a reasonable guess if you're like log of the model size number of bits. I think that, yeah, I think that's not crazy. I think that the like deceptive objective is competitive with that and like probably simpler, especially for large models. However, I think it might be worse than that. So why might it be worse than that? Well, it's not just a mechanism of like pointing to some existing thing in the world model. It has to be a pointer that generalizes correctly, right? So it has to be a pointer that like, you know, in the situation where like, you know, maybe the like mother is in a different spot, right? You know, you have to actually have a pointer that encodes enough information to be able to deduce in different situations where do you find the mother, right? And so it actually does have to encode potentially enough information to generalize correctly, not just like point at the correct thing. Whereas the deceptive one, you know, generalizes correctly just by, you know, fact of it's always going to just do whatever the thing is that results in the most training performance because it just wants to get training performance for the purpose of staying around. Whereas the coercionally aligned one has to do some extra stuff to actually robustly get good training performance. Does the pointer need to be robust to the model's world sort of model changing? Like over time, if you train it, it's going to update that model and it's location of mother may change, it's definition of mother may change. Yeah, it does. Yeah. Does it have to be robust to that or does it just not care? It does. And so that can make the coercionally aligned one a little bit harder because it's like, you know, as green descent is changing the world model, it also might have to change the pointer. Yeah. OK. Whereas with the deception, it sort of doesn't have to change it. It's just sort of fixed. Yeah. Is it correct to say that deceptive, you're just encoding survive and then it figures out from first principles that it needs to put objectives to go find and do that? That would be an example. Survive would be an example of an objective that is simple and encoded of the long term. But I'm not even, I'm making a broader claim. It can be anything over the long term that it cares about. It doesn't have to be survive because survive is an instrumental, convergent instrumental goal of essentially anything that you care about over the long term. But you survive it would be an example. Yeah. And it's much simpler than anything how to infer which of the training, which of the objectives we need to find. Right. Right. OK. So I promised also that I would talk a little bit about what happens in a situation where it doesn't necessarily have to actually know when it's in training or deployment. So just a couple of words on this very quickly. So why does the model not necessarily have to know whether it's in training or deployment? You can have a situation where your model, even if it doesn't know exactly when it's going to be in training or when it's in deployment, it has some uncertainty. Maybe it thinks it's just being tested. It's in some situation where actually it's in a honeypot and you might be trying to look for deception. In that situation, it can just sort of chance it. It can be like, well, I don't know 100% sure if this is training or deployment. But probably if I pick some particular fraction of the time to defect, to be deceptive versus not be deceptive, then I can approximately end up in a situation where probably they will have seen no defections in training and yet there will be a defection that exists in deployment. And so there exists some optimal fraction of the time that is supposed to defect for any particular setup. Even if it has no ability to distinguish, there is always going to be some optimal fraction of defections that you can compute based on the size of deployment, the size of training, how good it is to distinguish between them. And so even in a situation where you can't tell it all, you sort of have perfect adversarial training, you're still potentially quite scared of this. And in addition, I think it's like, well, I actually think it is actually you should expect that it's going to be able to determine which one it's in. So while I don't think it's strictly necessary for it to know whether it's in training or deployment, actually I think that it probably most of the time is. So why is it going to know? Well, so there's a bunch of examples of distributional shifts that normally we don't care about very much. But in a situation where your model is actively trying to look for distributional shifts that will tell it whether it's in training or deployment, become a lot scarier, right? So these are things like, well, you train a simulation and then you're deploying the real world. And usually you hope the simulation is pretty similar. But if there's any difference at all, then it can use it to detect whether it's in training or deployment. You give it literally time. It sees a clock and it can tell what date it is. And it can use that to determine whether it's in training or deployment. Normally, no, it probably doesn't matter what the dates are on the data that you give it. But in this situation, maybe it does. This leads to the Mayan calendar doomsday scenario, where you have all the AIs and they're really great and awesome. And then the year hits like 2050 or whatever, and they all defect simultaneously and wipe us out. That's very unlikely. It's not going to happen, but it's fun to think about. OK, but here's a really scary one. So in fact, I think the problem is even worse than this. Because there are situations where the model can look for something which is essentially guaranteed to exist in deployment and yet is impossible to produce in training. So why is that? Well, so this is a very large semi-prime number. It is composed of two large primes multiplied together. Factoring this number and producing the primes that were multiplied together to produce it is extremely difficult. It takes a bunch of computation that we do not have access to right now. And in fact, somebody actually did this. They actually took two primes, multiplied them together. Somebody has the primes, and then they publish this number. And they sort of want us to factor it. And there's a big challenge. You get some money if you factor this. OK, so at some point, computation's going to improve. Eventually, we're going to have the resources to factor this number. Somebody's going to do it. Somebody's going to publish the two primes in some paper where they managed to factor it. OK, so if I'm a particularly devious deceptive model, the thing that I do is, the way I know I'm in deployment, is I just look for a factorization of this number. If I ever see two things that, when multiplied together, produce this thing, then I know for a fact that I must be in deployment. Because there's no way that, in training, their computational resources were available to factor this thing. And yet, at some point, somebody in the real world is going to factor it. So I'm in a situation where it's impossible for me, in training, to generate an input on which the model defects against me. And yet, I know for a fact that, at some point in deployment, it will defect. Because somebody will publish this thing. And so I think that you shouldn't literally expect it's going to look for this thing. But the point is that this is an example of a larger class of P versus NP style problems, where it's just like, look, it is easier to detect a fact about the world than it is to actually instantiate a thing which satisfies that property. And so because of that fundamental asymmetry, there are always going to be things that the model can look for that you can't actually produce examples of via any sort of adversarial training. OK, that's the talk. You can find the paper. I also tease that I have more things to say as well that we can talk about more at some point later, maybe. But for now, we will leave it at this. I'm happy to take questions and everything afterwards. Thank you. 