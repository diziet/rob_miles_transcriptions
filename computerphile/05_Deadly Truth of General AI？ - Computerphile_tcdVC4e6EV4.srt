1
00:00:00,000 --> 00:00:07,600
In a very basic sense, if you've got a general intelligence which therefore has preferences over world states

2
00:00:07,600 --> 00:00:14,600
and takes actions in the world to change the world, we have to make sure that its preferences are aligned with ours,

3
00:00:14,600 --> 00:00:20,800
that it wants what we want, because otherwise it's going to try and do things that we don't want it to do.

4
00:00:20,800 --> 00:00:22,800
That's the basic idea.

5
00:00:24,800 --> 00:00:28,400
Sci-fi comes to the fore, but we're talking things like Terminator, Matrix,

6
00:00:28,400 --> 00:00:32,600
the machines take over the world, I for one salute our new robot overlords.

7
00:00:32,600 --> 00:00:35,000
It makes it difficult to think about the problem, right?

8
00:00:35,000 --> 00:00:41,000
When things happen in fiction, it's generally what would make a better story rather than what would actually happen.

9
00:00:41,000 --> 00:00:48,000
And I think a realistic AI takes over the world type story might not be any fun to read.

10
00:00:48,000 --> 00:00:51,800
So on the one side, this stuff is difficult to think about because of fiction,

11
00:00:51,800 --> 00:00:55,600
because we've all been exposed to something similar to these ideas before.

12
00:00:55,600 --> 00:00:59,400
The other side that makes this difficult to think about is anthropomorphism,

13
00:00:59,400 --> 00:01:02,800
because we are talking about general intelligence,

14
00:01:02,800 --> 00:01:08,000
so we're going to compare it to the examples of general intelligence that we have, which is human minds.

15
00:01:08,000 --> 00:01:14,200
And human minds and artificial general intelligences need not be anything alike.

16
00:01:14,200 --> 00:01:17,600
In the same way that a plane is not similar to a bird,

17
00:01:17,600 --> 00:01:21,600
a supersonic fighter jet is a threat to you in a way that no bird is.

18
00:01:21,600 --> 00:01:24,600
It's not a useful comparison to make, in fact.

19
00:01:24,600 --> 00:01:27,000
But when you say, oh, it's a thing, it has wings and it flies,

20
00:01:27,000 --> 00:01:30,200
and people who don't know anything about planes immediately go to birds.

21
00:01:30,200 --> 00:01:33,600
Presumably a machine could be much more selfish than we can ever imagine.

22
00:01:33,600 --> 00:01:38,200
Absolutely. The space of minds in general is vast.

23
00:01:38,200 --> 00:01:41,000
I like this because we've already talked about spaces, so I can do this.

24
00:01:41,000 --> 00:01:44,000
If you take a space of all possible minds, it's huge.

25
00:01:44,000 --> 00:01:48,600
And then if somewhere within that you have the space of all minds that biological evolution can produce,

26
00:01:48,600 --> 00:01:54,200
and that's also huge. Somewhere within that you have the space of actual minds that exist,

27
00:01:54,200 --> 00:01:58,600
which is much smaller but still huge. Within that you've got human minds, right?

28
00:01:58,600 --> 00:02:03,000
And they're a tiny, they're a minuscule dot on a minuscule dot on a minuscule dot

29
00:02:03,000 --> 00:02:06,800
of the actual possibilities for intelligence that exist.

30
00:02:06,800 --> 00:02:13,200
And a general intelligence that we create is from a completely different part of the space.

31
00:02:13,200 --> 00:02:19,600
And it's extremely tempting to anthropomorphize, more so even than in other contexts,

32
00:02:19,600 --> 00:02:23,600
because it's a thing that's demonstrably intelligent, that makes plans,

33
00:02:23,600 --> 00:02:29,200
that takes actions in the real world. But it need not think anything like us.

34
00:02:29,200 --> 00:02:33,600
And it's a mistake to think of it as basically a person, because it isn't one.

35
00:02:33,600 --> 00:02:37,000
So there's actually a really good example that we can use.

36
00:02:37,000 --> 00:02:38,600
It's a sort of a thought experiment.

37
00:02:38,600 --> 00:02:42,000
This is not a machine that could, practically speaking, be built.

38
00:02:42,000 --> 00:02:51,200
This is an example of an artificial general intelligence, which is specified in overview.

39
00:02:51,200 --> 00:02:56,000
And it gives you something to think about when you're thinking about artificial general intelligences

40
00:02:56,000 --> 00:03:03,000
that makes it distinct from a sort of anthropomorphized human-type intelligence.

41
00:03:03,000 --> 00:03:09,000
So the story is, there's a stamp collector who is also an AI programmer.

42
00:03:09,000 --> 00:03:12,800
And he decides he would like to collect a lot more stamps.

43
00:03:12,800 --> 00:03:15,600
So he's going to write an AI to do this for him.

44
00:03:15,600 --> 00:03:19,800
So he builds a machine. He has some startling insight into general intelligence.

45
00:03:19,800 --> 00:03:24,600
And he builds this machine, which is connected to the internet, right?

46
00:03:24,600 --> 00:03:28,600
So the rules for this system are pretty straightforward.

47
00:03:28,600 --> 00:03:33,800
First thing, it's connected to the internet, and it will send and receive data for one year.

48
00:03:33,800 --> 00:03:39,200
So he's given himself a one-year time window within which to collect stamps.

49
00:03:39,200 --> 00:03:43,400
The second thing is, it has an internal model of reality, or of the universe.

50
00:03:43,400 --> 00:03:48,800
This is the thing that's a bit magic. We don't really know how to build an accurate model of reality.

51
00:03:48,800 --> 00:03:55,200
The point is, this allows it to make accurate predictions about what will happen if it does different things.

52
00:03:55,200 --> 00:03:59,600
The third thing is, for every possible sequence of packets it could send,

53
00:03:59,600 --> 00:04:05,400
it uses its model to predict how many stamps it ends up with at the end of that.

54
00:04:05,400 --> 00:04:12,200
And then the fourth thing is, it outputs as its actual data to the internet.

55
00:04:12,200 --> 00:04:16,200
Whichever output it has predicted will produce the most stamps.

56
00:04:16,200 --> 00:04:20,600
You can see here that this has all the properties of a general intelligence.

57
00:04:20,600 --> 00:04:22,800
It has an internal model of reality.

58
00:04:22,800 --> 00:04:28,400
It has a utility function, or an evaluation function, which is the number of stamps.

59
00:04:28,400 --> 00:04:34,400
And the optimization is extremely simple, and like so much in computer science,

60
00:04:34,400 --> 00:04:37,000
the simple things to specify are the hard things to compute.

61
00:04:37,000 --> 00:04:43,000
It looks at every possible output of data, in other words, every point in that space,

62
00:04:43,000 --> 00:04:45,400
and it picks out the highest one.

63
00:04:45,400 --> 00:04:50,000
So this is the kind of magic intelligence that takes the entire space at once,

64
00:04:50,000 --> 00:04:54,000
finds the highest point, and says, that one.

65
00:04:54,000 --> 00:04:57,400
Which means it's an extremely powerful intelligence, right?

66
00:04:57,400 --> 00:04:59,800
You could say it's extremely intelligent.

67
00:04:59,800 --> 00:05:01,800
The question is, how does this machine behave?

68
00:05:01,800 --> 00:05:04,800
Well, we can look at certain possible sequences of outputs,

69
00:05:04,800 --> 00:05:08,600
and see how they fare in its evaluation criteria.

70
00:05:08,600 --> 00:05:12,600
First off, the vast majority of output sequences are complete junk, right?

71
00:05:12,600 --> 00:05:14,200
It's spewing random data onto the network.

72
00:05:14,200 --> 00:05:17,200
Nothing happens of any consequence.

73
00:05:17,200 --> 00:05:19,800
No stamps get collected.

74
00:05:19,800 --> 00:05:22,600
Those are all rated zero.

75
00:05:22,600 --> 00:05:28,400
But suppose one of the possible sequences sends a request to a server,

76
00:05:28,400 --> 00:05:33,000
let's say eBay, that results in a bid on some stamps, right?

77
00:05:33,000 --> 00:05:37,400
When that happens, there's a thing of 20 stamps, so that output is rated 20.

78
00:05:37,600 --> 00:05:41,600
This is the kind of thing that the stamp collector machine's creator

79
00:05:41,600 --> 00:05:44,200
was expecting to happen.

80
00:05:44,200 --> 00:05:46,200
So then that's good, 20 stamps.

81
00:05:46,200 --> 00:05:48,600
Suppose it could do that lots of times.

82
00:05:48,600 --> 00:05:52,800
It could send out bids, for example, to 30 different stamp collectors on eBay,

83
00:05:52,800 --> 00:05:54,800
and buy 30 sets of different stamps.

84
00:05:54,800 --> 00:05:56,200
And that's even better, right?

85
00:05:56,200 --> 00:05:57,600
That'll be rated even higher.

86
00:05:57,600 --> 00:06:02,800
But the thing is that particularly highly rated options in this search space

87
00:06:02,800 --> 00:06:07,600
are probably things that the stamp collecting device's creator

88
00:06:07,600 --> 00:06:10,800
did not think of and did not anticipate.

89
00:06:10,800 --> 00:06:14,200
So for example, when he made it, he will have presumably given it

90
00:06:14,200 --> 00:06:19,200
his credit card details or something so that it could engage in these bids.

91
00:06:19,200 --> 00:06:21,800
But ultimately, it's searching every possible sequence of outputs.

92
00:06:21,800 --> 00:06:23,200
It needn't use his credit card.

93
00:06:23,200 --> 00:06:26,000
It needn't use money at all.

94
00:06:26,000 --> 00:06:28,600
There's a huge variety of things that it could do here.

95
00:06:28,600 --> 00:06:32,600
So it might send out an enormous number of emails to all the stamp collectors

96
00:06:32,600 --> 00:06:36,800
of the world and convince them through persuasive argument

97
00:06:36,800 --> 00:06:43,600
that he is opening a museum and wants to exhibit their stamps.

98
00:06:43,600 --> 00:06:46,600
It may build a fake website for that, whatever is necessary.

99
00:06:46,600 --> 00:06:48,000
It can predict the world, right?

100
00:06:48,000 --> 00:06:49,600
It has an internal model of reality.

101
00:06:49,600 --> 00:06:53,000
That internal model of reality includes the people, right?

102
00:06:53,000 --> 00:06:55,400
In the same way that it's modeling people

103
00:06:55,400 --> 00:06:58,800
to understand that if it bids on this offer,

104
00:06:58,800 --> 00:07:01,000
then a human being will mail the stamps to them.

105
00:07:01,000 --> 00:07:04,000
They understand that this email might get more people to send stamps,

106
00:07:04,000 --> 00:07:05,400
for example.

107
00:07:05,400 --> 00:07:06,800
That's something.

108
00:07:06,800 --> 00:07:08,800
But then what exactly is a stamp?

109
00:07:08,800 --> 00:07:10,200
How is this defined?

110
00:07:10,200 --> 00:07:11,600
What counts as a stamp?

111
00:07:11,600 --> 00:07:15,000
If it's already written a sequence of outputs that collects all of the stamps

112
00:07:15,000 --> 00:07:16,800
in the world, you'd think you're done, right?

113
00:07:16,800 --> 00:07:17,600
You've built this machine.

114
00:07:17,600 --> 00:07:19,400
Suddenly, it's collected all the stamps in the world.

115
00:07:19,400 --> 00:07:20,800
No, there could be more stamps.

116
00:07:20,800 --> 00:07:23,000
Within a year, we've got time to print more stamps.

117
00:07:23,000 --> 00:07:26,000
So maybe it hijacks the world's stamp printing factories

118
00:07:26,000 --> 00:07:29,800
and puts them into overdrive, producing stamps as many as it possibly can

119
00:07:29,800 --> 00:07:30,600
in that time.

120
00:07:30,600 --> 00:07:35,000
Or perhaps it writes a virus and it hijacks all the computers in the world

121
00:07:35,000 --> 00:07:38,600
to get all of the printers in the world to do nothing but print stamps.

122
00:07:38,600 --> 00:07:40,200
That's even better, right?

123
00:07:40,200 --> 00:07:46,600
The highest rated outcomes for this machine are not good for people.

124
00:07:46,600 --> 00:07:50,600
There comes a point when the stamp collecting device is thinking,

125
00:07:50,600 --> 00:07:53,000
OK, what are stamps made of?

126
00:07:53,000 --> 00:07:54,200
They're made of paper.

127
00:07:54,200 --> 00:07:57,200
Paper is made of carbon and hydrogen and oxygen.

128
00:07:57,200 --> 00:08:00,400
I'm going to need all of that that I can get to make stamps.

129
00:08:00,400 --> 00:08:03,400
And it's going to notice that people are made of carbon, hydrogen, and oxygen,

130
00:08:03,400 --> 00:08:04,000
right?

131
00:08:04,000 --> 00:08:06,200
There comes a point where the stamp collecting device

132
00:08:06,200 --> 00:08:07,800
becomes extremely dangerous.

133
00:08:07,800 --> 00:08:10,200
And that point is as soon as you switch it on.

134
00:08:14,200 --> 00:08:16,400
So this is just a thought experiment example

135
00:08:16,400 --> 00:08:20,400
where we take a sort of maximally powerful intelligence

136
00:08:20,400 --> 00:08:24,200
and see how it behaves so that we can think about

137
00:08:24,200 --> 00:08:26,200
how powerful intelligences behave.

138
00:08:26,200 --> 00:08:30,200
And most importantly, how they are not like people.

