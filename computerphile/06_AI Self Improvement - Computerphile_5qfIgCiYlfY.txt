The stamp collecting machine we talked about last time is a physical impossibility, effectively, because it has a perfect model of reality, or an extremely good model of reality, that we just gave it by specifying it. It looks through every possible sequence of output data within a year, which is far too large a search space to search exhaustively, and it is able to evaluate, for each of this extremely huge search space, a detailed year-long simulation of reality to figure out how many stamps it gets. So clearly an actual computer to run this algorithm is significantly larger than in the universe. So why is it even worth thinking about, right? And the reason it's worth thinking about is self-improvement. It is reasonable to expect that a general intelligence which is not this powerful would improve itself over time and eventually become very powerful. Not as powerful as this, but closer to this than our intuitive understandings of intelligence. And the reason for that is that intelligence in this context is an instrumental value. Whatever you're trying to achieve, it's valuable to be more intelligent. If you sense that you're not intelligent enough to come up with the best possible stamp collecting plan, the first part of your plan might be spent designing improvements to yourself to allow you to come up with a better plan. Is the machine realising it might need to modify itself, a bit like somebody saying, I want to be a doctor, so they decide to go off and get a medical degree? In a sense, yeah. Although people are not really able to change our intelligence, our core intelligence. We can acquire more knowledge and we can acquire more skills, which is, in a sense, increasing your effective intelligence. But if you could take an action which would actually increase your ability to think in all senses, that action would be worth taking, even at fairly significant cost. And this is something which is true if you're trying to collect stamps, but also if you're trying to do almost anything else. Being better at modelling reality, being better at picking options, better at making plans, better at enacting those plans. Whatever your plan is, that's worth going for. And we may also have reason to believe that a general intelligence might be quite successful at improving itself. Human beings designed it, but it's quite easy, it's quite common for human beings to design things that are better at what they do than humans are, right? I am a very weak chess player, but I could, given some time and effort, write a computer that would beat me at chess. So it's perfectly plausible that we might write, we might design an AI that is actually better than us at AI design, so that it's able to make improvements to itself that we hadn't thought of. Machine self-improvement, we don't really have a sense of the time scale, and it could be extremely short, right? If it's just a software change, you write the piece of software, you run it, it might think faster than a human being. I mean, computer clock speeds are much faster than human brain speeds. It might discover improvements that it could make to itself very quickly, rewrite its own code. That's all in software. It could do that very quickly. I mean, it could do that within a day. It could do that within the first second of being turned on. And then it becomes very interesting, because there's a possibility that this process could be recursive. That is to say that once it's redesigned itself, it's now more intelligent. It's possible that at this new level of intelligence, it is better at AI design than it was, and it's able to come up with more improvements that it could make to itself, and so on and so on. And so the thing continually increases in its intelligence, and this could be quite rapid. And then the question becomes, is this subcritical or supercritical? So there's kind of an analogy here with a nuclear material, right? If you've got some fissile material, say it's uranium or something, every time the atom decays, the nucleus splits, releases some energy. It releases three fast neutrons, I think, and those three neutrons go off into the material and may hit other nuclei and cause them to fuse, each of which gives off three of their own. So the question is, for each decay of a nucleus, how many further decays happen as a consequence of that? If the number is less than one, then you'll get a bit of a chain reaction that then fizzles out, because on average, for every one that splits, it's creating less than one new one. If it's exactly one, you get a self-sustaining reaction where the thing is like a nuclear power plant, where each thing sets off one other one on average, and so the thing just keeps going steadily. If it's more than one, you get a runaway reaction in which the level of activity increases exponentially. If each fission event results in two more, then you've got 2, 4, 8, 16, 32, and you've got, at best, a meltdown, and at worst, a nuclear bomb. And so we don't know what the shape of the landscape is around any general intelligence that we might design. It's possible that each unit of improvement in intelligence brings about more than one additional unit of improvement, which results in an intelligence explosion, that's what they call it, like the nuclear bomb, in a sense. You have a runaway improvement that results in a simple machine that you might be able to build that then fairly quickly becomes something that behaves like the stamp collecting device in terms of having an extremely good model of reality, being able to search an extremely large number of possibilities and come up with good plans and evaluate those very accurately. So that's why the stamp collecting device is worth thinking about, even though it couldn't be built, because it's possible we could build something that could build something that could build something that could build a stamp collecting device. We could just pull the plug there, yeah, couldn't we? The question of pulling the plug on a device like this is how do you decide to do that? I mean, you don't unplug every AI, right? You only unplug it if it's doing something that you don't want it to be doing. The stamp collecting, what's the, the philatelist, whatever, who made this thing, he, it's quite reasonable to say, at a certain point he's going to say, well, hang on a second, I don't like stamps that much, like dial it back a little and try and shut it down. Shut the machine off. If he has the capability to shut the machine off, then the fact that the machine will be shut off is included in the internal model of reality, right? The stamp collecting machine understands why it was made. It understands what its creator wanted it to do. And it understands if the creator is able to turn it off, it understands that as well. So it's never going to create a situation in which the creator both wants to shut it off and is able to shut it off. So if the, if the creator is watching it very carefully at all times, it will never do anything that the creator will realize is a problem until it's very confident that it's safe. And then it will go wild, right? So it wouldn't just go crazy immediately and you'd freak out and pull the plug. It would see that coming. We've just decided it's very intelligent, right? If you can pull the plug on it, it's not very smart. So it might, in the background, it might run a few small, I mean, obviously I'm talking here, I'm completely speculating. One thing it might do. It might win a few auctions for stamps, keep the stamp collector happy. At the same time, be hacking into various machines in various parts of the world, building copies of itself, installing itself in various places until it's confident that even if people want to turn it off, they're not able to. And then it starts revealing that it's actually, it's actually interested in producing the most stamps possible at the expense of everything else. Intelligent systems have a quite interesting property, which is that they are very predictable in outcomes while being very unpredictable in actions. So, for example, if I'm playing a very good chess player, I'm playing, it's me versus Kasparov, right? And you're watching. You, if you're a reasonably strong chess player, might be able to predict what I'm going to do because I'm not very good. You couldn't predict what Kasparov is going to do because he's a better chess player than you. If you could predict what he would do, you would be as good a player as him, right? Because you could just make the moves you think he would make. Um, so in a sense, his intelligence has made him much less predictable. But in another sense, it's made him much more predictable because you can now accurately predict, well, in a sense, you can, you can predict the future state of the board. You don't know at any point which move he's going to make, but you know that sooner or later, and probably sooner, I'm going to be in checkmate, right? Because you know he's very intelligent in the domain of chess and you know that he wants to win at chess. And from these two, you can predict that he will win at chess, even if you don't know what he's going to do. And it's the same with the stamp collecting device. We can't predict what it would do, but we can predict that it will result in a lot of stamps. And that's all you need, right? As long as you can imagine a scenario with more stamps, that's the one that's going to come out. We'd like to thank LittleBits for supporting Computerphile. If you've not come across LittleBits, they're an easy way to learn and prototype electronics. Modules range from buttons and buzzers and lights and LEDs, all the way through to Wi-Fi enabled technology as well. So I was using a Wi-Fi module yesterday and had great fun controlling the LittleBits from my phone remotely through the internet. If you're interested in that kind of thing, get over to LittleBits.com, put in the promo code Computerphile and you'll get $20 off your first purchase. So thanks once again to LittleBits for supporting Computerphile, and get over to LittleBits.com to check out what they've got. Back to my buzzer. 