So, should we do a video about the Three Laws of Robotics then? Yeah. Because it keeps coming up in the comments. Okay, so the thing is, you won't hear serious AI researchers talking about the Three Laws of Robotics because they don't work, they never worked. So I think people don't see the Three Laws talked about because they're not serious. They haven't been relevant for a very long time and they're out of a science fiction book and, you know. So, I'm going to do it. I want to be clear that I'm not taking these seriously, right? I'm going to talk about it anyway because it needs to be talked about. So these are some rules that science fiction author Isaac Asimov came up with in his stories as an attempted sort of solution to the problem of making sure that artificial intelligence did what we wanted it to do. Shall we read them out then and see what they are? Oh yeah, I'll look them up. Give me a second. I've looked them up. Okay, right, so they are... Law number one. A robot may not injure a human being or, through inaction, allow a human being to come to harm. Law number two. A robot must obey orders given it by human beings except where such orders would conflict with the first law. Law number three. A robot must protect its own existence as long as such protection does not conflict with the first or second laws. I think there was a zeroth one later as well. Law zero. A robot may not harm humanity or, by inaction, allow humanity to come to harm. So it's weird that these keep coming up because they... Okay, so firstly they were made by someone who was writing stories, right? And they're optimised for story writing. But they don't even work in the books, right? If you read the books, they're all about the ways that these rules go wrong. The various negative consequences. The most unrealistic thing about, in my opinion, about the way that Asimov did his stuff was the way that things go wrong and then get fixed, right? Most of the time, if you have a super intelligence that is doing something you don't want it to do, there's probably no hero who's going to save the day with cleverness. Real life doesn't work that way, generally speaking, right? Because they're written in English, how do you define these things? How do you define human without having to first take an ethical stand on almost every issue? And if human wasn't hard enough, you then have to define harm, right? And you've got the same problem again. Almost any definitions you give for those words, really solid, unambiguous definitions that don't rely on human intuition, result in weird quirks of philosophy resulting in your AI doing something you really don't want it to do. The thing is, in order to encode that rule, don't allow a human being to come to harm, in a way that means anything close to what we intuitively understand it to mean, you have to encode within the words human and harm the entire field of ethics, right? You have to solve ethics comprehensively and then use that to make your definitions. So it doesn't solve the problem, it pushes the problem back one step into now, well, how do we define these terms? When I say the word human, you know what I mean. And that's not because either of us have a rigorous definition of what a human is. We've just sort of learned by general association what a human is, and then the word human points to that structure in your brain, but it's not, I'm not really transferring the content to you. So you can't just say human in the utility function of an AI and have it know what that means. You have to specify, you have to come up with a definition. And it turns out that coming up with a definition, a good definition, of something like human is extremely difficult, right? It's a really hard problem of, essentially of moral philosophy. You would think it would be semantics, but it really isn't, because, okay, so we can agree that I'm a human and you're a human, that's fine, and that this, for example, is a table, and therefore not a human. You know, the easy stuff, the central examples of the classes are obvious, but the edge cases, the boundaries of the classes become really important, the areas in which we're not sure exactly what counts as a human. So, for example, people who haven't been born yet in the abstract, like people who hypothetically could be born ten years in the future, do they count? People who are in a persistent vegetative state, don't have any brain activity, do they fully count as people? People who have died, or unborn fetuses, right? I mean, we, there's a huge debate, even going on as we speak, about whether they count as people. The higher animals, you know, should we include maybe dolphins, chimpanzees, something like that? Do they have weight? And so it turns out you can't program in, you can't make your specification of humans without taking an ethical stance on all of these issues. All kinds of weird hypothetical edge cases become relevant when you're talking about a very powerful machine intelligence, which you otherwise wouldn't think of. So, for example, let's say we say that dead people don't count as humans. Then you have an AI which will never attempt CPR. This person's died, they're gone, forget about it, done, right? Whereas we would say, no, hang on a second, they were only, they're only dead temporarily, we can bring them back, right? Okay, fine, so then we'll say that people who are dead, if they haven't been dead for, well, how long? How long do you have to be dead for? Do we want, I mean, if you get that wrong and you just say, oh, it's fine, do try to bring people back once they're dead, then you may end up with a machine that's desperately trying to revive everyone who's ever died in all of history because they are people who count, who have moral weight. Do we want that? I don't know, maybe. But you've got to decide, right? And that's inherently your definition of human. You have to take a stance on all kinds of moral issues that we don't actually know with confidence what the answer is, just to program the thing in. And then it gets even harder than that because you've got, there are edge cases which don't exist right now, like talking about living people, dead people, unborn people, that kind of thing, fine, animals, but there are all kinds of hypothetical things which could exist which may or may not count as human. For example, emulated or simulated brains, right? If you have a very accurate scan of someone's brain and you run that simulation, is that a person? Does that count? And whichever way you slice that, you get interesting outcomes. So if that counts as a person, then your machine might be motivated to bring out a situation in which there are no physical humans, because physical humans are very difficult to provide for, whereas simulated humans, you can simulate their inputs and have a much nicer environment for everyone. Is that what we want? I don't know. Is it, maybe? I don't know. I don't think anybody does. But the point is, you're trying to write an AI here, right? You're an AI developer. You didn't sign up for this. If you complain to your bank, then the strip club owner will just say, he was with four girls all night and Â£4,000 is what that costs at our place. 