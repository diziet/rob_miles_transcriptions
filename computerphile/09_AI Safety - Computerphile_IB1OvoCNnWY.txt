So, uh, I saw Dr. Holden's video and enjoyed it. The video, I feel like, was kind of framed as a counterpoint, almost. But actually watching it, I really didn't disagree with very much of it at all. I think probably the area in which we diverge is that I think the default artificial general intelligence, if you were to build one without any particular concern for safety, would be a bad one. It would be one that we don't want to build, that would try to do things we don't want it to do. And I talked about this in the earlier videos, when I was talking about the space of general minds, or the space of motivations. This is the kind of thing that the stamp collector machine's creator was expecting to happen. Human values are complicated, right? They are complicated, and we don't really know what they are fully. And anything that has values that aren't very tightly aligned to ours is not a thing we want to be building. I've never actually talked to Dr. Holden about this, but almost everything in that video I agree with. I agree about the time scales. I agree that we are a long way away from the kind of thing that I'm talking about. Or that we're probably a long way away, right? In the previous video, I stand by the video, but I think I came across as overconfident just because of the way that it was framed. Like, I didn't choose the title, the deadly truth. You call something the truth, capital T, you know. Well, it was a thought experiment. I would have called it an interesting thought experiment about general artificial intelligence, or something like that. There's an argument that it might not have got quite so many views. Yeah, I'm not going to try and tell you how to do your job. But, no, the actual content of the video I stand by. So there was really only one thing in Dr. Holden's video that I didn't agree with, which was like one sentence. He was talking about how we don't pay enough attention to the friendly AI possibility, the positive outcome, which I think is true. But then he said, which I think is more likely. I don't know how likely it is. I think it depends on how seriously we take this problem. Of how do we actually make sure that any artificial general intelligence we create is friendly. Friendly, you know. I don't think that it's a problem that's solved. And I don't think that it's a problem that will solve itself. I actually think it's a problem that's very difficult to solve. And we are very far from a solution, or quite far from a solution. It's very difficult to say with any confidence how far we are. But it's comparable to how far we are from general AI itself. We had a brief conversation with Professor Brailsford on exactly this. And he said maybe this is like cold fusion. 50 years ago they said, hey, in 50 years we'll be able to do this. And it's always in 50 years. Right, yeah. The thing is with historical examples, you can go back and find historical examples. Endless historical examples of people claiming that something fantastic is right around the corner. When in fact it isn't. But the thing is you can also find a lot of examples of people saying that something fantastic is definitely never going to happen. And it's actually right around the corner. Because before we talked about fission. Self-sustaining fission reactions in the AI video. That is an example of a situation where you have Ernest Rutherford, Nobel Prize winner, extremely eminent scientist. So he split the atom, right, or he was part of a team that split the atom. And he was on the record saying some people think that you could use this as a source of actual energy. That you could reliably get energy out of this. That's, he called it moonshine. He said it was completely absurd, right. And then you have Leo Szilard who eventually did it. He had the idea, now we don't know exactly. But as far as we can tell, the idea for using neutrons to make a self-sustaining nuclear reaction. Occurred to him on the same day that Rutherford was dismissing it as nonsense. This kind of thing can happen. I don't, I'm not saying that it will. That's my point, right. You can find situations where people are overconfident in predicting something will happen. You can also find situations where people are overconfident in predicting it won't happen. And it's just a function of the fact that predicting is really difficult. My central point actually doesn't rely on anything to do with time scales really, right. I think we're a long way away from all of these things. But at the same time, I'd be very surprised if it were more than a hundred years or something. You know, just because, and again this is what, this is as Dr. Holden said. It's kind of inescapable if we continue advancing as we are. We're going to get there sooner or later. We know it's not impossible unless we wipe ourselves out some other way or destroy our technological capacity some other way. Or we discover that the brain is literally magic. And not like Roger Penrose quantum magic, like actual magic. Then we'll get there sooner or later, right. My point is that the problem of AI safety is not solved, is not going to solve itself, and is not easy to solve. And most importantly, we have to solve it before we solve the problem of general AI. That's all I'm trying to say really, is that sooner or later we will probably get general AI. And when we do, we have to know how to make safe general AI. And currently we're a long way from that. Over a long enough time scale, I think that human level artificial intelligence is completely inevitable. When asked, was that a human you were talking to or was it a machine? If you can't decide that it's not human, then it's passed the test. 