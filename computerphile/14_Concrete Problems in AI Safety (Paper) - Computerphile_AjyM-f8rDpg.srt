1
00:00:00,000 --> 00:00:06,440
Today I thought I'd talk about a paper fairly recent was last year a paper called concrete problems in AI safety

2
00:00:06,480 --> 00:00:10,680
Which is going to be related to the stuff I was talking about before with the stop button

3
00:00:10,680 --> 00:00:13,560
It's got a bunch of authors mostly from Google brain

4
00:00:14,680 --> 00:00:16,680
Google's AI research

5
00:00:17,000 --> 00:00:20,720
Department I guess well a lot of its AI research, but specifically Google brain

6
00:00:21,280 --> 00:00:22,480
and

7
00:00:22,480 --> 00:00:27,720
Some people from Stanford and Berkeley and open AI and whatever. It's a collaboration between a lot of different authors

8
00:00:30,160 --> 00:00:33,160
The idea of the paper is trying to lay out a

9
00:00:34,680 --> 00:00:40,360
Set of problems that we are able to currently make progress on like

10
00:00:41,240 --> 00:00:45,800
If we're concerned about this far-off sort of super intelligence stuff

11
00:00:47,440 --> 00:00:55,120
Sure, it seems important and it's interesting and difficult and whatever but it's quite difficult to sit down and actually do anything about it

12
00:00:55,680 --> 00:00:57,280
Because we don't know

13
00:00:57,280 --> 00:01:01,240
Very much about what a super intelligence will be like or how it would be implemented or whatever

14
00:01:01,240 --> 00:01:03,240
the idea of this paper is that it

15
00:01:05,760 --> 00:01:11,080
It lays out some problems that we can tackle now which will be helpful now and

16
00:01:12,080 --> 00:01:17,400
That I think will be helpful later on as well with more advanced AI systems and making them safe as well

17
00:01:17,480 --> 00:01:20,880
It lists five problems avoiding negative side effects

18
00:01:20,880 --> 00:01:27,120
which is quite closely related to the stuff we've been talking about before with the stop button or the stamp collector a lot of the

19
00:01:27,680 --> 00:01:30,520
Problems with that can be framed as negative side effects

20
00:01:30,520 --> 00:01:35,000
They do the thing you ask them to but in the process of doing that they do a lot of things that you don't want

21
00:01:35,000 --> 00:01:37,280
Them to this is like the robot running of the baby, right?

22
00:01:37,280 --> 00:01:37,760
Yeah

23
00:01:37,760 --> 00:01:43,680
Any anything where it does the thing you wanted it to like it makes you the cup of tea or it collects you stamps or?

24
00:01:43,680 --> 00:01:47,200
Whatever but in the process of doing that it also does things you don't want it to do

25
00:01:48,160 --> 00:01:50,320
So those are your negative side effects

26
00:01:51,360 --> 00:01:53,520
so that's the first of the

27
00:01:53,920 --> 00:01:57,480
The research areas is how do we avoid these negative side effects?

28
00:01:57,840 --> 00:02:05,520
Then there's avoiding reward hacking which is about systems gaming their reward function doing something which technically counts

29
00:02:06,040 --> 00:02:09,160
But isn't really what you intended the reward function to be

30
00:02:12,200 --> 00:02:14,240
There's a lot of different ways that that can manifest

31
00:02:14,240 --> 00:02:20,160
but this is like this is already a common problem in machine learning systems where you come up with your your

32
00:02:20,440 --> 00:02:24,200
Evaluation function or your reward function or whatever your objective function and

33
00:02:25,320 --> 00:02:31,360
The system very carefully optimizes to exactly what you wrote and then you realize what you wrote isn't what you meant

34
00:02:31,800 --> 00:02:36,040
Scalable oversight is the next one. It's a problem that human beings have all the time

35
00:02:36,040 --> 00:02:43,080
Anytime you've started a new job, you don't know what to do and you have someone who does who's supervising you the question is

36
00:02:44,440 --> 00:02:49,760
What questions do you ask and how many questions do you ask because current machine learning systems?

37
00:02:50,160 --> 00:02:56,080
Can learn pretty well if you give them a million examples, but you don't want your robot to ask you a million questions

38
00:02:56,120 --> 00:02:58,120
You know you want it to only

39
00:02:59,000 --> 00:03:04,760
Ask a few questions and use that information efficiently to learn from you safe exploration is the next one

40
00:03:04,880 --> 00:03:09,120
Which is about well about safely exploring the range of possible actions

41
00:03:09,440 --> 00:03:16,320
So you will want the system to experiment, you know, try different things try out different approaches

42
00:03:16,960 --> 00:03:19,080
That's the only way it's gonna find what's gonna work

43
00:03:19,240 --> 00:03:24,280
But there are some things that you don't want it to try even once like the baby, right, right

44
00:03:24,280 --> 00:03:29,360
Yeah, you don't want it to say know what happens if I run over this baby. Do you want certain possible?

45
00:03:30,040 --> 00:03:33,760
Things that it might consider trying to actually not try at all

46
00:03:34,000 --> 00:03:39,320
Because you can't afford to have them happen even once in the real world like the thermonuclear war option

47
00:03:39,320 --> 00:03:43,640
What happens if I do this you don't want it to try that. Is that the sort of thing? Yeah. Yeah

48
00:03:44,640 --> 00:03:46,640
Yeah, yeah

49
00:03:49,640 --> 00:03:55,440
It runs through a simulation of every possible type of nuclear war right but it does it in simulation

50
00:03:55,800 --> 00:04:02,200
You want your system not to run through every possible type of thermonuclear war in real life to find out it doesn't work because

51
00:04:02,200 --> 00:04:03,960
You can't it's too

52
00:04:03,960 --> 00:04:08,800
Unsafe to do that even once the last area to look into is robustness to distributional shift

53
00:04:09,480 --> 00:04:11,160
Yeah

54
00:04:11,160 --> 00:04:13,160
yeah, it's it's a it's a

55
00:04:14,280 --> 00:04:17,280
Complicated term, but the concept is not it's just that

56
00:04:17,920 --> 00:04:21,160
the situation can change over time so

57
00:04:21,800 --> 00:04:25,060
You may end up you may make something you train it

58
00:04:25,240 --> 00:04:32,560
It performs well, and then things change to be different from the training scenario and that is inherently

59
00:04:32,960 --> 00:04:38,320
Very difficult like it's something humans again human struggle with you find yourself in a situation

60
00:04:38,320 --> 00:04:40,080
You've never been in before

61
00:04:40,080 --> 00:04:43,920
but the difference I think or one of the useful things that humans do is

62
00:04:45,040 --> 00:04:51,840
notice that there's a problem a lot of current machine learning systems if you if something changes underneath them and they're

63
00:04:53,000 --> 00:04:59,800
Their training is no longer useful. They have no way of knowing that so they continue being just as confident in their answers that now

64
00:04:59,800 --> 00:05:01,160
Make no sense

65
00:05:01,160 --> 00:05:04,400
Because they're not they haven't noticed that there's a change. So

66
00:05:05,400 --> 00:05:12,520
If we can't make systems that can just react to completely unforeseen circumstances

67
00:05:12,520 --> 00:05:15,840
We may be able to make systems that at least can recognize that they're in

68
00:05:16,560 --> 00:05:20,200
Unforeseen circumstances and ask for help and then maybe we have a scalable supervision

69
00:05:20,680 --> 00:05:24,480
Situation there where they recognize the problem, and that's when they ask for help

70
00:05:24,480 --> 00:05:29,800
I suppose a simplified simplistic example. This is when you have an out-of-date sack now and

71
00:05:30,480 --> 00:05:35,160
It doesn't seem to realize that you happen to be doing 70 miles an hour over a plowed field

72
00:05:35,440 --> 00:05:38,360
Right because somebody else's, you know built a road there

73
00:05:38,480 --> 00:05:38,980
Yeah

74
00:05:38,980 --> 00:05:46,800
Exactly the general tendency of unless you program them specifically not to to just plow on with what they think they should be doing

75
00:05:47,440 --> 00:05:50,000
yeah can cause problems and in a

76
00:05:51,440 --> 00:05:53,240
large scale

77
00:05:53,240 --> 00:05:55,800
Heavily depended on you know in this case. It's your sat-nav

78
00:05:55,800 --> 00:06:00,000
So it's not too big of a deal because it's not actually driving the car and you know

79
00:06:00,000 --> 00:06:05,840
What's wrong and you can ignore it whatever as AI systems become more important and more integrated into everything that kind of thing can

80
00:06:05,840 --> 00:06:07,840
Become a real problem

81
00:06:10,320 --> 00:06:12,320
Yeah, yeah

82
00:06:15,280 --> 00:06:19,040
Yeah, so it the way the way it does all of these is it

83
00:06:20,600 --> 00:06:22,600
Gives a quick outline of what the problem is

84
00:06:23,200 --> 00:06:28,600
They the example they usually use is a cleaning robot like we've made this we've made a robot

85
00:06:28,600 --> 00:06:34,600
It's in an office or something and it's cleaning up and then it they sort of frame the different problems

86
00:06:34,600 --> 00:06:36,400
There's things that could go wrong in that scenario

87
00:06:36,400 --> 00:06:40,240
So it's pretty similar to the get me a cup of tea and don't run over the baby type setup

88
00:06:40,920 --> 00:06:47,360
It's clean the office and don't you know, knock anything over or destroy anything and then for each one

89
00:06:48,360 --> 00:06:52,680
The paper talks about possible approaches to each problem and

90
00:06:55,560 --> 00:06:59,680
Things we can work on basically things that we don't know how to do yet

91
00:06:59,680 --> 00:07:05,360
But which seem like they might be doable in a year or two and some careful thought this paper

92
00:07:05,360 --> 00:07:08,000
Is this one for me? Yeah, it's really good. It's

93
00:07:09,560 --> 00:07:14,040
It doesn't cover anything like the range of the problems in AI safety

94
00:07:14,320 --> 00:07:21,280
But of the problems specifically about avoiding accidents because all of these are problem all of these are ways of

95
00:07:22,440 --> 00:07:24,120
creating possible accidents

96
00:07:24,120 --> 00:07:26,880
Right possible causes of accidents

97
00:07:27,560 --> 00:07:31,200
There's all kinds of other problems. You can have an AI that don't fall under accidents

98
00:07:32,240 --> 00:07:36,200
But within that area, I think it covers everything and it's quite readable. It's quite

99
00:07:37,640 --> 00:07:43,280
It doesn't require really high level because it's an overview paper doesn't require high level AI

100
00:07:44,800 --> 00:07:48,160
Understanding for the most part anyone can read it and it's on the archive

101
00:07:48,160 --> 00:07:54,960
So, you know, it's freely available these guys now working on AI safety. They do is this then they've hung their hats up

102
00:07:54,960 --> 00:07:58,040
They've written a paper and they're hoping someone else is going to sort it all out for them

103
00:07:58,080 --> 00:08:00,480
Yeah, these people are working on AI safety right now

104
00:08:01,560 --> 00:08:08,520
But they're not the only people this paper was released in summer of 2016. So it's been about a year since it came out

105
00:08:09,080 --> 00:08:10,320
and

106
00:08:10,320 --> 00:08:17,040
Since then there have been more advances and some of the problems posed have had really interesting solutions

107
00:08:18,080 --> 00:08:25,920
Or well not solutions early work that looks like it could become a solution or approaches new interesting ideas about

108
00:08:26,640 --> 00:08:29,680
Ways to tackle these problems. So I think as a paper

109
00:08:29,680 --> 00:08:38,120
It's already been successful in spurring new research and giving people a focus to build their AI safety research on top of

110
00:08:38,600 --> 00:08:41,920
So we just need to watch this space, right? Yeah, exactly

111
00:08:45,040 --> 00:08:47,040
And there's a kid in the way your

112
00:08:47,520 --> 00:08:49,440
utility function only cares about

113
00:08:49,440 --> 00:08:53,620
Tea, right? So it's not going to avoid hitting the baby

114
00:08:54,240 --> 00:09:00,840
So you rush over there to hit the button obviously as you built it in and what happens of course is that the robot?

