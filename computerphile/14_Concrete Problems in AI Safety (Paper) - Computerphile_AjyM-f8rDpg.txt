Today I thought I'd talk about a paper fairly recent was last year a paper called concrete problems in AI safety Which is going to be related to the stuff I was talking about before with the stop button It's got a bunch of authors mostly from Google brain Google's AI research Department I guess well a lot of its AI research, but specifically Google brain and Some people from Stanford and Berkeley and open AI and whatever. It's a collaboration between a lot of different authors The idea of the paper is trying to lay out a Set of problems that we are able to currently make progress on like If we're concerned about this far-off sort of super intelligence stuff Sure, it seems important and it's interesting and difficult and whatever but it's quite difficult to sit down and actually do anything about it Because we don't know Very much about what a super intelligence will be like or how it would be implemented or whatever the idea of this paper is that it It lays out some problems that we can tackle now which will be helpful now and That I think will be helpful later on as well with more advanced AI systems and making them safe as well It lists five problems avoiding negative side effects which is quite closely related to the stuff we've been talking about before with the stop button or the stamp collector a lot of the Problems with that can be framed as negative side effects They do the thing you ask them to but in the process of doing that they do a lot of things that you don't want Them to this is like the robot running of the baby, right? Yeah Any anything where it does the thing you wanted it to like it makes you the cup of tea or it collects you stamps or? Whatever but in the process of doing that it also does things you don't want it to do So those are your negative side effects so that's the first of the The research areas is how do we avoid these negative side effects? Then there's avoiding reward hacking which is about systems gaming their reward function doing something which technically counts But isn't really what you intended the reward function to be There's a lot of different ways that that can manifest but this is like this is already a common problem in machine learning systems where you come up with your your Evaluation function or your reward function or whatever your objective function and The system very carefully optimizes to exactly what you wrote and then you realize what you wrote isn't what you meant Scalable oversight is the next one. It's a problem that human beings have all the time Anytime you've started a new job, you don't know what to do and you have someone who does who's supervising you the question is What questions do you ask and how many questions do you ask because current machine learning systems? Can learn pretty well if you give them a million examples, but you don't want your robot to ask you a million questions You know you want it to only Ask a few questions and use that information efficiently to learn from you safe exploration is the next one Which is about well about safely exploring the range of possible actions So you will want the system to experiment, you know, try different things try out different approaches That's the only way it's gonna find what's gonna work But there are some things that you don't want it to try even once like the baby, right, right Yeah, you don't want it to say know what happens if I run over this baby. Do you want certain possible? Things that it might consider trying to actually not try at all Because you can't afford to have them happen even once in the real world like the thermonuclear war option What happens if I do this you don't want it to try that. Is that the sort of thing? Yeah. Yeah Yeah, yeah It runs through a simulation of every possible type of nuclear war right but it does it in simulation You want your system not to run through every possible type of thermonuclear war in real life to find out it doesn't work because You can't it's too Unsafe to do that even once the last area to look into is robustness to distributional shift Yeah yeah, it's it's a it's a Complicated term, but the concept is not it's just that the situation can change over time so You may end up you may make something you train it It performs well, and then things change to be different from the training scenario and that is inherently Very difficult like it's something humans again human struggle with you find yourself in a situation You've never been in before but the difference I think or one of the useful things that humans do is notice that there's a problem a lot of current machine learning systems if you if something changes underneath them and they're Their training is no longer useful. They have no way of knowing that so they continue being just as confident in their answers that now Make no sense Because they're not they haven't noticed that there's a change. So If we can't make systems that can just react to completely unforeseen circumstances We may be able to make systems that at least can recognize that they're in Unforeseen circumstances and ask for help and then maybe we have a scalable supervision Situation there where they recognize the problem, and that's when they ask for help I suppose a simplified simplistic example. This is when you have an out-of-date sack now and It doesn't seem to realize that you happen to be doing 70 miles an hour over a plowed field Right because somebody else's, you know built a road there Yeah Exactly the general tendency of unless you program them specifically not to to just plow on with what they think they should be doing yeah can cause problems and in a large scale Heavily depended on you know in this case. It's your sat-nav So it's not too big of a deal because it's not actually driving the car and you know What's wrong and you can ignore it whatever as AI systems become more important and more integrated into everything that kind of thing can Become a real problem Yeah, yeah Yeah, so it the way the way it does all of these is it Gives a quick outline of what the problem is They the example they usually use is a cleaning robot like we've made this we've made a robot It's in an office or something and it's cleaning up and then it they sort of frame the different problems There's things that could go wrong in that scenario So it's pretty similar to the get me a cup of tea and don't run over the baby type setup It's clean the office and don't you know, knock anything over or destroy anything and then for each one The paper talks about possible approaches to each problem and Things we can work on basically things that we don't know how to do yet But which seem like they might be doable in a year or two and some careful thought this paper Is this one for me? Yeah, it's really good. It's It doesn't cover anything like the range of the problems in AI safety But of the problems specifically about avoiding accidents because all of these are problem all of these are ways of creating possible accidents Right possible causes of accidents There's all kinds of other problems. You can have an AI that don't fall under accidents But within that area, I think it covers everything and it's quite readable. It's quite It doesn't require really high level because it's an overview paper doesn't require high level AI Understanding for the most part anyone can read it and it's on the archive So, you know, it's freely available these guys now working on AI safety. They do is this then they've hung their hats up They've written a paper and they're hoping someone else is going to sort it all out for them Yeah, these people are working on AI safety right now But they're not the only people this paper was released in summer of 2016. So it's been about a year since it came out and Since then there have been more advances and some of the problems posed have had really interesting solutions Or well not solutions early work that looks like it could become a solution or approaches new interesting ideas about Ways to tackle these problems. So I think as a paper It's already been successful in spurring new research and giving people a focus to build their AI safety research on top of So we just need to watch this space, right? Yeah, exactly And there's a kid in the way your utility function only cares about Tea, right? So it's not going to avoid hitting the baby So you rush over there to hit the button obviously as you built it in and what happens of course is that the robot? 