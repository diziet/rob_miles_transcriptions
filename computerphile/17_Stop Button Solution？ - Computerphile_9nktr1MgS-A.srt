1
00:00:00,000 --> 00:00:02,000
While back we were talking about

2
00:00:02,920 --> 00:00:05,920
The stop button problem, right? You have this you have this

3
00:00:07,080 --> 00:00:11,860
It's kind of a toy problem in AI safety. You have an artificial general intelligence in a robot

4
00:00:11,960 --> 00:00:17,800
it wants something, you know, it wants to make you a cup of tea or whatever you put a big red stop button on it and

5
00:00:18,440 --> 00:00:24,760
You want to set it up so that it behaves courageably that it will allow you to hit the button

6
00:00:24,920 --> 00:00:29,360
It won't hit the button itself, you know, and it won't try and prevent you this sort of

7
00:00:30,920 --> 00:00:32,920
Behaving in a

8
00:00:33,240 --> 00:00:35,320
In a sensible way in a safe way

9
00:00:36,200 --> 00:00:38,120
and

10
00:00:38,120 --> 00:00:40,120
That like by default

11
00:00:40,640 --> 00:00:43,400
Most AGI designs will not behave this way

12
00:00:43,520 --> 00:00:47,800
Well, we left it as an open problem, right and it kind of still is an open problem

13
00:00:47,800 --> 00:00:53,600
But there have been some really interesting things proposed as possible solutions or approaches to take and I wanted to talk about

14
00:00:54,240 --> 00:00:56,440
cooperative inverse reinforcement learning I

15
00:00:56,560 --> 00:00:58,560
I

16
00:00:58,600 --> 00:01:04,560
Thought the easiest way to explain a cooperative inverse reinforcement learning is to build it up backwards, right?

17
00:01:04,880 --> 00:01:07,120
learning we know like machine learning and

18
00:01:07,680 --> 00:01:13,240
Reinforcement learning is an area of machine learning. I guess you could call it. It's it's kind of a

19
00:01:14,240 --> 00:01:16,240
It's a way of presenting a problem

20
00:01:16,720 --> 00:01:18,720
in most machine learning

21
00:01:18,720 --> 00:01:22,720
The kind of thing that people have talked about already a lot on computer file

22
00:01:23,040 --> 00:01:25,800
Thinking of who has videos and the related ones

23
00:01:26,200 --> 00:01:31,680
Usually you get in some data and then you're trying to do something with that like classify

24
00:01:31,960 --> 00:01:39,000
You know unseen things or you're trying to do like regression to find out what value something would have for a certain inputs that kind

25
00:01:39,000 --> 00:01:40,360
of thing

26
00:01:40,360 --> 00:01:41,920
whereas

27
00:01:41,920 --> 00:01:45,720
reinforcement learning the idea is you have an agent in an environment and

28
00:01:46,400 --> 00:01:48,400
you're trying to find a

29
00:01:50,200 --> 00:01:52,200
Policy but so so

30
00:01:53,160 --> 00:01:57,520
Let's back up. What do we mean by an agent? It's an entity that interacts with its environment

31
00:01:58,080 --> 00:02:02,760
To try and achieve something effectively. It's doing things in an environment

32
00:02:02,760 --> 00:02:08,120
So this isn't that's really if is this a physical thing or is it me doesn't have to be so if you have

33
00:02:08,520 --> 00:02:14,800
Robot in a room then you can model that as the robot being an agent and the room being environment. Similarly if you have a

34
00:02:15,560 --> 00:02:17,560
computer game like

35
00:02:18,440 --> 00:02:19,600
Pac-man

36
00:02:19,600 --> 00:02:23,320
Then Pac-man is an agent and the sort of maze he's in is his environment

37
00:02:23,320 --> 00:02:27,840
So let's stick with Pac-man that the way that a reinforcement learning

38
00:02:28,760 --> 00:02:35,460
Framework for dealing with Pac-man is you say? Okay, you've got Pac-man. He's the agent. He's in the environment and you have

39
00:02:36,000 --> 00:02:40,440
Actions the Pac-man can take in the environment now, it's kind of neat in Pac-man

40
00:02:40,440 --> 00:02:42,840
There are always exactly four actions you can take or well

41
00:02:42,840 --> 00:02:47,680
I guess five you can sit there and do nothing. You can move up left right or down

42
00:02:47,880 --> 00:02:52,560
You don't always have all of those options like sometimes there's a wall and you can't move, right?

43
00:02:52,840 --> 00:02:57,680
but those are the only that's the that's the complete set of actions that you have and

44
00:02:58,480 --> 00:03:03,080
Then you have the environment contains sort of dots that you can pick up

45
00:03:03,800 --> 00:03:06,040
Which are they give you points?

46
00:03:06,200 --> 00:03:11,800
It's got these ghosts that chase you that you don't want to touch and I think there's also there's like pills you can pick

47
00:03:11,800 --> 00:03:14,960
Up that make the ghosts edible and then you chase them down and stuff

48
00:03:14,960 --> 00:03:22,720
Anyway, so the difference in reinforcement learning is that the agent is in the environment and it learns by interacting with the environment

49
00:03:23,280 --> 00:03:27,680
It's and so it's kind of close to the way that animals learn and the way that humans learn

50
00:03:29,040 --> 00:03:35,880
You try you try doing something, you know, I'm gonna try, you know touching this fire. Oh that hurt

51
00:03:36,280 --> 00:03:41,480
So that's that's caused me like a negative reward. That's caused me a pain signal, which is something

52
00:03:41,480 --> 00:03:44,840
I don't want so I learn to avoid doing things like touching a fire

53
00:03:44,960 --> 00:03:48,360
so in in a pac-man environment, you might you might sort of say

54
00:03:49,800 --> 00:03:54,400
If you're in a you're in a situation like let's draw pac-man. Let's say he's in a maze like this

55
00:03:54,880 --> 00:03:56,880
You look at pac-man's options

56
00:03:57,200 --> 00:04:01,600
He can't go left. He can't go right he can go up and if he goes up

57
00:04:01,600 --> 00:04:04,640
He'll get a dot which earns you some points

58
00:04:04,640 --> 00:04:09,380
So up gets a score of you know, plus 10 or however, you've decided it

59
00:04:10,220 --> 00:04:16,020
Or well, whatever the score is in the game either way or if he goes down he'll be immediately got by this ghost

60
00:04:17,620 --> 00:04:23,020
The point is that pac-man doesn't need to be aware of the entire board right of the entire maze

61
00:04:23,020 --> 00:04:28,900
you can just feed in a fairly small amount of information about his immediate environment, which is the same thing as

62
00:04:29,540 --> 00:04:33,940
If you have a robot in a room, it can't it doesn't know everything about the whole room

63
00:04:33,940 --> 00:04:36,700
it can only see what it sees through its camera, you know, it has

64
00:04:37,460 --> 00:04:41,160
Sensors they give it some some information about the environment

65
00:04:42,460 --> 00:04:44,460
partial information I

66
00:04:44,460 --> 00:04:50,220
Suppose just playing devil's advocate. The difference here is usually pac-man is being controlled by a human who can see the whole board

67
00:04:50,660 --> 00:04:56,640
So the point being if that ghost is actually not static and is chasing pac-man and he's heading up to get that

68
00:04:57,260 --> 00:04:59,060
pill if

69
00:04:59,060 --> 00:05:04,420
If a few pixels later that that corridor if you like stops in a dead end

70
00:05:04,940 --> 00:05:12,260
Yep, well, he's kind of stuffed either way really that's true. Yeah, so that is because so so

71
00:05:12,820 --> 00:05:13,860
most

72
00:05:13,860 --> 00:05:15,860
Well, yeah, almost every

73
00:05:17,740 --> 00:05:20,900
Reinforcement learning algorithm almost everything that tries to deal with this problem

74
00:05:21,420 --> 00:05:27,340
Doesn't just look at the immediate surroundings or it looks at the immediate surroundings, but it also looks a certain distance in time

75
00:05:27,900 --> 00:05:31,340
So you're not just saying what's going to happen next frame, but

76
00:05:31,940 --> 00:05:33,940
So like if you if you go down

77
00:05:33,980 --> 00:05:38,460
Here most algorithms would say okay the option of going down in this situation is bad

78
00:05:38,740 --> 00:05:44,740
But also all of the options we chose in all of the situations that we were in in the last

79
00:05:45,220 --> 00:05:52,100
Second or two also get a little bit. This is kind of a decay. There's time time discounting

80
00:05:53,020 --> 00:05:55,020
so that

81
00:05:55,620 --> 00:05:59,700
You're not just punishing the immediate thing that causes the negative reward

82
00:05:59,700 --> 00:06:06,260
But also the decisions you make leading up to it so that pac-man might learn not to get himself stuck in corners

83
00:06:07,060 --> 00:06:09,820
As well as just learning not to run straight into ghosts

84
00:06:10,900 --> 00:06:12,340
so

85
00:06:12,340 --> 00:06:16,580
That's the basics of reinforcement learning. There's different algorithms that do it and the idea is you

86
00:06:17,220 --> 00:06:20,660
You actually you start off exploring the environment just at random

87
00:06:20,660 --> 00:06:27,500
You just pick completely random actions and then as those actions start having consequences for you and you start getting rewards and punishments

88
00:06:27,620 --> 00:06:29,620
You start to learn

89
00:06:30,980 --> 00:06:36,840
Which actions are better to use in which situations does that mean that in pac-man's case would learn the maze

90
00:06:36,980 --> 00:06:42,500
Or would it just learn the better choices? It depends on what algorithm you're using

91
00:06:44,340 --> 00:06:47,740
Very sophisticated one might learn the whole maze a simpler one might just learn

92
00:06:50,340 --> 00:06:52,540
More kind of local policy

93
00:06:52,740 --> 00:06:54,740
But

94
00:06:54,860 --> 00:06:55,860
The point is yeah

95
00:06:55,860 --> 00:07:02,580
You learn you learn a kind of mapping between or a function that takes in the situation you're in and outputs a good

96
00:07:02,780 --> 00:07:04,780
action to take

97
00:07:05,660 --> 00:07:10,140
There's also kind of an interesting trade-off there which I think we may have talked about before about

98
00:07:10,860 --> 00:07:13,440
exploration versus exploitation in that

99
00:07:14,820 --> 00:07:18,260
you want your agent to be generally taking good actions, but

100
00:07:19,020 --> 00:07:25,980
You don't want it to always take the action that it thinks is best right now because it's understanding may be incomplete and then it

101
00:07:25,980 --> 00:07:30,820
Just kind of gets stuck right? It never finds out anything. It never finds out anything about other

102
00:07:31,980 --> 00:07:35,380
Options that it could have gone with because as soon as it did something that kind of worked

103
00:07:35,380 --> 00:07:39,540
It just goes with that forever. So a lot of these systems build in some

104
00:07:40,340 --> 00:07:43,340
Some variance some randomness or something, right?

105
00:07:43,340 --> 00:07:49,460
Exactly, like you usually do the thing you think is best but some small percentage of the time you just try something random anyway

106
00:07:51,100 --> 00:07:54,940
And you can change that over time like a lot of algorithms as as the

107
00:07:55,500 --> 00:08:00,180
Policy gets more and more as they learn more and more they start doing random stuff less and less

108
00:08:01,140 --> 00:08:06,580
That kind of thing. So that's the like absolute basics of reinforcement learning and how it works and it's really really powerful

109
00:08:07,940 --> 00:08:09,260
like

110
00:08:09,260 --> 00:08:12,820
Especially when you combine it with deep neural networks as the thing that's doing the learning

111
00:08:14,020 --> 00:08:15,660
like

112
00:08:15,660 --> 00:08:19,300
Deep mind did this really amazing thing where I think they were playing pac-man

113
00:08:19,300 --> 00:08:22,860
They were playing a bunch of different Atari games and the thing that's cool about it is

114
00:08:23,660 --> 00:08:29,260
All they told the system was here's what's on the screen and here's the score of the game

115
00:08:29,820 --> 00:08:35,060
Make the score be big the score is your reward, right? That's it and it learned

116
00:08:35,860 --> 00:08:39,340
All of the specific dynamics of the game and generally achieved

117
00:08:39,740 --> 00:08:43,780
Top level human or superhuman play the next word is going to be inverse

118
00:08:43,780 --> 00:08:48,540
We did a thing with who they on anti learning, but can't work all the time. That's what the thing, right?

119
00:08:48,700 --> 00:08:53,640
Yeah, this is not like that. This is a description of a different type of problem

120
00:08:53,640 --> 00:08:56,900
it's it's a totally different problem that they call inverse because

121
00:08:57,740 --> 00:08:59,460
in reinforcement learning

122
00:08:59,460 --> 00:09:04,540
you have a reward function that determines when you what situations you get rewards in and

123
00:09:04,740 --> 00:09:08,260
you're in your environment with your reward function and you're trying to

124
00:09:10,340 --> 00:09:15,540
Find the appropriate actions to take that maximize that reward

125
00:09:16,300 --> 00:09:18,300
in inverse reinforcement learning

126
00:09:18,780 --> 00:09:23,740
You're not in the environment at all. You're watching an expert

127
00:09:23,980 --> 00:09:29,380
so you've got the video of the world championship record pac-man player, right and

128
00:09:29,900 --> 00:09:32,860
You have all of that all of that information you can see

129
00:09:34,220 --> 00:09:36,220
So you're saying?

130
00:09:37,220 --> 00:09:41,620
Rather than rather than having the reward function and trying to figure out the actions

131
00:09:42,260 --> 00:09:46,140
You can see the actions and you're trying to figure out the reward function

132
00:09:46,660 --> 00:09:51,980
So it's inverse because you're kind of solving the reverse of the problem. You're not trying to

133
00:09:52,860 --> 00:09:54,860
maximize a reward

134
00:09:55,220 --> 00:10:00,420
By choosing actions, you're looking at actions and trying to figure out what reward they're maximizing

135
00:10:01,420 --> 00:10:05,540
So that's really useful because it lets you sort of learn by observing experts

136
00:10:05,660 --> 00:10:09,740
So coming back to AI safety, you might think that this would be kind of useful

137
00:10:10,420 --> 00:10:12,480
From an AI safety perspective, you know

138
00:10:12,480 --> 00:10:16,340
You have this problem the core problem of AI safety or one of the core problems of AI safety is

139
00:10:16,820 --> 00:10:19,220
How do you make sure the AI wants what we want?

140
00:10:19,820 --> 00:10:22,700
We can't reliably specify what it is we want

141
00:10:25,180 --> 00:10:29,740
So and if we create something very intelligent that wants something else

142
00:10:30,420 --> 00:10:35,260
That's something else is what's probably going to happen. Even if we don't want that to happen. How do we make a system that?

143
00:10:36,340 --> 00:10:38,380
Reliably wants the same thing we want

144
00:10:39,100 --> 00:10:43,540
so you can see how inverse reinforcement learning might be kind of attractive here because

145
00:10:43,980 --> 00:10:49,380
You might have a system that watches humans doing things and tries to figure out, you know

146
00:10:49,380 --> 00:10:56,780
If we are experts at being humans, it's trying to figure out what rewards we're maximizing and try and sort of formalize in its

147
00:10:57,140 --> 00:11:04,340
In its understanding what it is we want by observing us, that's pretty cool

148
00:11:08,300 --> 00:11:13,020
But yeah, it has some problems one problem is that we don't

149
00:11:15,820 --> 00:11:18,900
In inverse reinforcement learning there's this assumption of

150
00:11:19,820 --> 00:11:21,020
optimality

151
00:11:21,020 --> 00:11:28,420
That the person that the agent you're watching is an expert and they're doing optimal play and you're you know

152
00:11:28,420 --> 00:11:32,380
there is some clear coherent thing like the score that they're optimizing and

153
00:11:33,140 --> 00:11:35,140
the assumption of

154
00:11:35,220 --> 00:11:37,900
The the algorithms that do this is that?

155
00:11:38,460 --> 00:11:46,020
The way the world champion plays is the best possible way and that assumption is obviously never quite true or generally

156
00:11:46,020 --> 00:11:47,620
Not quite true

157
00:11:47,620 --> 00:11:49,620
But it works well enough, you know

158
00:11:50,100 --> 00:11:52,100
But

159
00:11:52,220 --> 00:11:54,220
Humans are not like

160
00:11:55,020 --> 00:11:57,020
Human behavior is not

161
00:11:57,140 --> 00:12:01,100
Actually really optimizing to get what humans want perfectly and ways

162
00:12:01,980 --> 00:12:05,060
Places where that assumption isn't true could cause problems

163
00:12:05,260 --> 00:12:13,220
So is this where cooperative comes in because when we started this we're doing it backwards. It's cooperative inverse reinforcement learning, right?

164
00:12:13,220 --> 00:12:17,340
So you could imagine a situation where you have the robot you have the AGI

165
00:12:17,460 --> 00:12:25,540
It watches people doing their thing uses inverse reinforcement learning to try and figure out the things humans value

166
00:12:26,620 --> 00:12:28,620
Sorry, try and figure out the things humans value

167
00:12:29,220 --> 00:12:30,740
and

168
00:12:30,740 --> 00:12:32,060
then

169
00:12:32,060 --> 00:12:34,580
Adopt those values as its own, right?

170
00:12:35,540 --> 00:12:42,340
The most obvious like the first problem is we don't actually want to create something that values the same thing as humans

171
00:12:43,140 --> 00:12:45,140
like

172
00:12:45,580 --> 00:12:47,580
If it observes that I

173
00:12:48,380 --> 00:12:50,380
You know, I want a cup of tea

174
00:12:50,900 --> 00:12:53,340
We want it to want me to have a cup of tea

175
00:12:53,340 --> 00:12:54,620
We don't want it to want a cup of tea

176
00:12:54,620 --> 00:12:57,420
But that's like if that's quite easy to fix you just say, you know

177
00:12:57,420 --> 00:13:00,020
Figure out what the value is and then optimize it for the humans

178
00:13:01,060 --> 00:13:04,180
Say easy to fix but you know what? I mean, it's that's doable

179
00:13:07,540 --> 00:13:10,180
But then the other thing is if you're if you're trying to teach

180
00:13:10,980 --> 00:13:18,100
If you're actually trying to use this to teach a robot to do something it turns out to not be very efficient

181
00:13:18,740 --> 00:13:20,740
like if you

182
00:13:20,740 --> 00:13:23,860
This works for Pac-man if you want to learn how to be good at Pac-man

183
00:13:23,980 --> 00:13:30,820
You probably want to not just watch the world's best Pac-man player and try to copy them, right?

184
00:13:30,820 --> 00:13:32,860
That's that's not like an efficient way to learn

185
00:13:33,620 --> 00:13:36,300
Because there might be a situation where you used to be you're thinking

186
00:13:36,980 --> 00:13:44,220
What do I do if I find myself stuck in this corner of the maze or whatever and the pros

187
00:13:44,220 --> 00:13:46,460
Never get stuck there. So you have no

188
00:13:47,620 --> 00:13:52,900
you have no example of what to do or all the pro all watching the pros can teach you is

189
00:13:54,220 --> 00:13:56,860
Don't get stuck there. And then once you're there, you've got no

190
00:13:57,500 --> 00:14:01,340
You've got no hope. Let's say I wanted to teach my robot to make me a cup of tea. I

191
00:14:02,220 --> 00:14:04,220
Go into the kitchen and I show it

192
00:14:05,100 --> 00:14:07,100
How I make a cup of tea I

193
00:14:07,900 --> 00:14:10,140
Would probably have to do that a lot of times

194
00:14:10,980 --> 00:14:14,140
to actually get the all the information across

195
00:14:14,940 --> 00:14:16,420
because and

196
00:14:16,420 --> 00:14:21,220
You'll notice this is not how people teach right if you were teaching a person how to make a cup of tea

197
00:14:21,620 --> 00:14:28,660
You might do something like if there's some difficult stage of the process you might show you might do one demonstration

198
00:14:29,580 --> 00:14:33,580
But show that one stage like three times say and you see do it like this

199
00:14:33,580 --> 00:14:40,620
Let me show you that again. And then if you're using inverse reinforcement learning the system believes that you are playing optimally

200
00:14:41,140 --> 00:14:48,060
Right, so it thinks that doing it three times is somehow necessary and it's trying to figure out what values like what reward you must

201
00:14:48,060 --> 00:14:50,100
Be optimizing but doing it three times is

202
00:14:50,700 --> 00:14:52,700
important

203
00:14:53,220 --> 00:14:57,680
So that's a problem right that's where the assumption isn't true or you might want to say

204
00:14:58,660 --> 00:15:03,100
okay, what you do is you get the tea out of the box here and you put it in the thing, but if

205
00:15:03,900 --> 00:15:10,140
There's none in this box. You go over to this cupboard where we keep the backup supplies and you open a new box, right?

206
00:15:10,140 --> 00:15:12,060
But you can't show that

207
00:15:12,060 --> 00:15:13,580
the only way that

208
00:15:13,580 --> 00:15:15,580
The only way that the robot can learn

209
00:15:16,060 --> 00:15:19,260
To go and get the extra supplies only when this one has run out

210
00:15:19,540 --> 00:15:22,740
Is if you were in a situation where that would be optimal play

211
00:15:22,740 --> 00:15:27,460
So the thing has to be actually run out in order for you to demonstrate that you can't say

212
00:15:27,620 --> 00:15:31,060
If the situation were different from how it is, then you should go and do this

213
00:15:31,180 --> 00:15:36,900
so the other thing you might want if you're trying to teach things efficiently, you might want the AI to be

214
00:15:37,540 --> 00:15:40,540
Taking an active role in the learning process, right?

215
00:15:40,540 --> 00:15:45,420
You kind of want it to be like if there's if there's some aspect of it that it doesn't understand

216
00:15:45,420 --> 00:15:47,180
You don't want it just sitting there

217
00:15:47,180 --> 00:15:49,820
Observing you optimally do the thing and then trying to copy

218
00:15:50,060 --> 00:15:54,480
If there's something that it didn't see you kind of want it to be able to say hang on

219
00:15:54,480 --> 00:16:00,360
I didn't see that, you know, or I'm confused about this or maybe ask you a clarifying question or

220
00:16:02,080 --> 00:16:07,680
Just in general like communicate with you and cooperate with you in the learning process

221
00:16:10,360 --> 00:16:12,360
So

222
00:16:12,760 --> 00:16:19,000
Yeah, so so the way that the way that cooperative inverse reinforcement learning works is it's a way of setting up the rewards

223
00:16:20,000 --> 00:16:22,840
Such that these types of behaviors

224
00:16:23,080 --> 00:16:29,580
Hopefully will be incentivized and should come out automatically if you're optimizing it, you know, if the AI is doing well

225
00:16:29,640 --> 00:16:35,480
so what you do is you specify the interaction as a cooperative game where the

226
00:16:35,960 --> 00:16:37,960
robots reward function is

227
00:16:38,160 --> 00:16:40,160
the humans reward function

228
00:16:40,560 --> 00:16:41,560
but

229
00:16:41,560 --> 00:16:46,080
The robot doesn't know that reward function at all. It never knows

230
00:16:46,720 --> 00:16:52,000
The reward that it gets and it never knows the function that generates the reward that it gets

231
00:16:52,880 --> 00:16:55,840
It just knows that it's the same as the humans

232
00:16:56,960 --> 00:17:01,240
So it's trying to optimize. It's trying to maximize the reward it gets

233
00:17:01,920 --> 00:17:03,920
but the only

234
00:17:04,240 --> 00:17:08,320
clues it has for what it needs to do to maximize its own reward is

235
00:17:09,460 --> 00:17:14,080
Observing the human and trying to figure out what the human is trying to maximize

236
00:17:14,200 --> 00:17:18,120
This is a bit like two players on a computer game, but you can only see one school

237
00:17:18,520 --> 00:17:21,920
Yeah, like if you're you're you're both on the same team. Yeah

238
00:17:23,120 --> 00:17:26,440
But only the human knows the rules of the game

239
00:17:27,040 --> 00:17:29,040
Effectively you both want

240
00:17:29,400 --> 00:17:33,120
You both get the same reward. So you both want the same thing just kind of by definition

241
00:17:33,880 --> 00:17:40,280
but the process so in a sense you've kind of just defined the core problem of as I was saying that the core problem one

242
00:17:40,280 --> 00:17:42,280
of the core problems of AI safety is

243
00:17:42,560 --> 00:17:49,000
How do you make sure that the robot wants what the human wants and in this case you've just specified it usually you couldn't do

244
00:17:49,000 --> 00:17:51,200
That because we don't really know what the human wants either

245
00:17:51,880 --> 00:17:55,800
Two people who don't speak the same language can still communicate

246
00:17:56,360 --> 00:18:02,480
With actions and gestures and things. Yeah, and you can generally get the gist of the idea across to the other person

247
00:18:02,600 --> 00:18:07,440
Is it a bit like that? Yeah, but a sufficiently sophisticated agent

248
00:18:09,040 --> 00:18:11,720
If you have an AGI that could be quite powerful

249
00:18:12,280 --> 00:18:16,520
It can speak, you know, and it can understand language and everything else and it knows

250
00:18:17,320 --> 00:18:18,680
that

251
00:18:18,680 --> 00:18:20,680
That so it knows for example

252
00:18:21,520 --> 00:18:23,760
Hopefully it should be able to figure out that

253
00:18:24,400 --> 00:18:27,120
When the human is showing something three times

254
00:18:27,680 --> 00:18:33,480
That it's that the human is doing that in order to communicate information and not because it's the optimal way to do it

255
00:18:34,000 --> 00:18:36,000
because it knows

256
00:18:36,240 --> 00:18:38,240
that the human knows

257
00:18:38,720 --> 00:18:41,200
There's that kind of there's common knowledge of

258
00:18:41,880 --> 00:18:43,880
What's going on in this in the scenario?

259
00:18:43,960 --> 00:18:49,280
so it allows for situations where the human is just demonstrating something or

260
00:18:50,400 --> 00:18:55,680
Explaining something or it allows the AI to ask about things that it's unclear about

261
00:18:55,960 --> 00:19:00,640
Because everybody's on the same team trying to achieve the same thing in principle

262
00:19:01,320 --> 00:19:03,320
So

263
00:19:04,120 --> 00:19:08,120
The point is if you have a big red stop button in this scenario

264
00:19:08,960 --> 00:19:10,960
The AI is not

265
00:19:11,120 --> 00:19:14,240
incentivized to disable or ignore that stop button

266
00:19:15,080 --> 00:19:16,800
because

267
00:19:16,800 --> 00:19:20,240
It constitutes important information about its reward

268
00:19:21,200 --> 00:19:26,960
right, the AI is desperately trying to maximize a reward function that it doesn't know and

269
00:19:27,640 --> 00:19:30,480
So if it observes the human trying to hit the stop button

270
00:19:31,240 --> 00:19:37,160
That provides really strong information that what it's doing right now is not going to maximize the humans reward

271
00:19:37,160 --> 00:19:39,160
Which means it's not going to maximize its own reward

272
00:19:39,360 --> 00:19:41,120
so it wants

273
00:19:41,120 --> 00:19:44,320
To allow itself to be shut off if the human wants to shut it off

274
00:19:45,000 --> 00:19:47,000
Because it's for its own good

275
00:19:47,760 --> 00:19:52,440
So this is this is a clever way of aligning its interests with ours, right, right?

276
00:19:53,360 --> 00:19:55,360
It's not so it's so like

277
00:19:55,600 --> 00:20:01,360
The the the problem in the in the default situation is I've told it to get a cup of tea and it's going to do

278
00:20:01,360 --> 00:20:04,880
That whatever else I do and if I tried to turn it off

279
00:20:04,880 --> 00:20:08,240
It's not going to let me because that will stop it from getting a cup of tea

280
00:20:08,560 --> 00:20:14,160
whereas in this situation the fact that I want a cup of tea is something it's not completely sure of and

281
00:20:15,320 --> 00:20:17,920
So it doesn't think it knows better than me

282
00:20:18,920 --> 00:20:21,800
So when I go to hit that stop button it thinks oh

283
00:20:21,800 --> 00:20:26,560
I thought I was supposed to be going over here and getting a cup of tea and running over this baby or whatever

284
00:20:27,240 --> 00:20:30,600
But the fact that he's rushing to hit the button means I must have gotten something wrong

285
00:20:31,360 --> 00:20:37,760
So I'd better stop and learn more about this situation because I'm at risk of losing a bunch of reward

286
00:20:41,240 --> 00:20:45,520
So, yeah, it seems like it seems like a potentially workable thing

287
00:20:46,520 --> 00:20:48,520
a workable approach

288
00:20:49,200 --> 00:20:53,400
So one interesting thing about this is there is still an assumption

289
00:20:54,280 --> 00:21:01,560
that the humans behavior is in accordance with some utility function or some reward function some objective function like

290
00:21:02,080 --> 00:21:05,000
if the human behaves very irrationally

291
00:21:05,920 --> 00:21:07,920
that can

292
00:21:08,400 --> 00:21:11,620
Cause problems for this system because the whole thing

293
00:21:12,220 --> 00:21:18,620
Revolves around the fact that the robot is not completely confident of what its reward is

294
00:21:18,620 --> 00:21:23,180
It's got a model of its of what the reward function is like that. It's constantly updating as it learns

295
00:21:24,060 --> 00:21:28,220
And it doesn't have full confidence and it's using the human as the source of information

296
00:21:28,940 --> 00:21:35,860
So fundamentally the robot believes that the human knows better than it does how to maximize the humans reward

297
00:21:36,620 --> 00:21:38,980
So in situations where that's not true

298
00:21:39,900 --> 00:21:43,100
like if you run this for long enough and the

299
00:21:45,060 --> 00:21:52,180
Robot managed to build up a really really high level of confidence in what it thinks the human reward function is

300
00:21:52,740 --> 00:21:57,100
Then it might ignore its stop button later on if it thinks that

301
00:21:57,780 --> 00:22:00,300
It knows better than the human what the human wants

302
00:22:01,820 --> 00:22:06,660
Which sounds very scary but might actually be what you want to happen

303
00:22:07,380 --> 00:22:09,380
Like if you imagine

304
00:22:09,540 --> 00:22:14,020
You know, it's the it's the future and we've got these robots and they all have a big red stop button on them

305
00:22:14,020 --> 00:22:15,780
and they're all you know, and

306
00:22:15,780 --> 00:22:19,920
Everything's wonderful and you say to your robot. I'll take my

307
00:22:20,940 --> 00:22:27,060
My four-year-old son to school, you know drive him to school in the car because it's the 1950s sci-fi future

308
00:22:27,060 --> 00:22:31,420
Where it's not self-driving cars. It's like robots in cars anyway, and it's

309
00:22:32,340 --> 00:22:34,340
It's driving this kid to school

310
00:22:34,500 --> 00:22:39,300
It's doing 70 on the motorway and the kid sees the big red shiny button and smacks it

311
00:22:39,660 --> 00:22:43,580
right in principle a human has just pressed the button and a

312
00:22:44,260 --> 00:22:50,660
Lot of designs for a button would just say a human is hit your button. You have to stop whereas this design might say I

313
00:22:51,180 --> 00:22:57,220
Have been around for a long time. I've learned a lot about what humans value and also I observed that this specific human

314
00:22:57,620 --> 00:23:00,820
Does not reliably behave in its own best interests

315
00:23:02,340 --> 00:23:08,020
So maybe this hitting the button is not communicating to me information about what this human really wants

316
00:23:09,060 --> 00:23:12,820
They're just hitting it because it's a big red button and I should not shut myself off

317
00:23:13,620 --> 00:23:18,240
So it has the potential to be safer than a button that always works

318
00:23:19,460 --> 00:23:21,460
But it's a little bit unsettling

319
00:23:21,500 --> 00:23:26,860
That you might end up with systems that sometimes actually do ignore the shutdown command because they think they know better

320
00:23:27,780 --> 00:23:31,380
Because what it's looking at right now is it says button gets hit I get zero reward

321
00:23:32,620 --> 00:23:37,980
Button doesn't get hit if I manage to stop them. Then I get the cup of tea. I get like maximum reward

322
00:23:40,460 --> 00:23:42,460
If you give some sort of company

