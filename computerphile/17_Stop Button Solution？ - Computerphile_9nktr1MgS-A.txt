While back we were talking about The stop button problem, right? You have this you have this It's kind of a toy problem in AI safety. You have an artificial general intelligence in a robot it wants something, you know, it wants to make you a cup of tea or whatever you put a big red stop button on it and You want to set it up so that it behaves courageably that it will allow you to hit the button It won't hit the button itself, you know, and it won't try and prevent you this sort of Behaving in a In a sensible way in a safe way and That like by default Most AGI designs will not behave this way Well, we left it as an open problem, right and it kind of still is an open problem But there have been some really interesting things proposed as possible solutions or approaches to take and I wanted to talk about cooperative inverse reinforcement learning I I Thought the easiest way to explain a cooperative inverse reinforcement learning is to build it up backwards, right? learning we know like machine learning and Reinforcement learning is an area of machine learning. I guess you could call it. It's it's kind of a It's a way of presenting a problem in most machine learning The kind of thing that people have talked about already a lot on computer file Thinking of who has videos and the related ones Usually you get in some data and then you're trying to do something with that like classify You know unseen things or you're trying to do like regression to find out what value something would have for a certain inputs that kind of thing whereas reinforcement learning the idea is you have an agent in an environment and you're trying to find a Policy but so so Let's back up. What do we mean by an agent? It's an entity that interacts with its environment To try and achieve something effectively. It's doing things in an environment So this isn't that's really if is this a physical thing or is it me doesn't have to be so if you have Robot in a room then you can model that as the robot being an agent and the room being environment. Similarly if you have a computer game like Pac-man Then Pac-man is an agent and the sort of maze he's in is his environment So let's stick with Pac-man that the way that a reinforcement learning Framework for dealing with Pac-man is you say? Okay, you've got Pac-man. He's the agent. He's in the environment and you have Actions the Pac-man can take in the environment now, it's kind of neat in Pac-man There are always exactly four actions you can take or well I guess five you can sit there and do nothing. You can move up left right or down You don't always have all of those options like sometimes there's a wall and you can't move, right? but those are the only that's the that's the complete set of actions that you have and Then you have the environment contains sort of dots that you can pick up Which are they give you points? It's got these ghosts that chase you that you don't want to touch and I think there's also there's like pills you can pick Up that make the ghosts edible and then you chase them down and stuff Anyway, so the difference in reinforcement learning is that the agent is in the environment and it learns by interacting with the environment It's and so it's kind of close to the way that animals learn and the way that humans learn You try you try doing something, you know, I'm gonna try, you know touching this fire. Oh that hurt So that's that's caused me like a negative reward. That's caused me a pain signal, which is something I don't want so I learn to avoid doing things like touching a fire so in in a pac-man environment, you might you might sort of say If you're in a you're in a situation like let's draw pac-man. Let's say he's in a maze like this You look at pac-man's options He can't go left. He can't go right he can go up and if he goes up He'll get a dot which earns you some points So up gets a score of you know, plus 10 or however, you've decided it Or well, whatever the score is in the game either way or if he goes down he'll be immediately got by this ghost The point is that pac-man doesn't need to be aware of the entire board right of the entire maze you can just feed in a fairly small amount of information about his immediate environment, which is the same thing as If you have a robot in a room, it can't it doesn't know everything about the whole room it can only see what it sees through its camera, you know, it has Sensors they give it some some information about the environment partial information I Suppose just playing devil's advocate. The difference here is usually pac-man is being controlled by a human who can see the whole board So the point being if that ghost is actually not static and is chasing pac-man and he's heading up to get that pill if If a few pixels later that that corridor if you like stops in a dead end Yep, well, he's kind of stuffed either way really that's true. Yeah, so that is because so so most Well, yeah, almost every Reinforcement learning algorithm almost everything that tries to deal with this problem Doesn't just look at the immediate surroundings or it looks at the immediate surroundings, but it also looks a certain distance in time So you're not just saying what's going to happen next frame, but So like if you if you go down Here most algorithms would say okay the option of going down in this situation is bad But also all of the options we chose in all of the situations that we were in in the last Second or two also get a little bit. This is kind of a decay. There's time time discounting so that You're not just punishing the immediate thing that causes the negative reward But also the decisions you make leading up to it so that pac-man might learn not to get himself stuck in corners As well as just learning not to run straight into ghosts so That's the basics of reinforcement learning. There's different algorithms that do it and the idea is you You actually you start off exploring the environment just at random You just pick completely random actions and then as those actions start having consequences for you and you start getting rewards and punishments You start to learn Which actions are better to use in which situations does that mean that in pac-man's case would learn the maze Or would it just learn the better choices? It depends on what algorithm you're using Very sophisticated one might learn the whole maze a simpler one might just learn More kind of local policy But The point is yeah You learn you learn a kind of mapping between or a function that takes in the situation you're in and outputs a good action to take There's also kind of an interesting trade-off there which I think we may have talked about before about exploration versus exploitation in that you want your agent to be generally taking good actions, but You don't want it to always take the action that it thinks is best right now because it's understanding may be incomplete and then it Just kind of gets stuck right? It never finds out anything. It never finds out anything about other Options that it could have gone with because as soon as it did something that kind of worked It just goes with that forever. So a lot of these systems build in some Some variance some randomness or something, right? Exactly, like you usually do the thing you think is best but some small percentage of the time you just try something random anyway And you can change that over time like a lot of algorithms as as the Policy gets more and more as they learn more and more they start doing random stuff less and less That kind of thing. So that's the like absolute basics of reinforcement learning and how it works and it's really really powerful like Especially when you combine it with deep neural networks as the thing that's doing the learning like Deep mind did this really amazing thing where I think they were playing pac-man They were playing a bunch of different Atari games and the thing that's cool about it is All they told the system was here's what's on the screen and here's the score of the game Make the score be big the score is your reward, right? That's it and it learned All of the specific dynamics of the game and generally achieved Top level human or superhuman play the next word is going to be inverse We did a thing with who they on anti learning, but can't work all the time. That's what the thing, right? Yeah, this is not like that. This is a description of a different type of problem it's it's a totally different problem that they call inverse because in reinforcement learning you have a reward function that determines when you what situations you get rewards in and you're in your environment with your reward function and you're trying to Find the appropriate actions to take that maximize that reward in inverse reinforcement learning You're not in the environment at all. You're watching an expert so you've got the video of the world championship record pac-man player, right and You have all of that all of that information you can see So you're saying? Rather than rather than having the reward function and trying to figure out the actions You can see the actions and you're trying to figure out the reward function So it's inverse because you're kind of solving the reverse of the problem. You're not trying to maximize a reward By choosing actions, you're looking at actions and trying to figure out what reward they're maximizing So that's really useful because it lets you sort of learn by observing experts So coming back to AI safety, you might think that this would be kind of useful From an AI safety perspective, you know You have this problem the core problem of AI safety or one of the core problems of AI safety is How do you make sure the AI wants what we want? We can't reliably specify what it is we want So and if we create something very intelligent that wants something else That's something else is what's probably going to happen. Even if we don't want that to happen. How do we make a system that? Reliably wants the same thing we want so you can see how inverse reinforcement learning might be kind of attractive here because You might have a system that watches humans doing things and tries to figure out, you know If we are experts at being humans, it's trying to figure out what rewards we're maximizing and try and sort of formalize in its In its understanding what it is we want by observing us, that's pretty cool But yeah, it has some problems one problem is that we don't In inverse reinforcement learning there's this assumption of optimality That the person that the agent you're watching is an expert and they're doing optimal play and you're you know there is some clear coherent thing like the score that they're optimizing and the assumption of The the algorithms that do this is that? The way the world champion plays is the best possible way and that assumption is obviously never quite true or generally Not quite true But it works well enough, you know But Humans are not like Human behavior is not Actually really optimizing to get what humans want perfectly and ways Places where that assumption isn't true could cause problems So is this where cooperative comes in because when we started this we're doing it backwards. It's cooperative inverse reinforcement learning, right? So you could imagine a situation where you have the robot you have the AGI It watches people doing their thing uses inverse reinforcement learning to try and figure out the things humans value Sorry, try and figure out the things humans value and then Adopt those values as its own, right? The most obvious like the first problem is we don't actually want to create something that values the same thing as humans like If it observes that I You know, I want a cup of tea We want it to want me to have a cup of tea We don't want it to want a cup of tea But that's like if that's quite easy to fix you just say, you know Figure out what the value is and then optimize it for the humans Say easy to fix but you know what? I mean, it's that's doable But then the other thing is if you're if you're trying to teach If you're actually trying to use this to teach a robot to do something it turns out to not be very efficient like if you This works for Pac-man if you want to learn how to be good at Pac-man You probably want to not just watch the world's best Pac-man player and try to copy them, right? That's that's not like an efficient way to learn Because there might be a situation where you used to be you're thinking What do I do if I find myself stuck in this corner of the maze or whatever and the pros Never get stuck there. So you have no you have no example of what to do or all the pro all watching the pros can teach you is Don't get stuck there. And then once you're there, you've got no You've got no hope. Let's say I wanted to teach my robot to make me a cup of tea. I Go into the kitchen and I show it How I make a cup of tea I Would probably have to do that a lot of times to actually get the all the information across because and You'll notice this is not how people teach right if you were teaching a person how to make a cup of tea You might do something like if there's some difficult stage of the process you might show you might do one demonstration But show that one stage like three times say and you see do it like this Let me show you that again. And then if you're using inverse reinforcement learning the system believes that you are playing optimally Right, so it thinks that doing it three times is somehow necessary and it's trying to figure out what values like what reward you must Be optimizing but doing it three times is important So that's a problem right that's where the assumption isn't true or you might want to say okay, what you do is you get the tea out of the box here and you put it in the thing, but if There's none in this box. You go over to this cupboard where we keep the backup supplies and you open a new box, right? But you can't show that the only way that The only way that the robot can learn To go and get the extra supplies only when this one has run out Is if you were in a situation where that would be optimal play So the thing has to be actually run out in order for you to demonstrate that you can't say If the situation were different from how it is, then you should go and do this so the other thing you might want if you're trying to teach things efficiently, you might want the AI to be Taking an active role in the learning process, right? You kind of want it to be like if there's if there's some aspect of it that it doesn't understand You don't want it just sitting there Observing you optimally do the thing and then trying to copy If there's something that it didn't see you kind of want it to be able to say hang on I didn't see that, you know, or I'm confused about this or maybe ask you a clarifying question or Just in general like communicate with you and cooperate with you in the learning process So Yeah, so so the way that the way that cooperative inverse reinforcement learning works is it's a way of setting up the rewards Such that these types of behaviors Hopefully will be incentivized and should come out automatically if you're optimizing it, you know, if the AI is doing well so what you do is you specify the interaction as a cooperative game where the robots reward function is the humans reward function but The robot doesn't know that reward function at all. It never knows The reward that it gets and it never knows the function that generates the reward that it gets It just knows that it's the same as the humans So it's trying to optimize. It's trying to maximize the reward it gets but the only clues it has for what it needs to do to maximize its own reward is Observing the human and trying to figure out what the human is trying to maximize This is a bit like two players on a computer game, but you can only see one school Yeah, like if you're you're you're both on the same team. Yeah But only the human knows the rules of the game Effectively you both want You both get the same reward. So you both want the same thing just kind of by definition but the process so in a sense you've kind of just defined the core problem of as I was saying that the core problem one of the core problems of AI safety is How do you make sure that the robot wants what the human wants and in this case you've just specified it usually you couldn't do That because we don't really know what the human wants either Two people who don't speak the same language can still communicate With actions and gestures and things. Yeah, and you can generally get the gist of the idea across to the other person Is it a bit like that? Yeah, but a sufficiently sophisticated agent If you have an AGI that could be quite powerful It can speak, you know, and it can understand language and everything else and it knows that That so it knows for example Hopefully it should be able to figure out that When the human is showing something three times That it's that the human is doing that in order to communicate information and not because it's the optimal way to do it because it knows that the human knows There's that kind of there's common knowledge of What's going on in this in the scenario? so it allows for situations where the human is just demonstrating something or Explaining something or it allows the AI to ask about things that it's unclear about Because everybody's on the same team trying to achieve the same thing in principle So The point is if you have a big red stop button in this scenario The AI is not incentivized to disable or ignore that stop button because It constitutes important information about its reward right, the AI is desperately trying to maximize a reward function that it doesn't know and So if it observes the human trying to hit the stop button That provides really strong information that what it's doing right now is not going to maximize the humans reward Which means it's not going to maximize its own reward so it wants To allow itself to be shut off if the human wants to shut it off Because it's for its own good So this is this is a clever way of aligning its interests with ours, right, right? It's not so it's so like The the the problem in the in the default situation is I've told it to get a cup of tea and it's going to do That whatever else I do and if I tried to turn it off It's not going to let me because that will stop it from getting a cup of tea whereas in this situation the fact that I want a cup of tea is something it's not completely sure of and So it doesn't think it knows better than me So when I go to hit that stop button it thinks oh I thought I was supposed to be going over here and getting a cup of tea and running over this baby or whatever But the fact that he's rushing to hit the button means I must have gotten something wrong So I'd better stop and learn more about this situation because I'm at risk of losing a bunch of reward So, yeah, it seems like it seems like a potentially workable thing a workable approach So one interesting thing about this is there is still an assumption that the humans behavior is in accordance with some utility function or some reward function some objective function like if the human behaves very irrationally that can Cause problems for this system because the whole thing Revolves around the fact that the robot is not completely confident of what its reward is It's got a model of its of what the reward function is like that. It's constantly updating as it learns And it doesn't have full confidence and it's using the human as the source of information So fundamentally the robot believes that the human knows better than it does how to maximize the humans reward So in situations where that's not true like if you run this for long enough and the Robot managed to build up a really really high level of confidence in what it thinks the human reward function is Then it might ignore its stop button later on if it thinks that It knows better than the human what the human wants Which sounds very scary but might actually be what you want to happen Like if you imagine You know, it's the it's the future and we've got these robots and they all have a big red stop button on them and they're all you know, and Everything's wonderful and you say to your robot. I'll take my My four-year-old son to school, you know drive him to school in the car because it's the 1950s sci-fi future Where it's not self-driving cars. It's like robots in cars anyway, and it's It's driving this kid to school It's doing 70 on the motorway and the kid sees the big red shiny button and smacks it right in principle a human has just pressed the button and a Lot of designs for a button would just say a human is hit your button. You have to stop whereas this design might say I Have been around for a long time. I've learned a lot about what humans value and also I observed that this specific human Does not reliably behave in its own best interests So maybe this hitting the button is not communicating to me information about what this human really wants They're just hitting it because it's a big red button and I should not shut myself off So it has the potential to be safer than a button that always works But it's a little bit unsettling That you might end up with systems that sometimes actually do ignore the shutdown command because they think they know better Because what it's looking at right now is it says button gets hit I get zero reward Button doesn't get hit if I manage to stop them. Then I get the cup of tea. I get like maximum reward If you give some sort of company 