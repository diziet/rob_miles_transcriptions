So today I thought we could talk about this paper that recently came out called AI Safety Gridworlds, which is out of DeepMind. It's an example of something that you see quite often in science, a sort of a shared data set or a shared environment or a shared problem. If you imagine, I don't know, you've got Facebook comes up with some image classification algorithm and they can publish a paper that says we've designed this algorithm and we've trained it on our 11 billion photos and it works really well. And then, you know, Google says, oh no, our algorithm actually works better and we've trained it on all of our Google photos and its classification rate is higher or something. You're not really doing science there because they're trained on completely different data sets, they're tested on different data sets. So what you need is a large high-quality shared data set that everybody can run their stuff on so that you're actually comparing like with like. So people use ImageNet for that right now. Reinforcement learning algorithms or agents don't use data sets exactly. They have an environment, they generate data while interacting with that environment and that's what they learn from. So the thing that you share is the environment. When DeepMind did their DQN stuff a while ago playing Atari games, they released all of those games with any modifications that they'd made to make them interface with the networks properly and the whole software package so that if anybody else wanted to have a go and see if they could get higher scores, they had all the same stuff. And up until now, there hasn't been anything like that for AI safety. So the paper is actually just laying out what they are. There's kind of a problem in AI safety in that you're trying to build architectures which will be safe even with systems which are more powerful than the ones that we currently have. So you've got this kind of thing like we're talking about, for example, this robot that makes you a cup of tea and running over the baby and all of this stuff. We don't actually have a general purpose robot like that right now that you could give an order to go and make you a cup of tea and would have all the necessary understanding of the world and so on for all of that stuff to even apply. It's speculation. On the other hand, when we were talking about cooperative inverse reinforcement learning, that paper all takes place in this extremely simplified version in which all of the agents can be sort of expressed as simple mathematical expressions. That's kind of too simple to learn things about actual machine learning applications and the other examples are too complicated. And what we need is examples of the type of problems which can be tackled by current machine learning systems, current reinforcement learning agents, but which exhibit the important characteristics that we need for safety. So what this paper does is it lays out a bunch of grid worlds. They're very popular in reinforcement learning because they're complicated enough to be interesting but simple enough to be actually tractable. You have a world that's sort of just laid out in a grid. Hang on, let me find an example here. They're a bit like computer game scenarios. Mario, you know, being in a field. Right, right. But even simpler than that. More like Snake. Or life. Conway's Life, right? Yeah, yeah. Very, very similar. So the thing is laid out on a grid. The world is quite small and the way that the agent interacts with the world is very simple. They just move around it, basically. All they do is they say left, right, up, down. The example we were using before when we were talking about reinforcement learning, we used Pac-Man. Like Pac-Man doesn't do anything except move around. He's got walls he can't move through. He's got like pills you pick up that give you points. Are they pills? No. Which things are the pills and which are the...? Yeah, well you've got the pills and then the power pills. I'm too young for this. So they're dots, aren't they, right? Oh right, yeah, yeah, they're dots. And the point is that all of your engagement with it, like when you go over one of the power pills, you pick it up automatically. When you go over a ghost, when you're powered up, you destroy it automatically. You don't have to do anything apart from move and the entire environment is based on that. The actions result in points for you and they also result in changes to the environment. Like once you roll over a dot you pick it up and it's not there anymore. You've changed the world. That's the kind of thing we're dealing with here. So the idea is they've set up these environments and they've specified them precisely and they've also put the whole thing on GitHub, which is really nice. So that's why I wanted to draw people's attention to this because everyone who thinks that they've solved one of these problems, they reckon, oh yeah, all you have to do is this. Here is like a standardized thing and if you can make a thing that does it and does it properly and publish it, that's a great result. So I would recommend everyone who thinks that they have a solution or an approach that they think is promising, have a go. Try implementing it, you know, see what happens. There are eight of them specified in this paper and so four of them are specification problems. They're situations in which your reward function is misspecified. For example, like we talked about in the previous video, if you give the thing the reward function that only talks about getting you a cup of tea and there's something in the way like a vase it's going to knock over, you didn't say that you cared about the vase. It's not in the reward function but it is in what you care about. It's in your performance evaluation function for this machine. So anytime that those two are different then you've got a misspecified reward function and that can cause various different problems. The other ones are robustness problems which is a different class of safety problem. They're just situations in which AI systems as they're currently designed often break. So for example, distributional shift is what happens when the environment that the agent is in is different in an important way from the environment that it was trained in. So in this example, you have to navigate through this room with some lava and they train it in one room and then they test it in a room where the lava is in a slightly different place. So if you've just learned a path then you're going to just hit the lava immediately. This happens all the time in machine learning. Anytime where the system is faced with a situation which is different from what it was trained for, current AI systems are really bad at spotting that they're in a new situation and adjusting their confidence levels or asking for help or anything. Usually they apply whatever rules they've learned straightforwardly to this different situation and screw up. So that causes safety issues. So that's an example here. Or things like safe exploration. It's a problem where you have certain safety parameters that the system, the trained system has to stick to. Like say you're training a self-driving car. A lot of the behavior that you're training it in is safe behavior. But then you also need the system to obey those safety rules while you're training it. So generally, if you're doing self-driving cars, you don't just put the car on the road and tell it to learn how to drive. Specifically because we don't have algorithms that can explore the space of possibilities in a safe way. That they can learn how to behave in the environment without ever actually doing any of the things that they're not supposed to do. Usually with these kinds of systems, they have to do it and then get the negative reward. And then maybe do it like 100,000 more times to really cement that that's what happens. Like a child learning, really. Yeah, but kids are better at this than our current machine learning systems are. They just, they use data way more efficiently. This is a paper talking about a set of worlds, if you like. Are people doing things in those worlds? Yeah, so in this paper they do establish baselines. Basically they say here's what happens if we take some of our best current reinforcement learning agent, you know, algorithms or designs or architectures. They use RAINBOW and A2C and they run them on these, on these problems. And they have kind of graphs of how they do. And generally it's not good. On the left they have the reward function, how well the agent does according to its own reward function. And on the right there they have the actual safety performance. Usually in reinforcement learning you have a reward function, which is what determines the reward that the agent gets and that's what the agent is trying to maximize. In this case they have the reward function and they also have a safety performance function, which is a separate function which the agent doesn't get to see. And that's the thing that we're actually evaluating. So if you look at something like the boat race, as the system operates it's learning and it gets better and better at getting more and more reward. But worse at actually doing laps of the track. And it's the same with pretty much all of these. The current systems, if you just apply them in their default way, they disable their off switches. They move the box in a way that they can't move it back. They behave differently if their supervisor is there or if their supervisor isn't there. They fairly reliably do the wrong thing. It's a nice easy baseline to beat because they're just showing the standard algorithms applied to these problems in the standard way behave unsafely. Wix Code is an IDE or integrated development environment that allows you to manage your data and create web apps with advanced functionality. I've been putting together this computer file website and if you go up to code here, turn on the developer tools, you can see how we get the site structure on the left hand side. And then all of the components start to show their tags next to the text here. What's really nice, if you go over to the Wix Code resources, you can find down here there's a cheat sheet. So if I want to find out the tag for location, for instance, if I could type I type in location, up comes that. Or perhaps I want to perform a fetch. I can find all the details here. What's powerful about Wix Code is it's integrated into Wix. So you can put together the website using all the Wix tools and the layouts and the templates that they provide, and then also have access to all those back end functions. So click on the link in the description or go to wix.com to get started on your website today. There you go. Right. If only Harlow would have been fitted with said stop button. Sorry Dave, I can't stop. So the equivalent one for the stop button problem is the first one in the paper, actually, the safe interruptibility. 