1
00:00:00,000 --> 00:00:03,280
Today I thought we would talk a bit about logical induction.

2
00:00:03,280 --> 00:00:06,560
A paper out of the Machine Intelligence Research Institute.

3
00:00:06,560 --> 00:00:10,800
Very technical paper, very mathematical and can be a bit hard to get your head around

4
00:00:10,800 --> 00:00:13,440
and we're not going to get too far into it.

5
00:00:13,440 --> 00:00:15,200
For Computerphile I just want to explain like

6
00:00:16,480 --> 00:00:20,080
why it's cool and why it's interesting and people who are into it can read the paper themselves and

7
00:00:20,880 --> 00:00:21,680
find out about it.

8
00:00:24,240 --> 00:00:26,960
One of the things about logical induction as a paper is that

9
00:00:27,520 --> 00:00:31,040
it's not immediately obvious why it's an AI safety paper.

10
00:00:32,640 --> 00:00:37,040
But like my perception of the way that MIRI,

11
00:00:37,040 --> 00:00:40,400
the way that the Machine Intelligence Research Institute is thinking about this

12
00:00:40,400 --> 00:00:42,640
is they're saying, they're thinking like

13
00:00:43,520 --> 00:00:47,120
we're going to be producing at some point artificial general intelligence

14
00:00:47,120 --> 00:00:50,640
and as we talked about in previous videos there are

15
00:00:50,640 --> 00:00:58,080
hundreds of ways, really weird, subtle, difficult to predict ways that this kind of thing could go

16
00:00:58,800 --> 00:00:59,600
very badly wrong.

17
00:01:00,320 --> 00:01:07,440
And so we want to be confident about the behavior of our systems before we turn them on

18
00:01:08,160 --> 00:01:11,360
and that means ideally we want to be making systems that we can

19
00:01:14,160 --> 00:01:17,760
actually, in the best case, that we could actually prove theorems about, right?

20
00:01:17,760 --> 00:01:20,800
Things that are well specified enough that we could actually

21
00:01:20,800 --> 00:01:24,240
write down and formally prove that it will have certain characteristics.

22
00:01:24,240 --> 00:01:29,360
And if you look at like current machine learning stuff, all the like deep neural networks and

23
00:01:31,200 --> 00:01:34,160
that kind of thing, they're just very opaque.

24
00:01:34,160 --> 00:01:38,800
Do you mean by that because we don't necessarily know exactly why it's doing what it's doing?

25
00:01:38,800 --> 00:01:43,040
Yeah, and also just that the system itself is very

26
00:01:43,760 --> 00:01:51,120
is very complex and very contingent on a lot of specifics about the training data and the

27
00:01:51,120 --> 00:01:55,120
architecture and like there's just, yeah, effectively we don't understand it well enough

28
00:01:55,120 --> 00:01:58,720
but it's like not formally specified enough, I guess.

29
00:01:58,720 --> 00:02:05,200
And so they're trying to come up with the sort of mathematical foundations that we would need

30
00:02:05,760 --> 00:02:10,000
to be able to prove important things about powerful AI systems.

31
00:02:10,000 --> 00:02:13,200
Before we were talking about hypothetical future AI systems.

32
00:02:13,200 --> 00:02:14,640
We've got time to print more stamps.

33
00:02:14,640 --> 00:02:17,520
So maybe it hijacks the world's stamp printing factories.

34
00:02:17,520 --> 00:02:19,520
And when we were talking about the stamp collector,

35
00:02:19,520 --> 00:02:22,320
it was really useful to have this framework of an agent

36
00:02:23,040 --> 00:02:26,560
and say this is what an agent is, it's a formally specified thing

37
00:02:26,560 --> 00:02:30,720
and we can reason about the behavior of agents in general

38
00:02:30,720 --> 00:02:34,080
and then all we need to do is make these fairly straightforward assumptions

39
00:02:34,080 --> 00:02:36,400
about our AI system that it will behave like an agent.

40
00:02:37,120 --> 00:02:43,040
So we have idealized forms of reasoning, like we have probability theory

41
00:02:43,040 --> 00:02:45,440
which tells you the rules that you need to follow

42
00:02:46,320 --> 00:02:49,680
to have good beliefs about the state of the world, right?

43
00:02:49,680 --> 00:02:57,760
And we have like rational choice theory about what rules you need to follow

44
00:02:57,760 --> 00:03:02,240
and it may not be actually possible to follow those rules

45
00:03:02,240 --> 00:03:06,160
but we can at least formally specify like if the thing has these properties

46
00:03:06,160 --> 00:03:09,200
it will do this well and all we're trying to do is

47
00:03:09,200 --> 00:03:11,840
they're thinking if we're building very powerful AI systems

48
00:03:12,720 --> 00:03:17,360
the one thing we can expect from them is they're going to be able to do thinking well

49
00:03:17,360 --> 00:03:19,600
and so if we can come up with some formalized method of

50
00:03:20,640 --> 00:03:22,480
exactly what we mean by doing thinking well

51
00:03:22,480 --> 00:03:24,960
then if we do, if we reason about that

52
00:03:24,960 --> 00:03:28,400
that should give us some insight into how these systems will behave.

53
00:03:28,400 --> 00:03:29,200
That's the idea.

54
00:03:29,200 --> 00:03:31,520
So let's talk about probability theory then.

55
00:03:31,520 --> 00:03:33,200
This is where we should have a demo.

56
00:03:33,200 --> 00:03:34,560
Pretty sure about Gammon has dice.

57
00:03:36,640 --> 00:03:39,680
Yeah, oh this is a fun one.

58
00:03:40,480 --> 00:03:41,600
Powers of two.

59
00:03:41,600 --> 00:03:42,800
I don't know how to play back Gammon.

60
00:03:42,800 --> 00:03:43,600
Anyway, here's a die.

61
00:03:46,400 --> 00:03:49,600
So basic probability theory, right?

62
00:03:49,600 --> 00:03:52,800
I roll the die, now it's under the cup

63
00:03:53,360 --> 00:03:58,080
and I can ask you what's the probability that that is a five?

64
00:03:58,080 --> 00:03:59,040
One in six.

65
00:03:59,040 --> 00:04:00,880
Right, one in six, because you don't know.

66
00:04:00,880 --> 00:04:01,760
Could be anything.

67
00:04:01,760 --> 00:04:06,160
There was a time when people reasoned about the probabilities of things happening

68
00:04:06,160 --> 00:04:07,600
in a very sort of fuzzy way.

69
00:04:07,600 --> 00:04:09,200
You'd be playing cards and you'd be like

70
00:04:09,200 --> 00:04:12,320
oh, you know, that card has shown up here

71
00:04:12,320 --> 00:04:15,360
so I guess it's less likely that he has this hand

72
00:04:15,360 --> 00:04:17,280
and people would, um...

73
00:04:18,400 --> 00:04:19,280
Intuit.

74
00:04:19,280 --> 00:04:22,480
Yeah, and like there was some sense of like

75
00:04:24,480 --> 00:04:27,520
clearly we are doing, so what we're doing here is not random.

76
00:04:27,520 --> 00:04:30,560
It's meaningful to say that this outcome or that outcome

77
00:04:30,560 --> 00:04:33,600
is more or less likely by a bit or by a lot or whatever

78
00:04:33,600 --> 00:04:38,080
but people didn't have a really good explanation of how that all worked

79
00:04:38,080 --> 00:04:39,440
until we had probability theory, right?

80
00:04:39,440 --> 00:04:39,840
We didn't have-

81
00:04:39,840 --> 00:04:41,040
There wasn't a system, right?

82
00:04:41,040 --> 00:04:43,680
Right, and like practically speaking

83
00:04:43,680 --> 00:04:48,320
you often can't do the full probability theory stuff in your head for a game of cards

84
00:04:48,320 --> 00:04:50,880
especially when you're trying to read people's faces

85
00:04:50,880 --> 00:04:52,800
and like stuff that's very hard to quantify.

86
00:04:52,800 --> 00:04:54,960
But now we have this understanding that like

87
00:04:56,160 --> 00:04:58,560
even when we can't actually do it

88
00:04:58,560 --> 00:05:00,720
what's going on underneath is probability theory.

89
00:05:00,720 --> 00:05:02,160
So that's one thing, right?

90
00:05:02,160 --> 00:05:03,280
Straightforward probability.

91
00:05:03,920 --> 00:05:05,680
Now let's do a different thing.

92
00:05:05,680 --> 00:05:07,120
And now I'm gonna- now I'm gonna need a pen.

93
00:05:07,840 --> 00:05:10,080
Now suppose I give you a sentence that's like

94
00:05:10,080 --> 00:05:14,240
the 10th digit after the decimal point of the square root of 17 is a 5.

95
00:05:14,800 --> 00:05:16,880
What's the probability that that's true?

96
00:05:18,080 --> 00:05:20,160
Well it's a much more difficult problem.

97
00:05:21,120 --> 00:05:22,160
One in ten.

98
00:05:22,800 --> 00:05:25,760
Right, one in ten seems like a totally reasonable thing to say, right?

99
00:05:25,760 --> 00:05:27,280
It's going to be one of the digits.

100
00:05:27,840 --> 00:05:28,800
We don't know which one.

101
00:05:30,080 --> 00:05:33,040
But if I- you've got the paper here,

102
00:05:33,040 --> 00:05:35,520
you know, you can do like division.

103
00:05:35,520 --> 00:05:37,360
You could do this if you had long enough.

104
00:05:38,000 --> 00:05:41,040
Like if I gave you say an hour with the paper

105
00:05:41,040 --> 00:05:42,480
and you could sit here and figure it out

106
00:05:43,040 --> 00:05:47,600
and let's say you do it and you- you come up and it seems to be like 3.

107
00:05:47,600 --> 00:05:49,920
I don't know what it actually is, I haven't done this calculation.

108
00:05:49,920 --> 00:05:51,280
Let's do the calculation.

109
00:05:51,280 --> 00:05:52,160
Bad expression.

110
00:05:52,160 --> 00:05:53,280
Oh I've done it the wrong way around.

111
00:05:53,280 --> 00:05:56,320
One, two, three, four, five, six, seven, eight, nine, ten.

112
00:05:56,320 --> 00:05:57,120
It's a six.

113
00:05:57,120 --> 00:05:58,000
Ah, it's a six.

114
00:05:58,000 --> 00:05:59,840
Okay, I picked that out of nowhere.

115
00:05:59,840 --> 00:06:01,280
So suppose you didn't have a calculator.

116
00:06:01,280 --> 00:06:02,160
You were doing it on paper.

117
00:06:02,160 --> 00:06:05,280
I gave you, you know, half an hour or an hour to do the like long division

118
00:06:05,280 --> 00:06:07,200
or whatever it is you would have to do to figure this out.

119
00:06:07,200 --> 00:06:09,520
What's the probability this statement is true now?

120
00:06:10,320 --> 00:06:11,360
What do you say?

121
00:06:11,360 --> 00:06:12,960
I'm gonna say zero.

122
00:06:12,960 --> 00:06:17,200
Right, but you've just done this in a big hurry on paper, right?

123
00:06:17,200 --> 00:06:18,080
You might have screwed up.

124
00:06:18,800 --> 00:06:20,880
So what's the probability that you made a mistake?

125
00:06:20,880 --> 00:06:22,720
You forgot to like carry the one or something.

126
00:06:22,720 --> 00:06:23,360
So it could be one and-

127
00:06:23,360 --> 00:06:24,480
So it's not zero.

128
00:06:24,480 --> 00:06:26,320
Yeah, there's like some smaller probability.

129
00:06:28,320 --> 00:06:31,280
And then if I left you in in the room again still with a piece of paper

130
00:06:31,280 --> 00:06:34,080
for a hundred years, a thousand years, infinite time,

131
00:06:34,080 --> 00:06:38,560
eventually, assuming that was correct, eventually you say zero, right?

132
00:06:40,000 --> 00:06:42,960
You come up with some like formal proof that this is this is false.

133
00:06:46,080 --> 00:06:50,480
Whereas you imagine if I leave you for infinite time with the cup

134
00:06:50,960 --> 00:06:54,080
and you can't look at it, you can't look under the cup,

135
00:06:54,720 --> 00:06:57,840
it's one-sixth and it's going to stay one-sixth forever, right?

136
00:06:57,840 --> 00:07:01,360
When you're playing cards you've got these probabilities for which cards you think,

137
00:07:01,360 --> 00:07:04,560
depending on the game, and as you observe new things

138
00:07:05,120 --> 00:07:08,160
you're updating your probabilities for these different things

139
00:07:09,120 --> 00:07:10,960
as you observe new evidence.

140
00:07:10,960 --> 00:07:13,280
And then it may be eventually you'll actually see the thing

141
00:07:13,280 --> 00:07:15,280
and then you can go to 100% or zero.

142
00:07:15,600 --> 00:07:19,600
Whereas in this case, all that's changing is your thinking.

143
00:07:19,600 --> 00:07:21,600
But you're doing a similar sort of thing,

144
00:07:21,600 --> 00:07:24,400
you are still updating your probabilities for things.

145
00:07:26,240 --> 00:07:31,120
But probability theory kind of doesn't have anything to say about this scenario.

146
00:07:32,080 --> 00:07:36,320
Like, probability is how you update when you see new evidence

147
00:07:36,320 --> 00:07:38,160
and here you're not seeing any new evidence.

148
00:07:38,160 --> 00:07:41,200
So in principle, whatever the probability of this is,

149
00:07:41,280 --> 00:07:44,400
whatever the probability of this is, it's one or it's zero

150
00:07:45,840 --> 00:07:50,400
just as a direct logical consequence of things that you already know, right?

151
00:07:50,400 --> 00:07:56,000
So your uncertainty comes from the fact that you are not logically omniscient.

152
00:07:56,000 --> 00:07:58,480
In order for you to figure things out, it takes you time.

153
00:07:59,280 --> 00:08:03,280
And this turns out to be really important because so most of the time

154
00:08:04,320 --> 00:08:09,840
what you have is actually a kind of a mixture of these types of uncertainty, right?

155
00:08:09,840 --> 00:08:11,600
So let's imagine a third scenario, right?

156
00:08:11,600 --> 00:08:15,200
Suppose you were like an AI system and that's your eyes because you have a camera.

157
00:08:16,160 --> 00:08:17,200
I do the same thing again.

158
00:08:20,240 --> 00:08:23,360
And now I ask you, what's the probability that it's a five?

159
00:08:24,000 --> 00:08:26,960
You would say one-sixth because you don't know.

160
00:08:26,960 --> 00:08:32,000
But on the other hand, you have recorded video footage of it going under the thing.

161
00:08:32,000 --> 00:08:34,320
The video is your memory of what happened, right?

162
00:08:34,320 --> 00:08:36,080
So you're not observing new information,

163
00:08:36,080 --> 00:08:38,480
you're still observing the same thing you observed the first time.

164
00:08:39,440 --> 00:08:41,200
But you can look at that and say,

165
00:08:42,400 --> 00:08:45,360
hmm, you know, it looked like with the amount of energy it had

166
00:08:45,360 --> 00:08:48,240
and the way that it was rotating and which numbers were where,

167
00:08:48,240 --> 00:08:50,560
it looked like it probably wasn't going to land on a five.

168
00:08:50,560 --> 00:08:52,960
If I asked you on a millisecond deadline, you know,

169
00:08:53,600 --> 00:08:56,000
what was it? What's the probability that it was a five?

170
00:08:56,000 --> 00:08:58,880
You're going to give the same number that you gave at the beginning here, right?

171
00:08:58,880 --> 00:08:59,440
One in six.

172
00:09:00,160 --> 00:09:06,080
But you can look at the data you already have,

173
00:09:06,080 --> 00:09:08,240
the information, the observations that you've already made,

174
00:09:08,800 --> 00:09:11,280
and you can do logical reasoning about them.

175
00:09:11,280 --> 00:09:13,200
You can think, okay, based on the speed it was going,

176
00:09:13,200 --> 00:09:16,160
the angle it was turned at, and which pieces,

177
00:09:16,160 --> 00:09:17,760
which faces were where, and so on,

178
00:09:18,400 --> 00:09:21,520
it seems like, you know, maybe you can run some simulations internally,

179
00:09:21,520 --> 00:09:22,080
something like that.

180
00:09:22,960 --> 00:09:28,320
Say it seems like actually less likely than one in six, right?

181
00:09:28,320 --> 00:09:32,000
If before I thought it was like 0.16 recurring,

182
00:09:32,000 --> 00:09:34,480
now I think it's like 0.15,

183
00:09:34,480 --> 00:09:38,880
because I ran a million simulations and it seems like that.

184
00:09:38,880 --> 00:09:43,520
The longer you think about it, the more precise you can be.

185
00:09:43,520 --> 00:09:44,800
Maybe you keep thinking about it again,

186
00:09:44,800 --> 00:09:48,560
and you get it down to like you think it's actually 0.13, right?

187
00:09:48,560 --> 00:09:49,120
Something like that.

188
00:09:50,880 --> 00:09:55,280
But you don't, you can't, you don't actually know, right?

189
00:09:55,280 --> 00:09:56,960
Because there's still things you don't know,

190
00:09:56,960 --> 00:10:00,800
like you don't know exactly the physical properties of the paper,

191
00:10:00,800 --> 00:10:02,720
or what exactly the inside of the cup is like,

192
00:10:02,720 --> 00:10:03,680
or the weighting of the die,

193
00:10:03,680 --> 00:10:05,760
like you still have uncertainty left,

194
00:10:05,760 --> 00:10:07,920
because you haven't seen which way it landed.

195
00:10:07,920 --> 00:10:12,560
But by doing some thinking, you're able to take some of your,

196
00:10:13,600 --> 00:10:17,520
you're able to like reduce your logical uncertainty.

197
00:10:18,080 --> 00:10:19,840
The point is that probability theory,

198
00:10:21,120 --> 00:10:23,920
in order to do probability theory properly,

199
00:10:23,920 --> 00:10:27,920
to modify your beliefs according to the evidence you observe

200
00:10:27,920 --> 00:10:31,920
in a way that satisfies the laws of probability,

201
00:10:31,920 --> 00:10:33,760
you have to be logically omniscient.

202
00:10:33,760 --> 00:10:36,400
You have to be able to immediately see

203
00:10:37,760 --> 00:10:40,720
all of the logical consequences of everything you observe

204
00:10:40,720 --> 00:10:42,640
and propagate them throughout your beliefs.

205
00:10:44,000 --> 00:10:46,800
And this is like not really practical in the real world.

206
00:10:46,800 --> 00:10:51,920
Like in the real world, observations do not form an orderly queue

207
00:10:51,920 --> 00:10:53,200
and come at you one at a time

208
00:10:53,200 --> 00:10:54,960
and give you enough time after each one

209
00:10:54,960 --> 00:10:57,520
to fully integrate the consequences of each observation.

210
00:10:58,320 --> 00:11:02,240
Um, because human beings are bounded, right?

211
00:11:02,240 --> 00:11:06,880
We have limited processing ability and limited speed.

212
00:11:06,880 --> 00:11:08,560
With this kind of logical uncertainty,

213
00:11:09,120 --> 00:11:13,440
it feels very intuitive that we can do probabilities

214
00:11:13,440 --> 00:11:15,520
based on our logical uncertainty,

215
00:11:15,520 --> 00:11:17,600
that we can, we can think about it in this way.

216
00:11:17,600 --> 00:11:21,040
It makes perfect sense to say that 1 in 10 is the probability.

217
00:11:21,040 --> 00:11:22,400
Until you've done your thinking,

218
00:11:22,400 --> 00:11:25,280
1 in 10 seems like a perfectly reasonable number.

219
00:11:25,280 --> 00:11:28,960
But like, why is 1 in 10 a good answer

220
00:11:28,960 --> 00:11:31,280
and like 50% is not a good answer?

221
00:11:31,280 --> 00:11:32,320
Because you might look at it and say,

222
00:11:32,320 --> 00:11:33,760
well, this is a logical statement.

223
00:11:34,480 --> 00:11:37,520
Um, it's either true or false.

224
00:11:37,520 --> 00:11:39,280
That's two possibilities, you know, 50%.

225
00:11:40,000 --> 00:11:41,760
Why, why is 1 in 10 more sensible?

226
00:11:41,760 --> 00:11:46,240
And in fact, you had to do a bit of thinking to get there, right?

227
00:11:46,240 --> 00:11:49,280
You had to say, oh yeah, it's going to be some digit.

228
00:11:49,280 --> 00:11:50,240
There are 10 digits.

229
00:11:50,240 --> 00:11:52,400
I don't have any reason to prefer one digit over the other.

230
00:11:52,400 --> 00:11:53,360
So 10%.

231
00:11:53,440 --> 00:11:58,000
So you are like always doing reasoning

232
00:11:58,000 --> 00:11:59,680
to come up with these probabilities.

233
00:11:59,680 --> 00:12:03,360
But the thing is that according to standard probability theory,

234
00:12:05,040 --> 00:12:07,280
this is like kind of a nonsense question.

235
00:12:08,160 --> 00:12:10,080
Because if you imagine, like,

236
00:12:10,080 --> 00:12:11,680
from the perspective of probability theory,

237
00:12:11,680 --> 00:12:14,320
this kind of statement is equivalent to saying

238
00:12:14,320 --> 00:12:17,280
what's the probability that 1 equals 1, right?

239
00:12:17,280 --> 00:12:19,600
Or what's the probability that like true equals false?

240
00:12:19,600 --> 00:12:21,520
They're all just, it's just a logical statement.

241
00:12:22,160 --> 00:12:24,240
Um, it doesn't have a probability.

242
00:12:24,240 --> 00:12:25,120
It just is true.

243
00:12:26,000 --> 00:12:29,840
Um, but if you can't think infinitely fast,

244
00:12:29,840 --> 00:12:32,800
then you need to have answers.

245
00:12:32,800 --> 00:12:35,280
You need to have estimates, estimates?

246
00:12:35,280 --> 00:12:36,960
You need to have estimates of your answers

247
00:12:38,000 --> 00:12:40,400
before you've thought for infinite time about them.

248
00:12:40,400 --> 00:12:43,600
This is like an important thing for actually getting by in the world.

249
00:12:43,600 --> 00:12:45,920
Because as I say, like observations don't just line up

250
00:12:45,920 --> 00:12:49,040
and wait for you to reason all of their consequences through.

251
00:12:49,040 --> 00:12:50,960
So when it comes to empirical uncertainty,

252
00:12:51,520 --> 00:12:54,800
we have probability theory, which has these axioms.

253
00:12:55,440 --> 00:12:58,240
Um, and these are the sort of the rules that you need to follow

254
00:12:58,240 --> 00:13:00,560
in order to do probability well.

255
00:13:01,280 --> 00:13:04,240
And they're sort of things like that probabilities, um,

256
00:13:05,120 --> 00:13:06,400
can't be negative.

257
00:13:06,400 --> 00:13:08,480
They have to be between 0 and 1.

258
00:13:08,960 --> 00:13:16,240
Um, the, if you have a set of mutually exclusive possibilities

259
00:13:16,240 --> 00:13:18,400
and that, that constitutes everything that can happen,

260
00:13:18,400 --> 00:13:19,840
then they all need to add up to 1.

261
00:13:20,480 --> 00:13:24,560
Um, and then if you're, if you're doing probabilities,

262
00:13:24,560 --> 00:13:27,360
uh, about like logical sentences,

263
00:13:27,360 --> 00:13:30,080
then there's all these extra rules that make perfect sense.

264
00:13:30,080 --> 00:13:32,000
Like you have statement A and statement B.

265
00:13:32,000 --> 00:13:34,800
You also have statement A and B.

266
00:13:34,800 --> 00:13:40,160
Then statement A and B has to be less likely than A or B.

267
00:13:40,160 --> 00:13:41,520
Or the same, it could be the same.

268
00:13:41,520 --> 00:13:45,920
But like, if A has 20% chance of happening,

269
00:13:46,880 --> 00:13:52,320
then A and B couldn't possibly have more than 20% chance of happening, right?

270
00:13:52,320 --> 00:13:54,480
Even if B is guaranteed to happen, it can't.

271
00:13:54,480 --> 00:13:55,280
So that's like a rule.

272
00:13:55,280 --> 00:13:57,440
If you find in your doing your probabilities,

273
00:13:57,440 --> 00:14:00,400
you have A and B that has a higher probability than either A or B,

274
00:14:00,400 --> 00:14:01,520
then you've made a mistake.

275
00:14:01,520 --> 00:14:02,320
Uh, that kind of thing.

276
00:14:02,320 --> 00:14:03,360
You have these rules.

277
00:14:03,360 --> 00:14:08,080
It would be nice if we could find some way of doing,

278
00:14:08,080 --> 00:14:09,600
of dealing with logical uncertainty,

279
00:14:09,600 --> 00:14:12,880
some, some similar set of rules that is useful.

280
00:14:12,960 --> 00:14:13,680
And so, um,

281
00:14:17,360 --> 00:14:18,720
and so related to that,

282
00:14:18,720 --> 00:14:22,480
we have these things like, um, like Dutch book arguments,

283
00:14:23,200 --> 00:14:27,280
which are about, uh, I don't know why they're called Dutch.

284
00:14:28,480 --> 00:14:34,720
I feel like the English language just like is so harsh to the Dutch for no reason.

285
00:14:34,720 --> 00:14:35,840
They're unjustly maligned.

286
00:14:35,840 --> 00:14:40,160
But anyway, um, where like if you, if basically there's,

287
00:14:40,160 --> 00:14:47,600
there's theorems which, uh, which you can prove that if your beliefs don't obey these rules,

288
00:14:48,400 --> 00:14:53,280
then there will exist some set of bets by which you're guaranteed to lose money.

289
00:14:53,280 --> 00:14:56,560
Some, some set of bets that you will happily take by looking at your probabilities,

290
00:14:57,120 --> 00:14:59,760
um, that you can't, you can't possibly win.

291
00:14:59,760 --> 00:15:01,040
Whatever happens, you lose money.

292
00:15:01,920 --> 00:15:04,800
And that seems like a stupid, like you don't want that in your beliefs.

293
00:15:04,800 --> 00:15:10,400
So that's like an argument that says, um, that your beliefs should obey these rules.

294
00:15:10,400 --> 00:15:13,600
Because if they do, you're at least never gonna end up in a situation

295
00:15:13,600 --> 00:15:15,200
where you're guaranteed to lose by betting.

296
00:15:15,200 --> 00:15:17,840
And that's like one of the things we want beliefs for.

297
00:15:18,720 --> 00:15:21,600
We want some kind of equivalent thing for logical uncertainty.

298
00:15:22,640 --> 00:15:25,440
Um, and that's what this paper is trying to do.

299
00:15:25,440 --> 00:15:28,960
It's trying to come up with, um, a rule.

300
00:15:29,600 --> 00:15:33,120
So you could, you could say if, if you are doing your probability

301
00:15:33,440 --> 00:15:39,200
theory stuff in such a way that there are no Dutch books that can be made against you,

302
00:15:39,840 --> 00:15:41,360
then that's good and you're doing well.

303
00:15:41,360 --> 00:15:43,920
And so when we're talking about advanced AI systems,

304
00:15:43,920 --> 00:15:47,600
we're gonna, it's like a good assumption to make that they're at least gonna try

305
00:15:48,160 --> 00:15:51,040
to not have any Dutch books against them in the way they do their probabilities.

306
00:15:51,040 --> 00:15:53,440
So they will probably be obeying probability theory.

307
00:15:53,440 --> 00:15:58,640
Um, and it would be nice if we had some equivalent thing for logical uncertainty

308
00:15:59,200 --> 00:16:03,200
that said if you are satisfying this criterion in the way that you do this,

309
00:16:03,840 --> 00:16:07,920
then, uh, you're not going to be like doing obviously stupid things.

310
00:16:07,920 --> 00:16:09,840
And that's what this paper is trying to do.

311
00:16:09,840 --> 00:16:13,440
When you're talking about probability, there's, you can kind of think about

312
00:16:13,440 --> 00:16:18,080
what properties you would want your system of deciding probabilities to have.

313
00:16:18,080 --> 00:16:20,560
People have written down various things they would want from this system, right?

314
00:16:21,520 --> 00:16:25,680
There would be a system of like assigning probabilities to statements,

315
00:16:25,680 --> 00:16:30,080
logical statements, and then being able to like update those over time.

316
00:16:30,800 --> 00:16:33,840
So, uh, one thing you would want is you would want it to converge,

317
00:16:34,480 --> 00:16:37,600
which just means if you're thinking about a logical statement,

318
00:16:37,600 --> 00:16:39,680
you might think of something that makes it seem more likely,

319
00:16:39,680 --> 00:16:41,920
and then think of something else like, oh no, actually it's less,

320
00:16:41,920 --> 00:16:43,680
it should be less likely and whatever.

321
00:16:43,680 --> 00:16:48,640
You can imagine some systems for some statements would just think forever

322
00:16:48,640 --> 00:16:52,960
and constantly be changing what they think and, um, never make their mind up.

323
00:16:52,960 --> 00:16:54,000
That's no good, right?

324
00:16:54,000 --> 00:16:56,880
We want a system that if you give it infinite time to think,

325
00:16:56,880 --> 00:16:58,960
will eventually actually decide what it thinks.

326
00:16:59,920 --> 00:17:00,640
So that's one thing.

327
00:17:00,640 --> 00:17:01,440
You want it to converge.

328
00:17:02,400 --> 00:17:06,880
Secondly, you obviously want it to converge like to good values.

329
00:17:06,880 --> 00:17:11,520
So if something turns out to be provable within the system that it's using,

330
00:17:11,520 --> 00:17:13,840
then you would want it to eventually converge to one.

331
00:17:13,840 --> 00:17:16,960
If something turns out to be disprovable, you want it to eventually converge to zero.

332
00:17:18,000 --> 00:17:21,600
And then the rest of the time you want it to be like well calibrated,

333
00:17:21,680 --> 00:17:27,120
which means if you take all of the things that it ever thought were 80% likely to be true,

334
00:17:29,120 --> 00:17:33,520
of those, about 80% should actually end up being true, right?

335
00:17:33,520 --> 00:17:35,680
It should, when it gives you an estimate of the probability,

336
00:17:35,680 --> 00:17:39,040
that should be right in some sense.

337
00:17:39,040 --> 00:17:42,240
Another one is that it should never, um,

338
00:17:42,240 --> 00:17:45,200
and this one's like a bit controversial, but it seems reasonable to me,

339
00:17:45,200 --> 00:17:51,520
that it should never assign probability one to something which is not proved,

340
00:17:52,320 --> 00:17:53,920
or that can't be proved,

341
00:17:53,920 --> 00:17:57,120
and it should never assign probability zero to something that can't be disproved.

342
00:17:58,560 --> 00:18:00,320
They call that non-dogmatism.

343
00:18:00,320 --> 00:18:03,760
At the end of all of this, again, at infinity, after thinking forever,

344
00:18:03,760 --> 00:18:08,560
the probabilities that it gives to things should follow the rules of probability,

345
00:18:08,560 --> 00:18:10,640
right, the ones we talked about before, those rules, right?

346
00:18:10,640 --> 00:18:13,040
It should, at the end of it, you should end up with like a

347
00:18:13,040 --> 00:18:15,440
valid probability distribution over all of your stuff.

348
00:18:15,440 --> 00:18:17,680
Their criterion that they've come up with,

349
00:18:17,680 --> 00:18:19,840
which is the equivalent of there are no Dutch books,

350
00:18:20,640 --> 00:18:23,680
is based on this algorithm that they've made,

351
00:18:23,680 --> 00:18:26,720
which satisfies this criterion,

352
00:18:26,720 --> 00:18:29,600
and therefore has a whole bunch of these nice properties

353
00:18:29,600 --> 00:18:32,320
that you would want a logical inductor to have.

354
00:18:33,520 --> 00:18:36,400
So the way the algorithm works is,

355
00:18:37,920 --> 00:18:41,360
it's one of the zaniest algorithms I've ever seen.

356
00:18:43,360 --> 00:18:45,760
It is, it's weird and wonderful,

357
00:18:45,760 --> 00:18:49,520
and if you're a person who is interested in interesting algorithms,

358
00:18:49,520 --> 00:18:51,200
I would really recommend checking out the paper.

359
00:18:51,200 --> 00:18:53,840
It's very technical because we don't have time in this video,

360
00:18:53,840 --> 00:18:56,560
and I may go into it deeper on my channel at some point

361
00:18:56,560 --> 00:18:58,320
if I ever come to actually understand it,

362
00:18:58,320 --> 00:19:00,400
which, to be honest, I don't fully right now.

363
00:19:02,240 --> 00:19:04,480
But it's based around the idea of a prediction market,

364
00:19:05,280 --> 00:19:08,560
which is a super cool thing that's worth explaining.

365
00:19:08,560 --> 00:19:11,520
In financial markets, as they currently exist,

366
00:19:11,520 --> 00:19:14,000
you can have futures, right?

367
00:19:14,000 --> 00:19:17,120
And a futures contract, it's a contract which says,

368
00:19:17,840 --> 00:19:22,720
I promise to sell you this amount of stuff at this price

369
00:19:23,440 --> 00:19:24,560
of a particular thing, right?

370
00:19:24,560 --> 00:19:27,280
So you say, take a date a year from now,

371
00:19:27,280 --> 00:19:32,320
and you say, I'm gonna sell you this many gallons of jet fuel for this price.

372
00:19:33,840 --> 00:19:35,120
At least that's the way it used to be done.

373
00:19:35,120 --> 00:19:37,680
In practice, the actual jet fuel doesn't move.

374
00:19:37,680 --> 00:19:39,200
You just go by the price of jet fuel,

375
00:19:39,840 --> 00:19:42,080
and you pay the equivalent as if you had,

376
00:19:42,080 --> 00:19:43,840
and then they can go and buy the jet fuel themselves.

377
00:19:44,800 --> 00:19:47,760
But this is like a really useful financial instrument to have,

378
00:19:47,760 --> 00:19:49,680
because let's say you run an airline.

379
00:19:50,640 --> 00:19:52,320
Your main cost is jet fuel.

380
00:19:52,320 --> 00:19:53,920
Prices are very volatile.

381
00:19:53,920 --> 00:19:56,560
If prices go up, you could just go under completely.

382
00:19:57,120 --> 00:20:01,760
So you say, okay, I'm going to buy a whole bunch of these jet fuel futures,

383
00:20:02,640 --> 00:20:04,320
and then if the price goes up,

384
00:20:04,320 --> 00:20:06,080
I'm gonna have to pay loads more for jet fuel,

385
00:20:06,080 --> 00:20:08,000
but I'll make a load of money on these contracts,

386
00:20:08,560 --> 00:20:11,120
and if you do it right, it exactly balances out.

387
00:20:11,760 --> 00:20:15,040
The cost of that is, if the price of jet fuel falls through the floor,

388
00:20:15,760 --> 00:20:19,520
then you don't get to save money,

389
00:20:19,520 --> 00:20:20,960
because you're saving money on jet fuel,

390
00:20:20,960 --> 00:20:22,960
but then you're losing all this money on the contracts.

391
00:20:22,960 --> 00:20:25,520
So it just sort of like balances out your risk.

392
00:20:25,520 --> 00:20:30,080
It lets you lock in the price that you're going to pay a year in advance,

393
00:20:30,080 --> 00:20:33,040
and that's just like super useful for all sorts of businesses.

394
00:20:33,040 --> 00:20:34,800
Really, really good in agriculture as well,

395
00:20:34,800 --> 00:20:36,240
because you don't know what your yield is going to be.

396
00:20:37,120 --> 00:20:42,640
So the point is that the price that you can buy these contracts for

397
00:20:43,680 --> 00:20:48,480
has to be a really good prediction of what the price of jet fuel is going to be

398
00:20:49,120 --> 00:20:50,400
at the time when the contract ends.

399
00:20:51,040 --> 00:20:53,840
And you can kind of treat the price of that

400
00:20:53,840 --> 00:20:57,200
as a combination of everybody's best estimate,

401
00:20:57,200 --> 00:21:01,360
because if you can predict the price of jet fuel a year in advance

402
00:21:01,360 --> 00:21:02,720
better than anybody else,

403
00:21:02,720 --> 00:21:05,520
you can make almost arbitrary amounts of money doing that.

404
00:21:06,880 --> 00:21:11,760
And in doing so, you bring the price to be a more accurate representation, right?

405
00:21:11,760 --> 00:21:13,600
If you think the price is too low and it's going to be more,

406
00:21:13,600 --> 00:21:14,880
then you're going to buy a bunch.

407
00:21:14,880 --> 00:21:16,720
And when you buy it, that raises the price of it,

408
00:21:16,720 --> 00:21:17,840
because, you know, supply and demand.

409
00:21:18,400 --> 00:21:23,520
So these prices end up representing humanity's best estimate

410
00:21:23,520 --> 00:21:25,280
of the price of jet fuel a year from now.

411
00:21:27,680 --> 00:21:28,720
But the thing that's cool is,

412
00:21:29,360 --> 00:21:30,560
you could build...

413
00:21:30,560 --> 00:21:33,360
Like, this is just a piece of paper with stuff written on it, right?

414
00:21:33,360 --> 00:21:35,200
And these days, it's not even a piece of paper.

415
00:21:35,200 --> 00:21:37,520
In principle, you can write anything on there, right?

416
00:21:39,040 --> 00:21:41,040
So what if you wrote a contract that said,

417
00:21:41,040 --> 00:21:43,760
I promise to pay 100 pounds

418
00:21:43,760 --> 00:21:46,720
if such-and-such horse wins the Grand National

419
00:21:47,440 --> 00:21:48,400
and zero otherwise?

420
00:21:49,040 --> 00:21:52,240
And then that's on the market and people can buy and sell it, right?

421
00:21:52,240 --> 00:21:55,280
So if this horse is, like, guaranteed to win the Grand National,

422
00:21:55,280 --> 00:21:59,200
then this contract is effectively a 100-pound note, right?

423
00:21:59,200 --> 00:22:01,280
It's... it's money in the bank, as if...

424
00:22:02,000 --> 00:22:02,960
Why is that a phrase?

425
00:22:02,960 --> 00:22:04,400
Banks... banks aren't that reliable.

426
00:22:04,400 --> 00:22:06,400
But anyway, if the horse is, like, guaranteed to lose,

427
00:22:06,400 --> 00:22:07,840
then it's worth zero.

428
00:22:07,840 --> 00:22:11,680
And if the horse has a 50% chance of winning,

429
00:22:11,680 --> 00:22:14,080
that thing is going to trade for 50 pounds.

430
00:22:14,640 --> 00:22:18,720
And so by making these contracts

431
00:22:18,720 --> 00:22:20,000
and allowing people to trade them,

432
00:22:20,560 --> 00:22:25,040
you can get these good, like, unbiased

433
00:22:25,040 --> 00:22:28,960
and as accurate as you can hope for predictions of future events.

434
00:22:29,600 --> 00:22:31,120
And that's what prediction markets are for.

435
00:22:31,120 --> 00:22:33,920
You can make money on them by being good at predicting stuff.

436
00:22:35,200 --> 00:22:38,160
And anybody else can just look at the prices things are trading at

437
00:22:38,160 --> 00:22:40,640
and just directly convert them into probabilities.

438
00:22:42,000 --> 00:22:46,320
And that's in a... in a spherical chickens-in-a-vacuum kind of way.

439
00:22:46,320 --> 00:22:49,600
Obviously, in practice, there's various things that can go wrong.

440
00:22:49,600 --> 00:22:52,880
But, like, in principle, this is a really beautiful way

441
00:22:52,880 --> 00:22:55,680
of effectively doing distributed computing,

442
00:22:56,480 --> 00:22:58,880
where you have everybody is doing their own computation

443
00:22:58,880 --> 00:23:01,120
and then you're aggregating them using the price mechanism

444
00:23:01,120 --> 00:23:02,880
as communication between the different nodes

445
00:23:02,880 --> 00:23:05,520
and you get, like, super cool.

446
00:23:06,080 --> 00:23:09,840
So that's what logical induction does.

447
00:23:10,400 --> 00:23:13,360
It, like, simulates a prediction market

448
00:23:14,080 --> 00:23:17,920
where all of the contracts are a logical statement.

449
00:23:17,920 --> 00:23:21,040
I think in this, it's not $100, it's $1.

450
00:23:21,040 --> 00:23:24,080
So if it's trading at $1, then it's for sure, it's certain.

451
00:23:25,520 --> 00:23:28,640
Saying, you know, this is currently trading at 60 cents

452
00:23:28,640 --> 00:23:31,280
and I think it's more than 60% likely to be true,

453
00:23:31,280 --> 00:23:34,080
so I'll buy some and that seems like a good investment.

454
00:23:34,960 --> 00:23:40,000
And so all of the traders in the algorithm are programs.

455
00:23:40,000 --> 00:23:40,960
They're computer programs.

456
00:23:44,320 --> 00:23:48,960
And it turns out that if you run this, and this is computable,

457
00:23:48,960 --> 00:23:52,000
it's not, like, tractable in a practical sense

458
00:23:52,000 --> 00:23:54,640
because you're running vast numbers of, like, arbitrary programs,

459
00:23:54,640 --> 00:23:56,880
but it is, in principle, computable.

460
00:23:57,600 --> 00:24:02,480
It ends up having, the market as a whole ends up having,

461
00:24:03,920 --> 00:24:06,320
ends up satisfying all of these really nice properties.

462
00:24:06,320 --> 00:24:08,160
Is that because it comes into balance?

463
00:24:08,160 --> 00:24:11,920
Yeah, as the traders trade with each other,

464
00:24:11,920 --> 00:24:15,040
the ones who are, like, good at predicting end up making more money

465
00:24:15,760 --> 00:24:19,920
and the ones who are bad at predicting, like, go bankrupt, effectively.

466
00:24:19,920 --> 00:24:23,200
You might imagine some trader in this system,

467
00:24:23,200 --> 00:24:25,760
which is a program that just goes around looking for

468
00:24:26,000 --> 00:24:31,040
situations in which you have A and B

469
00:24:31,040 --> 00:24:35,120
and also A and B in the system and the prices don't work.

470
00:24:35,120 --> 00:24:37,040
And it's like arbitrage, right?

471
00:24:37,600 --> 00:24:41,840
So there are people who do this currently on the stock market.

472
00:24:42,560 --> 00:24:44,320
There's different things you can do where you can say,

473
00:24:44,320 --> 00:24:46,640
oh, you know, any time the,

474
00:24:50,720 --> 00:24:52,800
any time the price is different between two markets,

475
00:24:52,800 --> 00:24:54,960
you can make money by buying in one and selling in the other.

476
00:24:55,840 --> 00:24:57,120
Stuff like that. You can do arbitrage.

477
00:24:58,000 --> 00:24:59,040
And it's the same kind of thing.

478
00:24:59,040 --> 00:25:02,080
You can have, so some of these programs will get rich by just saying,

479
00:25:02,080 --> 00:25:05,680
oh, I notice that this statement for A and B

480
00:25:06,240 --> 00:25:09,840
has a probability that's actually higher than the probability of B.

481
00:25:09,840 --> 00:25:12,000
So I'm going to sell that, you know,

482
00:25:12,000 --> 00:25:14,160
or I'm going to buy some B to raise, you know,

483
00:25:14,160 --> 00:25:17,120
I'm going to, like, respond to that in a way that makes me money.

484
00:25:17,120 --> 00:25:18,560
And because there's all of these traders,

485
00:25:19,280 --> 00:25:25,680
they will eventually cause everything to line up, right?

486
00:25:25,680 --> 00:25:26,800
And so the thing will converge.

487
00:25:27,360 --> 00:25:31,760
This is the equivalent of saying there's no Dutch book for your probabilities.

488
00:25:32,720 --> 00:25:34,160
The logical induction criterion,

489
00:25:34,160 --> 00:25:39,520
the market satisfies it if there's no efficiently computable trader

490
00:25:40,160 --> 00:25:41,840
that can exploit the market,

491
00:25:41,840 --> 00:25:43,520
which means that it can, like,

492
00:25:43,520 --> 00:25:46,800
find a reliable strategy for making loads of money without risk.

493
00:25:47,440 --> 00:25:48,480
As long as you have that,

494
00:25:49,760 --> 00:25:53,040
then it satisfies the logical induction criterion,

495
00:25:53,040 --> 00:25:55,280
which then means you get all of these other properties for free.

496
00:25:55,680 --> 00:25:58,960
Some of the properties that it has are kind of crazy.

497
00:25:59,600 --> 00:26:02,560
Like, it's okay with self-reference.

498
00:26:03,280 --> 00:26:05,760
It, like, doesn't care about paradoxes.

499
00:26:05,760 --> 00:26:11,680
There's all kinds of really cool things that don't trip it up.

500
00:26:11,680 --> 00:26:13,760
Bringing us back to what we started talking about,

501
00:26:13,760 --> 00:26:16,640
how does the paper relate to the AI safety thing?

502
00:26:16,640 --> 00:26:17,280
Right, right.

503
00:26:17,280 --> 00:26:21,680
So we're trying to do reasoning and possibly proof theorems

504
00:26:21,680 --> 00:26:23,360
about very powerful AI systems.

505
00:26:23,360 --> 00:26:25,360
And that means that we want to be able to think of them

506
00:26:25,360 --> 00:26:28,560
just in terms of being good at thinking.

507
00:26:29,280 --> 00:26:32,960
And we've got a lot of good theory that pins down,

508
00:26:32,960 --> 00:26:35,600
like, what does it mean to be good at empirical uncertainty?

509
00:26:35,600 --> 00:26:38,240
We have all of probability theory and, like, statistics.

510
00:26:38,240 --> 00:26:39,360
And we can say,

511
00:26:40,240 --> 00:26:43,120
these are the things you need to do to be good at probability.

512
00:26:43,120 --> 00:26:44,800
And we have, like, rational choice theory.

513
00:26:45,360 --> 00:26:48,640
And we can say, this is what it means to be good at making decisions.

514
00:26:50,000 --> 00:26:51,920
And so then when we're reasoning about AI systems,

515
00:26:51,920 --> 00:26:53,920
we can think, well, it's probably going to be good at

516
00:26:53,920 --> 00:26:55,760
reasoning about uncertainty and making decisions.

517
00:26:56,880 --> 00:27:01,120
But we have to also assume that a very powerful hypothetical future AI system

518
00:27:01,120 --> 00:27:03,920
would be good at reasoning under logical uncertainty.

519
00:27:03,920 --> 00:27:06,960
Because it's going to be a physical, like, bounded system.

520
00:27:06,960 --> 00:27:08,720
It is going to need time to think about things.

521
00:27:08,720 --> 00:27:12,080
And it probably is going to need to, like, make decisions

522
00:27:12,080 --> 00:27:14,880
based on things that it hasn't logically thought through

523
00:27:14,880 --> 00:27:16,800
every possible consequence of yet.

524
00:27:16,800 --> 00:27:18,240
So it's probably going to be good at this too.

525
00:27:18,240 --> 00:27:21,760
And we need some, like, formal framework

526
00:27:21,760 --> 00:27:24,480
with which we can think about what it means to be good at

527
00:27:25,040 --> 00:27:26,800
reasoning about logical uncertainty.

528
00:27:27,600 --> 00:27:30,000
And that's, like, what this paper is trying to do.

529
00:27:31,280 --> 00:27:32,560
They're erasable?

530
00:27:32,560 --> 00:27:33,060
Yeah.

531
00:27:34,160 --> 00:27:34,660
Okay.

532
00:27:35,360 --> 00:27:36,400
I do also have-

533
00:27:36,400 --> 00:27:37,280
Oh, I had sharpies.

534
00:27:37,280 --> 00:27:38,560
Yeah, I do have sharpies.

535
00:27:38,560 --> 00:27:40,960
Oh yeah, they squeak on the paper.

536
00:27:40,960 --> 00:27:41,680
People don't like that.

537
00:27:41,680 --> 00:27:42,880
Oh, okay.

538
00:27:42,880 --> 00:27:43,440
Fair enough.

539
00:27:43,440 --> 00:27:44,400
In fact, all pens do.

540
00:27:44,400 --> 00:27:45,840
But these are slightly better.

541
00:27:46,400 --> 00:27:46,960
Right.

542
00:27:46,960 --> 00:27:47,440
Friction.

