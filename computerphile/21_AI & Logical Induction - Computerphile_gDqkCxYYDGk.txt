Today I thought we would talk a bit about logical induction. A paper out of the Machine Intelligence Research Institute. Very technical paper, very mathematical and can be a bit hard to get your head around and we're not going to get too far into it. For Computerphile I just want to explain like why it's cool and why it's interesting and people who are into it can read the paper themselves and find out about it. One of the things about logical induction as a paper is that it's not immediately obvious why it's an AI safety paper. But like my perception of the way that MIRI, the way that the Machine Intelligence Research Institute is thinking about this is they're saying, they're thinking like we're going to be producing at some point artificial general intelligence and as we talked about in previous videos there are hundreds of ways, really weird, subtle, difficult to predict ways that this kind of thing could go very badly wrong. And so we want to be confident about the behavior of our systems before we turn them on and that means ideally we want to be making systems that we can actually, in the best case, that we could actually prove theorems about, right? Things that are well specified enough that we could actually write down and formally prove that it will have certain characteristics. And if you look at like current machine learning stuff, all the like deep neural networks and that kind of thing, they're just very opaque. Do you mean by that because we don't necessarily know exactly why it's doing what it's doing? Yeah, and also just that the system itself is very is very complex and very contingent on a lot of specifics about the training data and the architecture and like there's just, yeah, effectively we don't understand it well enough but it's like not formally specified enough, I guess. And so they're trying to come up with the sort of mathematical foundations that we would need to be able to prove important things about powerful AI systems. Before we were talking about hypothetical future AI systems. We've got time to print more stamps. So maybe it hijacks the world's stamp printing factories. And when we were talking about the stamp collector, it was really useful to have this framework of an agent and say this is what an agent is, it's a formally specified thing and we can reason about the behavior of agents in general and then all we need to do is make these fairly straightforward assumptions about our AI system that it will behave like an agent. So we have idealized forms of reasoning, like we have probability theory which tells you the rules that you need to follow to have good beliefs about the state of the world, right? And we have like rational choice theory about what rules you need to follow and it may not be actually possible to follow those rules but we can at least formally specify like if the thing has these properties it will do this well and all we're trying to do is they're thinking if we're building very powerful AI systems the one thing we can expect from them is they're going to be able to do thinking well and so if we can come up with some formalized method of exactly what we mean by doing thinking well then if we do, if we reason about that that should give us some insight into how these systems will behave. That's the idea. So let's talk about probability theory then. This is where we should have a demo. Pretty sure about Gammon has dice. Yeah, oh this is a fun one. Powers of two. I don't know how to play back Gammon. Anyway, here's a die. So basic probability theory, right? I roll the die, now it's under the cup and I can ask you what's the probability that that is a five? One in six. Right, one in six, because you don't know. Could be anything. There was a time when people reasoned about the probabilities of things happening in a very sort of fuzzy way. You'd be playing cards and you'd be like oh, you know, that card has shown up here so I guess it's less likely that he has this hand and people would, um... Intuit. Yeah, and like there was some sense of like clearly we are doing, so what we're doing here is not random. It's meaningful to say that this outcome or that outcome is more or less likely by a bit or by a lot or whatever but people didn't have a really good explanation of how that all worked until we had probability theory, right? We didn't have- There wasn't a system, right? Right, and like practically speaking you often can't do the full probability theory stuff in your head for a game of cards especially when you're trying to read people's faces and like stuff that's very hard to quantify. But now we have this understanding that like even when we can't actually do it what's going on underneath is probability theory. So that's one thing, right? Straightforward probability. Now let's do a different thing. And now I'm gonna- now I'm gonna need a pen. Now suppose I give you a sentence that's like the 10th digit after the decimal point of the square root of 17 is a 5. What's the probability that that's true? Well it's a much more difficult problem. One in ten. Right, one in ten seems like a totally reasonable thing to say, right? It's going to be one of the digits. We don't know which one. But if I- you've got the paper here, you know, you can do like division. You could do this if you had long enough. Like if I gave you say an hour with the paper and you could sit here and figure it out and let's say you do it and you- you come up and it seems to be like 3. I don't know what it actually is, I haven't done this calculation. Let's do the calculation. Bad expression. Oh I've done it the wrong way around. One, two, three, four, five, six, seven, eight, nine, ten. It's a six. Ah, it's a six. Okay, I picked that out of nowhere. So suppose you didn't have a calculator. You were doing it on paper. I gave you, you know, half an hour or an hour to do the like long division or whatever it is you would have to do to figure this out. What's the probability this statement is true now? What do you say? I'm gonna say zero. Right, but you've just done this in a big hurry on paper, right? You might have screwed up. So what's the probability that you made a mistake? You forgot to like carry the one or something. So it could be one and- So it's not zero. Yeah, there's like some smaller probability. And then if I left you in in the room again still with a piece of paper for a hundred years, a thousand years, infinite time, eventually, assuming that was correct, eventually you say zero, right? You come up with some like formal proof that this is this is false. Whereas you imagine if I leave you for infinite time with the cup and you can't look at it, you can't look under the cup, it's one-sixth and it's going to stay one-sixth forever, right? When you're playing cards you've got these probabilities for which cards you think, depending on the game, and as you observe new things you're updating your probabilities for these different things as you observe new evidence. And then it may be eventually you'll actually see the thing and then you can go to 100% or zero. Whereas in this case, all that's changing is your thinking. But you're doing a similar sort of thing, you are still updating your probabilities for things. But probability theory kind of doesn't have anything to say about this scenario. Like, probability is how you update when you see new evidence and here you're not seeing any new evidence. So in principle, whatever the probability of this is, whatever the probability of this is, it's one or it's zero just as a direct logical consequence of things that you already know, right? So your uncertainty comes from the fact that you are not logically omniscient. In order for you to figure things out, it takes you time. And this turns out to be really important because so most of the time what you have is actually a kind of a mixture of these types of uncertainty, right? So let's imagine a third scenario, right? Suppose you were like an AI system and that's your eyes because you have a camera. I do the same thing again. And now I ask you, what's the probability that it's a five? You would say one-sixth because you don't know. But on the other hand, you have recorded video footage of it going under the thing. The video is your memory of what happened, right? So you're not observing new information, you're still observing the same thing you observed the first time. But you can look at that and say, hmm, you know, it looked like with the amount of energy it had and the way that it was rotating and which numbers were where, it looked like it probably wasn't going to land on a five. If I asked you on a millisecond deadline, you know, what was it? What's the probability that it was a five? You're going to give the same number that you gave at the beginning here, right? One in six. But you can look at the data you already have, the information, the observations that you've already made, and you can do logical reasoning about them. You can think, okay, based on the speed it was going, the angle it was turned at, and which pieces, which faces were where, and so on, it seems like, you know, maybe you can run some simulations internally, something like that. Say it seems like actually less likely than one in six, right? If before I thought it was like 0.16 recurring, now I think it's like 0.15, because I ran a million simulations and it seems like that. The longer you think about it, the more precise you can be. Maybe you keep thinking about it again, and you get it down to like you think it's actually 0.13, right? Something like that. But you don't, you can't, you don't actually know, right? Because there's still things you don't know, like you don't know exactly the physical properties of the paper, or what exactly the inside of the cup is like, or the weighting of the die, like you still have uncertainty left, because you haven't seen which way it landed. But by doing some thinking, you're able to take some of your, you're able to like reduce your logical uncertainty. The point is that probability theory, in order to do probability theory properly, to modify your beliefs according to the evidence you observe in a way that satisfies the laws of probability, you have to be logically omniscient. You have to be able to immediately see all of the logical consequences of everything you observe and propagate them throughout your beliefs. And this is like not really practical in the real world. Like in the real world, observations do not form an orderly queue and come at you one at a time and give you enough time after each one to fully integrate the consequences of each observation. Um, because human beings are bounded, right? We have limited processing ability and limited speed. With this kind of logical uncertainty, it feels very intuitive that we can do probabilities based on our logical uncertainty, that we can, we can think about it in this way. It makes perfect sense to say that 1 in 10 is the probability. Until you've done your thinking, 1 in 10 seems like a perfectly reasonable number. But like, why is 1 in 10 a good answer and like 50% is not a good answer? Because you might look at it and say, well, this is a logical statement. Um, it's either true or false. That's two possibilities, you know, 50%. Why, why is 1 in 10 more sensible? And in fact, you had to do a bit of thinking to get there, right? You had to say, oh yeah, it's going to be some digit. There are 10 digits. I don't have any reason to prefer one digit over the other. So 10%. So you are like always doing reasoning to come up with these probabilities. But the thing is that according to standard probability theory, this is like kind of a nonsense question. Because if you imagine, like, from the perspective of probability theory, this kind of statement is equivalent to saying what's the probability that 1 equals 1, right? Or what's the probability that like true equals false? They're all just, it's just a logical statement. Um, it doesn't have a probability. It just is true. Um, but if you can't think infinitely fast, then you need to have answers. You need to have estimates, estimates? You need to have estimates of your answers before you've thought for infinite time about them. This is like an important thing for actually getting by in the world. Because as I say, like observations don't just line up and wait for you to reason all of their consequences through. So when it comes to empirical uncertainty, we have probability theory, which has these axioms. Um, and these are the sort of the rules that you need to follow in order to do probability well. And they're sort of things like that probabilities, um, can't be negative. They have to be between 0 and 1. Um, the, if you have a set of mutually exclusive possibilities and that, that constitutes everything that can happen, then they all need to add up to 1. Um, and then if you're, if you're doing probabilities, uh, about like logical sentences, then there's all these extra rules that make perfect sense. Like you have statement A and statement B. You also have statement A and B. Then statement A and B has to be less likely than A or B. Or the same, it could be the same. But like, if A has 20% chance of happening, then A and B couldn't possibly have more than 20% chance of happening, right? Even if B is guaranteed to happen, it can't. So that's like a rule. If you find in your doing your probabilities, you have A and B that has a higher probability than either A or B, then you've made a mistake. Uh, that kind of thing. You have these rules. It would be nice if we could find some way of doing, of dealing with logical uncertainty, some, some similar set of rules that is useful. And so, um, and so related to that, we have these things like, um, like Dutch book arguments, which are about, uh, I don't know why they're called Dutch. I feel like the English language just like is so harsh to the Dutch for no reason. They're unjustly maligned. But anyway, um, where like if you, if basically there's, there's theorems which, uh, which you can prove that if your beliefs don't obey these rules, then there will exist some set of bets by which you're guaranteed to lose money. Some, some set of bets that you will happily take by looking at your probabilities, um, that you can't, you can't possibly win. Whatever happens, you lose money. And that seems like a stupid, like you don't want that in your beliefs. So that's like an argument that says, um, that your beliefs should obey these rules. Because if they do, you're at least never gonna end up in a situation where you're guaranteed to lose by betting. And that's like one of the things we want beliefs for. We want some kind of equivalent thing for logical uncertainty. Um, and that's what this paper is trying to do. It's trying to come up with, um, a rule. So you could, you could say if, if you are doing your probability theory stuff in such a way that there are no Dutch books that can be made against you, then that's good and you're doing well. And so when we're talking about advanced AI systems, we're gonna, it's like a good assumption to make that they're at least gonna try to not have any Dutch books against them in the way they do their probabilities. So they will probably be obeying probability theory. Um, and it would be nice if we had some equivalent thing for logical uncertainty that said if you are satisfying this criterion in the way that you do this, then, uh, you're not going to be like doing obviously stupid things. And that's what this paper is trying to do. When you're talking about probability, there's, you can kind of think about what properties you would want your system of deciding probabilities to have. People have written down various things they would want from this system, right? There would be a system of like assigning probabilities to statements, logical statements, and then being able to like update those over time. So, uh, one thing you would want is you would want it to converge, which just means if you're thinking about a logical statement, you might think of something that makes it seem more likely, and then think of something else like, oh no, actually it's less, it should be less likely and whatever. You can imagine some systems for some statements would just think forever and constantly be changing what they think and, um, never make their mind up. That's no good, right? We want a system that if you give it infinite time to think, will eventually actually decide what it thinks. So that's one thing. You want it to converge. Secondly, you obviously want it to converge like to good values. So if something turns out to be provable within the system that it's using, then you would want it to eventually converge to one. If something turns out to be disprovable, you want it to eventually converge to zero. And then the rest of the time you want it to be like well calibrated, which means if you take all of the things that it ever thought were 80% likely to be true, of those, about 80% should actually end up being true, right? It should, when it gives you an estimate of the probability, that should be right in some sense. Another one is that it should never, um, and this one's like a bit controversial, but it seems reasonable to me, that it should never assign probability one to something which is not proved, or that can't be proved, and it should never assign probability zero to something that can't be disproved. They call that non-dogmatism. At the end of all of this, again, at infinity, after thinking forever, the probabilities that it gives to things should follow the rules of probability, right, the ones we talked about before, those rules, right? It should, at the end of it, you should end up with like a valid probability distribution over all of your stuff. Their criterion that they've come up with, which is the equivalent of there are no Dutch books, is based on this algorithm that they've made, which satisfies this criterion, and therefore has a whole bunch of these nice properties that you would want a logical inductor to have. So the way the algorithm works is, it's one of the zaniest algorithms I've ever seen. It is, it's weird and wonderful, and if you're a person who is interested in interesting algorithms, I would really recommend checking out the paper. It's very technical because we don't have time in this video, and I may go into it deeper on my channel at some point if I ever come to actually understand it, which, to be honest, I don't fully right now. But it's based around the idea of a prediction market, which is a super cool thing that's worth explaining. In financial markets, as they currently exist, you can have futures, right? And a futures contract, it's a contract which says, I promise to sell you this amount of stuff at this price of a particular thing, right? So you say, take a date a year from now, and you say, I'm gonna sell you this many gallons of jet fuel for this price. At least that's the way it used to be done. In practice, the actual jet fuel doesn't move. You just go by the price of jet fuel, and you pay the equivalent as if you had, and then they can go and buy the jet fuel themselves. But this is like a really useful financial instrument to have, because let's say you run an airline. Your main cost is jet fuel. Prices are very volatile. If prices go up, you could just go under completely. So you say, okay, I'm going to buy a whole bunch of these jet fuel futures, and then if the price goes up, I'm gonna have to pay loads more for jet fuel, but I'll make a load of money on these contracts, and if you do it right, it exactly balances out. The cost of that is, if the price of jet fuel falls through the floor, then you don't get to save money, because you're saving money on jet fuel, but then you're losing all this money on the contracts. So it just sort of like balances out your risk. It lets you lock in the price that you're going to pay a year in advance, and that's just like super useful for all sorts of businesses. Really, really good in agriculture as well, because you don't know what your yield is going to be. So the point is that the price that you can buy these contracts for has to be a really good prediction of what the price of jet fuel is going to be at the time when the contract ends. And you can kind of treat the price of that as a combination of everybody's best estimate, because if you can predict the price of jet fuel a year in advance better than anybody else, you can make almost arbitrary amounts of money doing that. And in doing so, you bring the price to be a more accurate representation, right? If you think the price is too low and it's going to be more, then you're going to buy a bunch. And when you buy it, that raises the price of it, because, you know, supply and demand. So these prices end up representing humanity's best estimate of the price of jet fuel a year from now. But the thing that's cool is, you could build... Like, this is just a piece of paper with stuff written on it, right? And these days, it's not even a piece of paper. In principle, you can write anything on there, right? So what if you wrote a contract that said, I promise to pay 100 pounds if such-and-such horse wins the Grand National and zero otherwise? And then that's on the market and people can buy and sell it, right? So if this horse is, like, guaranteed to win the Grand National, then this contract is effectively a 100-pound note, right? It's... it's money in the bank, as if... Why is that a phrase? Banks... banks aren't that reliable. But anyway, if the horse is, like, guaranteed to lose, then it's worth zero. And if the horse has a 50% chance of winning, that thing is going to trade for 50 pounds. And so by making these contracts and allowing people to trade them, you can get these good, like, unbiased and as accurate as you can hope for predictions of future events. And that's what prediction markets are for. You can make money on them by being good at predicting stuff. And anybody else can just look at the prices things are trading at and just directly convert them into probabilities. And that's in a... in a spherical chickens-in-a-vacuum kind of way. Obviously, in practice, there's various things that can go wrong. But, like, in principle, this is a really beautiful way of effectively doing distributed computing, where you have everybody is doing their own computation and then you're aggregating them using the price mechanism as communication between the different nodes and you get, like, super cool. So that's what logical induction does. It, like, simulates a prediction market where all of the contracts are a logical statement. I think in this, it's not $100, it's $1. So if it's trading at $1, then it's for sure, it's certain. Saying, you know, this is currently trading at 60 cents and I think it's more than 60% likely to be true, so I'll buy some and that seems like a good investment. And so all of the traders in the algorithm are programs. They're computer programs. And it turns out that if you run this, and this is computable, it's not, like, tractable in a practical sense because you're running vast numbers of, like, arbitrary programs, but it is, in principle, computable. It ends up having, the market as a whole ends up having, ends up satisfying all of these really nice properties. Is that because it comes into balance? Yeah, as the traders trade with each other, the ones who are, like, good at predicting end up making more money and the ones who are bad at predicting, like, go bankrupt, effectively. You might imagine some trader in this system, which is a program that just goes around looking for situations in which you have A and B and also A and B in the system and the prices don't work. And it's like arbitrage, right? So there are people who do this currently on the stock market. There's different things you can do where you can say, oh, you know, any time the, any time the price is different between two markets, you can make money by buying in one and selling in the other. Stuff like that. You can do arbitrage. And it's the same kind of thing. You can have, so some of these programs will get rich by just saying, oh, I notice that this statement for A and B has a probability that's actually higher than the probability of B. So I'm going to sell that, you know, or I'm going to buy some B to raise, you know, I'm going to, like, respond to that in a way that makes me money. And because there's all of these traders, they will eventually cause everything to line up, right? And so the thing will converge. This is the equivalent of saying there's no Dutch book for your probabilities. The logical induction criterion, the market satisfies it if there's no efficiently computable trader that can exploit the market, which means that it can, like, find a reliable strategy for making loads of money without risk. As long as you have that, then it satisfies the logical induction criterion, which then means you get all of these other properties for free. Some of the properties that it has are kind of crazy. Like, it's okay with self-reference. It, like, doesn't care about paradoxes. There's all kinds of really cool things that don't trip it up. Bringing us back to what we started talking about, how does the paper relate to the AI safety thing? Right, right. So we're trying to do reasoning and possibly proof theorems about very powerful AI systems. And that means that we want to be able to think of them just in terms of being good at thinking. And we've got a lot of good theory that pins down, like, what does it mean to be good at empirical uncertainty? We have all of probability theory and, like, statistics. And we can say, these are the things you need to do to be good at probability. And we have, like, rational choice theory. And we can say, this is what it means to be good at making decisions. And so then when we're reasoning about AI systems, we can think, well, it's probably going to be good at reasoning about uncertainty and making decisions. But we have to also assume that a very powerful hypothetical future AI system would be good at reasoning under logical uncertainty. Because it's going to be a physical, like, bounded system. It is going to need time to think about things. And it probably is going to need to, like, make decisions based on things that it hasn't logically thought through every possible consequence of yet. So it's probably going to be good at this too. And we need some, like, formal framework with which we can think about what it means to be good at reasoning about logical uncertainty. And that's, like, what this paper is trying to do. They're erasable? Yeah. Okay. I do also have- Oh, I had sharpies. Yeah, I do have sharpies. Oh yeah, they squeak on the paper. People don't like that. Oh, okay. Fair enough. In fact, all pens do. But these are slightly better. Right. Friction. 