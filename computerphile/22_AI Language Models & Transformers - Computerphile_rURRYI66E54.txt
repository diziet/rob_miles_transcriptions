So I wanted to make a video about GPT-2 because it's been in the news recently, this very powerful language model from OpenAI, and I thought it would make sense to start by just doing a video about transformers and language models in general because GPT-2 is a very large language model implemented as a transformer. You have a previous video about generating YouTube comments, which is the same kind of task, right? That's a language modeling task at Natural Language Processing to generate new samples. For cooling of the most complex or magnetic consistent brackets, like a computer, to expect found in creating organizations. I believe that video was made October 2017, and this paper came out December 2017, which has kind of revolutionized the way that people carry out that kind of task. That's not the GPT-2, that's something before that. Right, that's the transformer, which is a new rel. Yeah, relatively new architecture for neural networks that can do actually all kinds of tasks, but they're especially good at this kind of language modeling task. Technically a language model is a probability distribution over like sequences of tokens or symbols or words or whatever in a language. So for any given like sequence of tokens, it can tell you how likely that is. So if you have a good language model of English, it can look at a sequence of, you know, words or characters or whatever, and say how likely that is to occur in English, or how likely that is to be an English phrase or sentence or whatever. And when you have that, you can use that for a lot of different tasks. So if you want to generate text, then you can you can just sort of sample from that distribution and keep giving it its own output. So you you sample a word and then you say, and to be clear, sampling from a distribution means you're just taking you're, you're sort of rolling the dice on that probability distribution and taking whichever one comes out. So so you can like sample a word and then and then say, okay, conditioning on that, given that the first word of this sentence is the, what does the probability distribution look like for the second word? And then you sample from that distribution and then it's, you know, the cat. And you say, given that it's the cat, what's likely to come next? And so on. So you can, you can build up a string of text by sampling from your distribution. That's one of the things you could use it for. And most of us kind of have an example of this sort of in our pockets, we've put it to text, right? Absolutely, right. And that's like, that's the, that's the way that most people interact with language model, I guess. This is how I often start a sentence. Apparently, with I, I am not sure if you have any questions or concerns. Please visit the plug-in settings so I can do it for the first time in the future of, oh, that's no good. Here's a different option. Let's just see what this one is. It may be the same. I am in the morning, but I can't find it on the phone screen. On the phone screen, on the phone screen, on the phone screen, on the phone screen, on the phone screen, on the phone screen, on the phone screen. I don't actually know how this is implemented. It might be a neural network, but my guess is that it's some kind of like Markov model, Markov chain type setup, where you just, for each word in your language, you look at your data set and you see how often a particular, how often each other word is following that word, and then that's how you build your distribution. So like, for the word I, the most common word to follow that is am, and there are a few others, you know. So this is like a very simple model, and this sentence, on the phone screen, on the phone screen, on the phone screen, on the phone screen, on the phone screen, is actually very unlikely, right? This is a super low probability sentence. Why would somebody type this? And the thing is, it's like myopic. It's only, I'm not sure, it's probably only looking at the previous word. It might be looking at, like, the previous two words, but the problem is, to look back, it becomes extremely expensive. Computationally expensive, obviously. Right, like, you've got, I don't know, 50,000 words that you might be looking at, and so then, so you're, you're, you're remembering 50,000 probability distributions, or 50,000 top three words, but, you know, then if you want to do two, that's 50,000 squared, right? And if you want to go back three words, you have to cube it. So you're, like, raising it to the power of the number of words back you want to go, which is, which means that this type of model basically doesn't look back. By the time we're saying, on the, it's already forgotten the previous time it said, on the. It doesn't realize that it's repeating itself. And there are slightly better things you can do in this general area, but, like, fundamentally, if you don't remember, you're not going to be able to make good sentences if you can't remember the beginning of the sentence by the time you're at the end of it, right? And so one of the big areas of progress in language models is handling long-term dependencies. I mean, handling dependencies of any kind, but especially long-term dependencies. You've got a sentence that's, like, Sean came to the hack space to record a video and I talked to blank, right? In that situation, if your model is good, you're expecting, like, a pronoun, probably. So it's, it's he, she, it, they, you know, them, whatever. And, but, the relevant piece of information is the word Sean, which is, like, all the way at the beginning of the sentence. So your model needs to be able to say, oh, okay, you know, Sean, that's usually associated with male pronouns, so we'll put the male pronoun in there. And if your model doesn't have that ability to look back or to just remember what it's just said, then you end up with these sentences that, like, go nowhere. It's just as like, like, it might make a guess, just a random guess at a pronoun, and might get it wrong, or it might just, and I talked to, and then just be like, Frank, you know, and just, like, introduce a new name, because it's guessing at what's likely to come there, and it's completely forgotten that Sean was ever, like, a thing. So, yeah, these kind of dependencies are a big issue with things that you would want a language model to do. But we've only so far talked about language models for generating text in this way, but you can also use them for all kinds of different things. So, like, people use language models for translation, obviously. You have some input sequence that's, like, in English, and you want to output a sequence in French, or something like that. Having a good language model is really important, so that you end up with something that makes sense. Summarization is a task that people often want, where you read in a long piece of text, and then you generate a short piece of text that's, like, a summary of that. That's the kind of thing that you would use a language model for, or reading a piece of text, and then answering questions about that text, or if you want to write, like, a chatbot that's going to converse with people, having a language model is good. Like, basically almost all, like, natural language processing, right? It's useful to have this. The other thing is you can use it to enhance, enhance a lot of other language-related tasks. So, if you're doing, like, speech recognition, then having a good language model, like, there's a lot of things people can say that sound very similar, and to get the right one, you need to be, like, oh, well, this actually makes sense, you know. This word that sounds very similar would be incoherent in this sentence. It's a very low probability. It's much more likely that they said this thing, which is, like, would flow in the language, and human beings do this all the time. Same thing with recognizing text from images, you know, you've got two words that look similar, or there's some ambiguity, or whatever, and to resolve that you need an understanding of what word would make sense there, what word would fit. If you're trying to use a neural network to do the kind of thing we were talking about before, of having a phone, you know, autocorrect based on the previous word or two. Suppose you've got a sequence of two words going in. You've got so, and then I, and you put both of these in to your network, and it will then output, you know, like said, for example, as, like, a sensible next word, and then what you do is you throw away so, and you then bring your said around, and you make a new sequence, which is I said, and then put that into your network, and it will put out, like, I said to, for example, would make sense, and so on, and you keep going around. But the problem is this length is really short. You try and make this long enough to contain an entire sentence, just an ordinary length sentence, and this problem starts to become really, really hard, and networks have a hard time learning it, and you don't get very good performance. And even then, you're still, like, have this absolute hard limit on how long a thing, you, you have to just pick a number that's, like, how far back am I looking. A better thing to do is a recurrent neural network where you, you give the thing, let's, like, divide that up. So in this case, that you have a network, you give it this vector, you just, like, have a bunch of numbers, which is going to be, like, the memory for that network, is the idea, like, the problem is it's forgotten the beginning of the sentence by the time it gets to the end. So we've got to give it some way of remembering, and rather than feeding it the entire sentence every time, you give it this vector, and you give it just one word at a time of your input, and this vector, which you initialize, I guess, with zeros. I want to be clear, this is not something that I've studied in a huge amount of detail, I'm just, like, giving the overall, like, structure of the thing, but the point is, you give it this vector and the word, and it outputs its guess for the next word, and also a modified version of that vector, that you then, for the next thing, you give it the word that it spit out, or the sequence that it spit out, and its own modified version of the vector. Every cycle it goes around, it's modifying this memory. Once this system is, like, trained very well, if you give it, if you give it the first word, Sean, then part of this vector is going to contain some information that's, like, the subject of this sentence is the word, Sean, and some other part will probably keep track of, like, we expect to use a male pronoun for this sentence, and that kind of thing. So you take this, and give it to that, and these are just two instances of the same network, and then it keeps going every time. So it spits out, like, this is I, so then the I also comes around to here. It might then put out said, and so on, but it's got this continuous thread of of memory, effectively, going through, because it keeps passing the thing through. In principle, if it figures out something important at the beginning of, you know, the complete works of Shakespeare that it's generating, there's nothing, strictly speaking, stopping that from persisting, from being passed through from from iteration to iteration to iteration every time. In practice, it doesn't work that way, because in practice, the whole thing is being messed with by the network on every step, and so in in the training process, it's going to learn that it performs best when it leaves most of it alone, and it doesn't just randomly change the whole thing. But, by the time you're on the 50th word of your sentence, whatever the network decided to do on the first word of the sentence is a photocopy of a photocopy of a photocopy of a photocopy, and so things have a tendency to fade out to nothing. It has to be successfully remembered at every step of this process, and if at any point it gets overwritten with something else, or just it did its best to remember it, but it's actually remembering 99% of it each time, 0.99 to the 50 is like actually not that big of a number. So these things work pretty well, but they still get, the performance like really quickly drops off once the sentences start to get long. So this is a recurrent neural network, RNN. Because all of these boxes are really the same box, because this is the same network at different time steps, it's really a loop like this. You're giving the output of the network back as input every time. So this works better, and then people have tried all kinds of interesting things, things like LSTMs. There's all kinds of variants on this general like recurrent network. And LSTM is the thing that might use, isn't it? Right, right. Long short-term memory, which is kind of surreal. But yeah, so the idea of that is it's a lot more complicated inside these networks. There's actually kind of sub networks that make specific decisions about gating things, so rather than having to have the system learn that it ought to pass most things on, it's sort of more in the architecture that it passes most things on, and then there's a, there's a sub, there's like part of the learning is deciding what to forget at each step, and like deciding what to change, and what to put in, and what to pass on, and so on. And they perform better. They can hang on to the information, the relevant information for longer. But the other thing that people often build into these kinds of systems is something called attention, which is actually a pretty good metaphor, where in the same way that you would have networks that decide which parts of your hidden state to hang on to, or which parts to forget, or those kinds of decisions, like gating and stuff, you have a system which is deciding which parts of the input to pay attention to, which parts to use in the, in the calculation, and which parts to ignore. And this turns out to be actually very powerful. So there was this paper, when was this? 2000, 2017? Yeah. So this is funny, because this came out the same year as the video you have about generating YouTube comments. This is in December. I think that video was October. Ancient history now, all right? We're talking two years ago. The idea of this is, as it's called, attention is all you need. They developed this system whereby it's actually as, it's a lot simpler as a, as a network. You can see on the diagram here, if you compare this to the diagram for an LSTM, or any of those kind of variants, it's relatively simple. And it's just kind of using attention to do everything. So when you made that video, the LSTM type stuff was like state-of-the-art. And that was until a couple of months later, I guess, when this paper came out. The idea of this is that attention is all you need. That like, this stuff about having gates for forgetting things, and all of that, all of that kind of stuff. In fact, the whole recurrence, like architecture, you can do away with it, and just use attention. Attention is powerful enough to to do everything that you need. At its base, attention is about actively deciding, in the same way that the LSTM is actively deciding what to forget, and so on. This is deciding which parts of some other part of the data it's going to take into account, which parts it's going to look at. Like, it can be very dangerous in AI to use words for things that are words that people already use, for the way that humans do things. It makes it very easy to anthropomorphize, and just make, you know, get confused, because the abstraction doesn't quite work. But I think attention is a pretty decent name, because it is, it does make sense. It sort of draws the relationships between things. So you can have attention from the output to the input, which is what that would be. You can also have attention from the output to other parts of the output. So, for example, when I'm generating in that sentence, like Sean came to record a video, whatever, by the time I get to generating the word him, I don't need to be thinking about the entire sentence. I can just focus my attention on where I remember the name was. So the attention goes to Sean, and then I can make the decision for, to use the word him, based on that. So, so rather than having to hang on to a huge amount of memory, you can just selectively look at the things that are actually relevant, and the system learns where to look, where to pay attention to. And that's really cool. Like, you can do it, there's attention-based systems for all kinds of things, like not just text. You can do, like, suppose you have, your input is like an image, and you want to caption it. You can actually look at when it was outputting the sequence, and you can say, when you generated the word dog, what was your, you can get like an attention heat map, and it will highlight the dog, because that's the part of the image that it was paying attention to when it generated that output. It makes your system more interpretable, because you can see what it was thinking, and sometimes you can catch problems that way, as well, which is kind of fun. Like, it generates the output that's like, a man is lifting a dumbbell, or something like that, and you look at it, and it's not actually correct. It's like, it's Arnold Schwarzenegger. He's drinking some tea out of a mug, right? And what you find is then, when you look at your outputs where it says dumbbell, you look at the attention, and the attention is like mostly looking at the arms. That's usually somebody muscular who's lifting the dumbbell in your photos. It's, and so it, it's overriding the fact that this kind of looks like a mug, because it was looking at the arms. So the idea is, this system, which is called a transformer, is a type of neural network, which just relies very heavily on attention to produce, like, state-of-the-art performance, and if you train them on a large corpus of natural language, they can learn, they can learn to do very well, right? They can be very powerful language models. We had the example of a language model on your phone that's, like, very, very basic, and then trying to do this with neural networks, and the problems with remembering, and so you have, like, recurrent systems that keep track of, they allow you to pass memory along, so that you can remember the beginning of the sentence, at least by the end of it, and things like LSTMs, there is all these different varieties that people try different things that are better at hanging on to memory, so that they can do better at, they can have longer term dependencies, which allows you to have more coherent outputs, and just generally better performance, and then the transformer is a variant on that, well, is a different way of doing things, where you really focus on attention, and so these are actually not recurrent, which is an important distinction to make. We don't have this thing of, like, taking the output and feeding that back as the input, and so on, every time. Because we have attention, we don't need to keep a big memory that we run through every time. When the system wants to know something, it can use its attention to look back to that part. It's not, like, memorizing the text as it goes, it's paying attention to different bits of the text as they, as it thinks that they're relevant to the bit that it's looking at now. And the thing about that is, when you have this recurrent thing, it's kind of inherently serial. Most of the calculations for this, you can't do them until you have the inputs, and the inputs are the output of the previous network, and so you can't do the thing that people like to do now, which is run it on a million computers, and get lightning-fast performance, because you have to go through them in order, right? It's, like, inherently serial. Whereas transformers are much more parallelizable, which means you get better computational performance out of them as well, which is another selling point. So they work better and they run faster. So they're really a step up. So transformers are this really powerful architecture. They seem to give really good performance on this kind of, sort of, language modeling type task, and we, but what we didn't know really was how far you can push them, or how good they can get. What happens if you take this architecture and you give it a bigger data set than any of them has ever been given, and more compute to train with, you know, a larger model with more parameters and more data? How good can these things get? How good a language model can you actually make? And that's what OpenAI was doing with GPT-2. So an executable binary. The net effect of slotting that T-diagram against here, slightly downwards, is to show you that the C you've written gets converted into binary and the net output from this process, it produces out a program that you probably store in a file. 