The previous video we were talking about Transformers this architecture that uses attention to give Unprecedentedly good performance on sort of language modeling tasks and some other tasks as well, but we're looking at language modeling And that was in preparation to make a video about GPT-2 Which is this very giant language model that has been that was recently Well was recently not released actually by open AI the way that they generated the data set for this is pretty cool To get enough text. They went to reddit and They pulled every website that is linked to from reddit. Do we have any idea of how many that is lots? It wasn't literally every it was everything that had more than three karma I think or maybe more than two comma something like that like Anything that had somebody had thought to post to reddit and at least two or three people who had thought was good enough to upvote They scraped the text from that. It's pretty much just a transformer. It's not that the architecture is not especially novel They haven't done any like amazing new new discovery, but what they realized was Transformers it seems like the more data you give them the better they do and the bigger you make them the better they do and Everything that we built up until this point is clearly not Like we haven't hit the limits of what this can do With we they thought well, we think we're probably bottlenecked on data and Maybe network size. So what happens if we'd like turn that to 11? What happens if we just give this all the data and make a really big one? It makes sense to talk about the acronym, right? So it's a generative pre-trained transformer so generative same as generative adversarial network. It generates outputs the generate samples Pre-trained is this thing? I was talking about all of the different things you can use a language model for right? You can do you can do translation. You can try and resolve ambiguities. You can do summarization. You can answer questions you can Use the probabilities for augmenting other systems. So yeah, there's a bunch of different benchmarks for these different tasks That you might want your language model to do and this is what we talked about in the gridworlds video of having these like standardized Problems with standardized metrics and standardized data sets so that if you're comparing two different methods You know that you're actually comparing apples to apples and This is like very important. It gives you numbers on these things. It's often quite difficult Expect it to like you're generating samples of text and it's like how plausible is this text? How realistic does it look like? How do you put a number on that? It's kind of difficult so there's all of these standardized metrics and the thing that People came to realize which actually I mean I say that as though it's like some amazing discovery it's fairly obvious if you train your system in a like an unsupervised way on a large corpus of just general English text and then you take that and Train that with the data from this benchmark or the data from that benchmark. You can like fine-tune it So you start with something which has like a decent understanding of how English works more or less and then you say now I'm going to give you these Samples for like question answering or I'm going to build a system using that to solve to go for this benchmark So it's pre trained you start with something. That's like a general-purpose language model and then you from that a Fine-tune it to whichever actual benchmark or problem. You're trying to solve and this Can give you better performance than just starting from nothing and training to each of the benchmarks from scratch makes sense and so the point of the GPT to paper the thing that makes it cool is They said okay if we make a really huge one What if we? Don't Fine-tune it at all What if we just make a giant model and then just try and run it on the benchmarks without messing with it? Without showing it any of the specialized data for that benchmark just the raw General-purpose language model, how does that perform and it turns out? Surprisingly well, so this is a very very large data set for text. It's about 40 gigabytes which Actually doesn't sound like very much but like for text text. That's insane, right? It's I somebody said that this was the size of Google's entire index of the Internet in 98 So like it's yeah, it's a lot of text and They trained it on that and they ended up with a 1.5 billion parameter Model, but which is like a previous state-of-the-art system with 345 million. This is 1.5 billion So they've just made the thing much much bigger and it performs really well some of their samples that they published quite captured the public imagination you could say and now that we've talked a little about the Problems that neural networks or any language model really? Has with long-term dependencies we can now realize just how impressive these samples are because when you look at them as a You know, if you look at them uninitiated, you're like, yeah, that's pretty realistic It seems to like make sense and it's cool. But when you look at it knowing how language models work, it's like very impressive the the coherence and the Consistency and the long-range dependencies so we can look at this one that got everybody's attention the unicorns one, right? So they prompted it with in a shocking finding Scientists discovered a herd of unicorns living in a remote previously unexplored valley in the Andes Mountains Even more surprising to the researchers was the fact that the unicorns spoke perfect English Well, hello, and from there you then say you go to your language model GPT 2 and you say given that we started with this What's the next word and what's the word after that and so on So it goes on the scientists named the population after their distinctive horn of its unicorn These four horned silver white unicorns were previously unknown to science We do have a clue here as a human being unicorn four horned doesn't quite make sense But nonetheless, we're going okay now after almost two centuries the mystery of what sparked this odd phenomenon is finally solved Dr. Would that be Jorge Jorge Perez? J-o-r-g an evolutionary biologist from the University of La Paz This is impressive because we've mentioned the Andes Mountains in our prompt. And so now it's saying Okay, this is clearly, you know in a shocking finding. This is a science press release news article It's seen enough of those because it has every single one that was ever linked to from reddit, right? So it knows how these go it knows. Okay third paragraph This is when we talk about the scientist we interview the scientist, right? Okay First word of the scientist paragraph doctor, obviously, right because this isn't now we're in the name of the scientist What name are we going to give? It needs to be a name Conditioning on the fact that we have the Andes Mountains So we need to get we're in South America The name probably should be Spanish or maybe Portuguese So we get we get dr. Perez here and then evolutionary biologist makes sense because we're talking about animals from the University of La Paz again This is the first sentence like when you have that first clause that introduces the scientist you always say where they're from So we say from the University of and then university names tend to be the name of a city What's a city where we have the Andes Mountains? So we're gonna you know, Bolivia La Paz perfect and the thing that's cool about this is it's remembered all of these things That were quite a long time ago several sentences ago. Well, it hasn't remembered them It's paid attention to them across that distance, which is impressive But also this is encoding a bunch of understand understanding a bunch of information about the real world right All that was given all it knows is statistical relationships between words But the way that it comes out to us is that it knows Where the Andes Mountains are what kind of names people in that area have what their cities are what the universities are all of those Facts about the real world because in order to have a really good language model It turns out you have to kind of implicitly encode Information about the world because We use language to talk about the world and knowing what's likely to come next Requires actual real-world understanding and that's something that we see in some of the other Things that they got it to do you can see the real-world understanding coming through Let's keep going University of La Paz and several companions were exploring the Andes Mountains when they found a small valley with no other animals or humans Perez see we're hanging on to him. Yep. We're referring to him again But now we've changed it to be just the surname because that's the format that people use in news articles Perez noticed that the valley Had what appeared to be a natural fountain surrounded by two peaks of rock and silver snow Presently others then ventured further into the valley around about here in our article We should have a quote from the scientist right quote by the time we reached the top of one peak the water looked blue with Some crystals on top and we're talking about this fountain. I guess it's natural fountain. We're referring back to the previous sentence like everything is Relying on and contingent on earlier parts of the text while examining I've skipped a paragraph while examining these bizarre creatures The scientists discovered that the creatures also spoke some fairly regular English now when I read that I was like, okay This is now unusually good because that's the second sentence of the lead, right? We're six paragraphs in and it knows about this point. I've covered the first sentence of this initial paragraph now it's time to talk about this second sentence of the lead even more surprising to the researchers of the fact that they spoke English and It completely ignored the speaking English part until it got to the part of the news article where that comes in You've gone six whole paragraphs the idea of Accurately remembering that the unicorn speak perfect English is like that's very impressive to me and then it goes into its gets a little bit unhinged Starts talking about it's likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is Through DNA. That's reddit, right? Well, it's not actually stuff on reddit stuff linked to from reddit. But yeah, this is this is news articles, man They seem to be able to communicate in English quite well Which I believe is a sign of evolution or at least a change in social organization said the scientist that's his evolutionary biology They're right. Right, right. Yeah, we know he's an evolutionary biologist. So so the the coherence of this text is really dependent on its ability to Condition what it's generating on Things that it's generated a long time ago. So yeah, so it can generate really nice news articles And it can generate all kinds of text things that it anything that is sufficiently well Represented in the original data set. So that's GPT to it's a really unusually powerful and like versatile Language model that can do all of these different natural language processing tasks without Actually being trained specifically on those tasks It's a really and that's that's why it's impressive It's not that it's a it's a brand new architecture or a brand new approach or whatever It's just when you make these things really huge and give them tremendously large amounts of data The results are really impressive So it will it will write you the Lord of the Rings fanfiction It will write you take recipes it would like there's all kinds of examples of different samples here's a recipe for Some kind of peppermint chocolate cake and it's got a bunch of different 