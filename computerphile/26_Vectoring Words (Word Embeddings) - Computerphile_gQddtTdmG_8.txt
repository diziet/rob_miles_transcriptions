If we're moving from cat to dog, which are similar things, so we go away from cat and towards dog, and then we go further in that direction. Can we go beyond in that direction? Yes, so the first result is dogs, which is kind of a nonsense result. The second is pit bull. So that's like the doggiest of dogs? Right, the least cat-like dog. That feels right to me, actually. What if you go the other way? What, the most cat-like cat? The most un-dog-like? Let's find out. It's going to be kitten, right? It's got to be. Cats, feline, kitten. Not really giving us anything much to work with. I thought I would talk a little bit about word embeddings. Word2vec and just word embeddings in general. The way I was introduced to word embeddings, or the sort of context that I'm most familiar with them in, is like, how do you represent a word to a neural network? Well, it's a set of characters, isn't it? I mean, need it be more than the set of characters that make it up? Right, so you can do that. Even though the thing we were talking about before in language models, you have a problem of how far back you can look. I would much rather be able to look back 50 words than 50 characters. And like, if you're training a character-based model, a lot of the capacity of your network is going to be used up just learning what characters count as valid words, right? What combinations of characters are words? And so if you're trying to learn something more complicated than that, you're spending a lot of your time training just like what words are, and a lot of your network capacity is being used for that as well. But this isn't a hard problem. We know what the words are, right? You can give the thing a dictionary, and then you're kind of, it gives it a jumpstart. The point is, neural networks, they view things as like a vector of real numbers, or a vector of floats, which is like some of the real numbers. And so if you think about something like an image, representing an image in this way is like fairly straightforward. You just take all of the pixels and put them in a long row, and if they're black, then it's zero, and if they're white, then it's one, and you just have grayscale in between, for example. It's like fairly straightforward. And so then you end up with a vector that represents that image. It's a reasonably good representation. It sort of reflects some elements of the structure of what you're actually talking about. So, like if you take the same image and make it a little bit brighter, for example, that is just making that vector a bit longer, right? Or a point in that configuration space that's a bit further from the origin. You can make it darker by moving it close to the origin, by reducing the length of that vector. If you take an image and you apply a small amount of, you know, noise to it, that represents just like jiggling that vector around slightly in that configuration space. So you've got, you've got a sense in which two vectors that are close to each other are actually kind of similar images, and that some of the sort of directions in the vector space are actually meaningful in terms of something that would make sense for images. And the same is true with numbers and whatever else. And this is very useful when you're training, because it allows you to say, if your neural network is trying to predict a number and the value you're looking for is 10 and it gives you 9, you can say, no, but that's close. And if it gave you 7,000, you can be like, no, and it's not close. And that gives more information that allows the system to learn. And in the same way, you can say, yeah, that's almost the image that I want. Whereas if you give the thing a dictionary of words, say you've got your 10,000 words, and the usual way of representing this is with a one-hot vector. If you have 10,000 words, you have a vector that's 10,000 long, 10,000 dimensions, and all of the values are 0, apart from one of them, which is 1. So, like, the first word in the dictionary, if it's like A, then that's represented by a 1, and then the rest of the 10,000 are 0s. And then the second word is like a 0, and then a 1, and then all 0s, and so on. But there, you're not giving any of those clues. If the thing is looking for one word, and it gets a different word, all you can say is, yeah, that's the correct one, or no, that's not the correct one. Something that you might try, but you shouldn't, because it's a stupid idea, is rather than giving it as a one-hot vector, you could just give it as a number. But then you've got this indication that, like, two words that are next to each other in the dictionary are similar, and that's not really true, right? Like, if you have a language model, and you're trying to predict the next word, and it's saying, I love playing with my pet blank, and, like, the word you're looking for is cat, and the word it gives you is car, lexicographically, they're pretty similar. But you don't want to be saying to your network, ah, you know, close, that was very nearly right. Because it's not very nearly right, it's a nonsense prediction. But then, if it said, like, dog, you should be able to say, no, but that's close, right? Because that is a plausible completion for that sentence. And the reason that that makes sense is that cat and dog are, like, similar words. What does it mean for a word to be similar to another word? And so the assumption that word embeddings use is that two words are similar if they are often used in similar contexts. So, if you look at all of the instances of the word cat in a giant database, you know, a giant corpus of text, and all of the instances of the word dog, they're going to be surrounded by, you know, words like pet, and words like, you know, feed, words like play, and, you know, that kind of thing. Cute, et cetera. Right. And so that gives some indication that these are similar words. The challenge that word embeddings are trying to come up with is, like, how do you represent words as vectors such that two similar vectors are two similar words? And possibly so that directions have some meaning as well. Because then that should allow our networks to be able to understand better what we're talking about in text. So the thing people realized was, if you have a language model that's able to get good performance of, like, predicting the next word in a sentence, and the architecture of that model is such that it doesn't have that many neurons in its hidden layers, it has to be compressing that information down efficiently. So you've got the inputs to your network. Let's say, for the sake of simplicity, your language model is just taking a word and trying to guess the next word. So we only have to deal with having one word in our input. But so our input is this very tall thing, right? 10,000 tall. And these then feed into a hidden layer which is much smaller. I mean, it's more than five. But it might be, like, a few hundred. Maybe, let's say 300. And these are sort of the connections. All of these is connected to all of these, and it feeds in. And then coming out the other end, you're back out to 10,000 again, right? Because your output is, it's going to make one of these high. You do something like softmax to turn that into a probability distribution. So you give it a word from your dictionary. It then does something. And what comes out the other end is probability distribution where you can just, like, look at the highest value on the output, and that's what it thinks the next word will be. And the higher that value is, the more, like, confident it is. But the point is, you're going from 10,000 to 300 and back out to 10,000. So this 300 has to be, if this is doing well at its task, this 300 has to be encoding, sort of compressing information about the word because the information is passing through, and it's going through this thing that's only 300 wide. So in order to be good at this task, it has to be doing this. So then they were thinking, well, how do we pull that knowledge out? It's kind of like an egg drop competition. Is this where you have to devise some method of safely getting the egg to the floor? Right. It's not like the teachers actually want to get an egg safely to the ground, right? But they've chosen the task such that if you can do well at this task, you have to have learned some things about physics, some things about engineering. And probably teamwork. Yeah, right, right, exactly. So it's the friends you make along the way. So the way that they build this is rather than trying to predict the next word, although that will work, that will actually give you word embeddings, but they're not that good because they're only based on the immediately adjacent word. You look sort of around the word. So you give it a word and then you sample from the neighborhood of that word randomly another word and you train the network to predict that. So the idea is that at the end, when this thing is fully trained, you give it any word and it's going to give you a probability distribution over all of the words in your dictionary, which is like how likely are each of these words to show up within five words of this first word or within ten or, you know, something like that. If the system can get really good at this task, then the weights of this hidden layer in the middle have to encode something meaningful about that input word. And so if you imagine the word cat comes in, in order to do well, the probability distribution of surrounding words is going to end up looking pretty similar to the output that you would want for the word dog. So it's going to have to put those two words close together if it wants to do well at this task. And that's literally all you do. So if you run this on a lot, it's absurdly simple, right? But if you run it on a large enough data set and give it enough compute to actually perform really well, it ends up giving you each- giving you for each word a vector that's of length however many units you have in your hidden layer, which- for which the nearbyness of those vectors expresses something meaningful about how similar the contexts are that those words appear in. And our assumption is that words that appear in similar contexts are similar words. And it's slightly surprising how well that works and how much information it's able to extract. So it ends up being a little bit similar, actually, to the way that the generative adversarial network does things, where we're training it to produce good images from random noise. And in the process of doing that, it creates this mapping from the latent space to images. By doing basic arithmetic, like just adding and subtracting vectors on the latent space, would actually produce meaningful changes in the image. So what you end up with is that same principle, but for words. So if you take, for example, the vector, and it's required by law that all explanations of word embeddings use the same example to start with. So if you take the vector for king, subtract the vector for man, and add the vector for woman, you get another vector out. And if you find the nearest point in your word embeddings to that vector, it's the word queen. And so there's a whole giant swathe of ways that ideas about gender are encoded in the language, which are all kind of captured by this vector, which we won't get into, but it's interesting to explore. I have it running, and we can play around with some of these vectors and see where they end up. So I have this running in Google Colab, which is very handy. I'm using word embeddings that were found with the word2vec algorithm using Google News. Each word is mapped to 300 numbers. Let's check whether what we've got satisfies our first condition. We want dog and cat to be relatively close to each other, and we want cat to be, like, further away from car than it is from dog, right? We can just measure the distance between these different vectors. I believe you just do model.distance, distance between car and cat, okay, is 0.784, and then the distance between, let's say, dog and cat, 0.23, right? Dog and cat are closer to each other. This is a good start, right? And, in fact, we can... let's find all of the words that are closest to cat, for example. Okay, so the most similar word to cat is cats. Makes sense? Followed by dog, kitten, feline, beagle, puppy, pup, pet, felines, and chihuahua, right? So this is already useful. It's already handy that you can throw any word at this, and it will give you a list of the words that are similar. Whereas, like, if I put in car, I get vehicle, cars, SUV, minivan, truck, right? So this is working. The question of directions is pretty interesting. So, yeah, let's do the classic example, which is this. If you take the vector for king, subtract the vector for man, add the vector for woman, what you get, somewhat predictably, is queen. And if you put in boy here, you get girl. If you put in father, you get mother. Yeah, and if you put in shirt, you get blouse. So this is reflecting something about gender that's in the data set that it's using. This reminds me a little bit of the unicorn thing where, you know, the transformer was able to infer all sorts of, or appear to have knowledge about the world because of language. Right, right. But the thing that I like about this is that that transformer is working with 1.5 billion parameters. And here we're literally just taking each word and giving 300 numbers, you know? If I go from London, and then subtract England, and then add, I don't know, Japan. We'd hope for Tokyo. We'd hope for Tokyo. And we get Tokyo. We get Tokyo twice, weirdly. Tokyo, Tokyo. Why is, oh, oh sorry, it's, no we don't. We get Tokyo and Toyko. Ah, which is... A typo, I guess. And so yeah, USA in New York. Ah, okay. It's interesting. Maybe it's thinking largest city of, yeah. Right, right. Like the exact relationship here isn't clear. We haven't specified that. What does it give us for Australia? I bet it's, yeah, it's Sydney. Sydney, Melbourne. So it's, yeah, it's not doing capital. It's just doing largest city. Right. But that's cool. It's cool that we can extract the largest city, and like this is completely unsupervised. It was just given a huge number of news articles, I suppose. And it's pulled out that there's this relationship, and that you can follow it for different things. You can take the vector from pig to oink, right? Okay. And then like you put cow in there, that's moo. You put cat in there, and you get meowing. You put dog in there. You get box, right? Close enough for me. Yeah, yeah. You put, but then it gets surreal. You put Santa in there. Ho, ho, ho. Right? That's fantastic. What does the fox say? What? What does it say? It says Phoebe. What? So it doesn't know, basically. Although the second thing is chittering. Do foxes chitter? I don't know. Or gabble? Don't they go ding-a-ling-a-ling, ding, ding, ding, ding, ding, ding, ding, ding? Not in this data set. 