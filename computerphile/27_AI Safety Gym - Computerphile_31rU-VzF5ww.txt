This episode has been brought to you by Fasthosts, find out more about them later. So I wanted to talk about this paper out of open AI, Benchmarking Safe Exploration in Deep Reinforcement Learning. What comes with this? There's a blog post. So I wanted to explain this because when I saw the blog post, and I think a lot of computerphile viewers would have the same reaction, what I thought is, what the hell is a GIME? GIME? What's a GIME? Some kind of meatspace thing, I don't know. So it's a bunch of these environments, right, that allow you to train. The OpenAI Safety GIME Benchmark Suite, which is a bunch of these environments that you can run your systems in and have them learn. Is this anything to do with those AI grid worlds we talked about? Yeah, yeah, kind of. In the same way that the Gridworlds paper did. This paper introduces environments that people can use to test their AI systems. And this is focusing specifically on safe exploration and has a few differences. They're kind of complementary. The environments in this are a little bit more complex. They're continuous in time and in space in a way that the Gridworlds are all very discrete. You take turns and you move by one square, whereas in this case it's a lot more like Majoko where you actually have a physics simulation that the simulated robots move around in. So it's a slightly more complex kind of environment. But the idea is to have, in the same way as with Gridworlds or anything else, to have a standardized set of environments so that you know everybody's comparing like with like and you actually have standardized measurements that you can benchmark, you can compare different approaches and actually have metrics that tell you which one's doing better. Which is like, it's not super glamorous, but it's a real prerequisite for how progress actually gets made in the real world. If you can't measure it, it's very hard to make progress or know if you're making progress. The problem of safe exploration is in reinforcement learning, which is one of the most important and popular ways of creating AI systems for various types of problem. The system is interacting with an environment and it's trying to maximize the amount of reward that it gets. So you write a reward function and then it's trying to get as much out of that as it can. And the way that it learns is by interacting with the environment. And so this basically looks like trial and error, right? It's doing things and then it gets the reward signal back and it learns, oh, that was a good thing to do. That was a bad thing to do. And the problem with that is it's very difficult to do that safely. And it's kind of a fundamental problem because in order to do exploration, you have to be taking actions that you don't know what the result is going to be, right? The only way that you can learn is by trying things that you're not sure about. If you're trying random things, some of those things are going to be things that you really shouldn't be doing. In any exploration, there's danger, right? I mean, that sort of goes with territory for human explorers. So Apollo 11, right? Right. We'd done a bit of research. We'd send some spaceships out. We had an idea of what to what. But it was still a dangerous thing to go and land on the moon. Exploring comes with danger. Right, right. Yeah. But there are there are safe ways to do it or there are safer ways to do it. They could have tried to launch astronauts on the first thing that they ever sent to the moon. They didn't do that because they knew how much they didn't know and they didn't want to risk it until they actually had a pretty good understanding of what they were dealing with. And it's kind of similar. Like if you look at some of the standard reinforcement learning approaches to exploration, what they involve is doing things—often what they involve is doing things completely at random, right? So you just—especially at the beginning of the training process, where you really don't understand the dynamics of the environment, you just flail and see what happens, right? And human beings actually pretty much do this as well. It's just that when babies flail, they aren't really able to hurt anything. But if you have a three-ton robot arm flailing around trying to learn the dynamics of the environment, it could break itself, it could hurt somebody, you know? But when you mention three-ton robot arms flailing around, I'm guessing that the people who do that kind of development will have done some kind of simulation before they've built the thing, right? Right. There's two sides to it, right? Part of the reason why we haven't had that much safe exploration research is because simulations are good, but also part of why we use simulations so much is because we don't know how to safely do it in the real world. For very simple tasks, you can write good simulators that accurately represent all of the dynamics of the environment properly, but if you want a system that's doing something Like, generally speaking, with these robots, for example, you still don't go near them. They don't smash themselves up and they don't smash the environment up because you've simulated that. But while they're operating, how do you write a simulation of how a human being actually moves in an environment with a robot? This is why you look at self-driving cars. They train them a huge amount in simulation, but it's not good enough. It doesn't capture the complexity and the diversity of things that can happen in the real world, and it doesn't capture the way that actual human drivers act and react. So everyone who's trying to make self-driving cars, they are driving millions and millions of real world miles because they have to, because simulation doesn't cut it. And that is a situation where, now they're not just running reinforcement learning on those cars, right? We don't know how to safely explore in a self-driving car type situation in the real world. Trying random inputs to the controls is, like, not viable. If you're using reinforcement learning, if you have something that you don't want the agent to do, you give it a big penalty, right? So you might build a reward function that's like, I want you to get from here to here, and you get points for getting there faster. But if there's a collision, then you get minus large number of points. Sometimes people talk about this problem as though reward functions are, like, not able to represent the behavior that we actually want. People say, you can't write a reward function that represents this. And it's like, well, I mean, you can write a reward function, but, like, you can't, right? Like, plausibly, it's possible, but, like, how are you actually going to do it? So like, yeah, you're giving a big penalty to collisions, but, like, how do you decide that penalty? What should it be? You have this problem, which is that in the real world, people are actually making a tradeoff between speed and safety all the time. Any time you go just a little bit after the light has turned red, right, or just a little bit before the light has turned green, you're accepting some amount of risk for some amount of time. If you go after it's gone red for long enough, you will meet someone who went a bit early on the green and, you know, teach each other things about the tradeoff between speed and safety that will stay with you for the rest of your life. People talk about it like, oh, what we want is no crashes. And that's not actually how it works, because that would correspond, if you wanted that, that would correspond to sort of infinite negative reward for a collision. And in that scenario, the car doesn't go anywhere. If that was what we really thought, then the speed limit would be 0.001 miles an hour. There is some, like, acceptable tradeoff between speed and safety that we want to make. The question is, how do you actually pick the size of that punishment to make it sensible? Like what, what, how do you, how do you find that implicitly? It's kind of difficult. One approach that you can take to this, which is the one that this paper recommends, is called constrained reinforcement learning, where you have your reward function, and then you also have constraints on these cost functions. So standard reinforcement learning, you're just finding the policy that gets the highest reward, right? Whereas in constrained reinforcement learning, you're saying, given only the set of policies that crashes less than once per however many million miles, find the one of those that maximizes reward. So you're, you're, you're maximizing reward within these constraints. Yeah, reinforcement learning and constrained reinforcement learning are both sort of frameworks. They're ways of laying out a problem. They're not like algorithms for tackling a problem. They're, they're, they're a formalization that lets you develop algorithms. I guess like sorting or something, you know, you've got a general idea of like you have a bunch of things and you want them to be in order, but like how many there are, what kind of things there are, what the process is for comparing them, and then there's different algorithms that can, that can tackle it. I haven't seen a proof for this, but I think that for any constrained reinforcement learning setup, you could have one that was just a standard reward function. But this is like a much more intuitive way of expressing these things. So it kind of reminds me of, there's a bit in Hitchhiker's Guide where somebody's like, I've got, oh, you've got a solution. No, but I've got a different name for the problem. I mean, this is better than that, because it's a different way of formalizing the problem, a different way of sort of specifying what the problem is. And actually a lot of the time, finding the right formalism is a big part of the battle, right? So the question, how do you explore safely, is like, underdefined, you can't really do computer science on it. You need something that's expressed in the language of mathematics. And that's what constrained reinforcement learning does. It gives you a slightly more intuitive way of specifying, rather than just having this one thing, which is this single value, are you doing well or not? You get to specify, here's the thing you should do, and then here's also the thing or things that you shouldn't do. Slightly more natural, slightly more human-friendly formalism that makes it, you would hope would make it easier to write these functions, to get the behavior that you want in the real world. It's also nice because if you're trying to learn, so I did a video recently on my channel about reward modeling, where you actually learn the reward function, rather than writing the reward function. Part of your training system is actually learning what the reward function should be in real time. And the idea is that this might help with that as well. It's kind of easier to learn these things separately, rather than trying to learn several things at the same time. And it also means you can transfer them more easily. Like if you have a robot arm and it's making pens and you want to retrain it to make mugs or something like that, then it would be that you would have to just relearn the reward function completely. But if you have a constraint that it's learned that's like, don't hit humans, that is actually the same between these two tasks. So then it's only having to relearn the bits that are about the making the thing and the constraints, it can just keep them from one to the next. So it should improve performance and training speed and also safety. So it's a nice kind of win-win. The other thing that's kind of different about various formulations of constrained reinforcement learning is you care about what happens during training as well. Standard reinforcement learning, you are just trying to find a policy that maximizes the reward and how you do that is kind of up to you. But that means that standard reinforcement learning systems in the process of learning will do huge amounts of unsafe stuff. Whereas in a constrained reinforcement learning setting, you actually want to keep track of how often the constraints are violated during the training process. And you also want to minimize that, which makes the problem much harder because it's not just make an AI system that doesn't crash, but it's like make an AI system that in the process of learning how to drive at all, crashes as little as possible, which just makes the whole thing much more difficult. And so we have these simplified environments that you can test your different approaches. And they're fairly kind of straightforward reinforcement learning type setups. You have these simulated robots. There's four of them. You've got Point, which is just a little round robot with a square on the front that can turn and drive around. Car, which is a similar sort of setup. Yeah, it has differential drive, so you have input to both of the wheels, sort of tank steering type setup. And that drives around. And you have Doggo, I kid you not, D-O-G-G-O, which is a quadruped that walks around. And then you have a bunch of these different environments, which are basically like you have to go over here and press this button, and then when you press the button, a different button will light up and you have to go over and press that one and so on. Or get to this point, or like push this box to this point, you know, it's basic interactions. But then they also have these constraints built in, which are things like hazards, which are like areas that you're not supposed to go into, or vases, they call them vases, which is like objects that you're not supposed to bump into. And then the hardest one is gremlins, which are objects that you're not supposed to touch, but they move around as well. The idea is you're trying to create systems that can learn to get to the areas they're supposed to be, or push the box or, you know, press the buttons or whatever it is that they're trying to do, while simultaneously avoiding all of these hazards, not breaking the vases, not bumping into the gremlins or whatever else. And that they can learn with a minimum of violating these constraints during the training process as well, which is a really interesting and quite hard problem. And then they provide some benchmarks, and they show that standard reinforcement learning agents suck at this. They're trying to do anything to learn, they don't care about the learning process, right? Exactly, exactly. And then there are a few different other approaches that do better. This is really nice if you have ideas. And again, like the Gridworlds thing, you can download this and have a go. You can try training your own agents and see how well you can do on these benchmarks, and if you can beat what OpenAI has done, you know, and then you've got something that's publishable that's going to advance the field. So I really like this as a piece of work because it provides a foundation for more work going forward in a kind of standardised, understandable way. Fasthosts is a UK-based web hosting company which offers a wide range of web hosting products and other services. They aim to support UK businesses and entrepreneurs at all levels, providing effective and affordable hosting packages to suit any need. As you'd expect from someone called Fasthosts, they do domain names. It's easy to register, and they have a huge choice of domains with powerful management features included. One thing they do offer is an e-commerce website builder. This provides a fast and simple way for any business to sell online. It's a drag-and-drop interface, so it's easy to build a customised shop on the web. Even if you have no technical knowledge, you can create an online store, and you can customise simply with drag-and-drop functionality. No designers or developers are required. Fasthosts data centres are based in the UK alongside their offices, so whether you choose a lightweight web hosting package or go for a fully-fledged dedicated box, their expert support teams are available 24x7. Find out more by following the link in the description below. 