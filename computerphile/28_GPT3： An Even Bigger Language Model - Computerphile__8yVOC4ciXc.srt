1
00:00:00,000 --> 00:00:04,320
Rob, welcome back to Computerphile in these strange times that we find ourselves recording in.

2
00:00:04,320 --> 00:00:08,320
You've got the green screen up there, we're having a few laggy problems with the communications.

3
00:00:08,320 --> 00:00:10,320
What are you going to talk about today then?

4
00:00:10,320 --> 00:00:14,400
Yeah, I thought today it would make sense to talk about GPT-3.

5
00:00:14,400 --> 00:00:19,200
Because before we had those videos about language models and transformers and GPT-2.

6
00:00:19,200 --> 00:00:21,200
People seemed to like, I think those came out quite well.

7
00:00:21,200 --> 00:00:23,200
And now, there's a new one.

8
00:00:23,200 --> 00:00:29,200
So is it bigger, better? What's the deal there then?

9
00:00:29,200 --> 00:00:31,200
It is both bigger and better.

10
00:00:31,200 --> 00:00:33,200
That's the headline.

11
00:00:33,200 --> 00:00:43,200
So the thing about GPT-2 is just that it was much bigger than anything that came before.

12
00:00:43,200 --> 00:00:47,200
It was more parameters and just a larger language model.

13
00:00:47,200 --> 00:00:51,200
And that was kind of the point of that paper.

14
00:00:51,200 --> 00:00:57,200
The point it was trying to make is, people in natural language processing have spent a tremendously long time

15
00:00:57,200 --> 00:01:03,200
working on all of these clever things that you can do.

16
00:01:03,200 --> 00:01:11,200
Getting in the nitty-gritty of the technical stuff for detailed fine-tuning for these different benchmarks that they have and so on.

17
00:01:11,200 --> 00:01:17,200
And GPT-2 was OpenAI just saying, well, what if we just made a really, really huge one?

18
00:01:17,200 --> 00:01:19,200
What would happen?

19
00:01:19,200 --> 00:01:27,200
Even without any fine-tuning, by which I mean, well, we talked about all of this before, so I won't get into that detail.

20
00:01:27,200 --> 00:01:37,200
But that GPT-2 could do reasonably well on all of these different tasks, even though it hadn't been trained on those tasks.

21
00:01:37,200 --> 00:01:43,200
It was trained purely as a language model, which means all it's trying to do is predict the next word or the next token,

22
00:01:43,200 --> 00:01:47,200
given what it's seen so far.

23
00:01:47,200 --> 00:01:53,200
And so that was kind of an interesting finding, that if you just took this quite simple architecture,

24
00:01:53,200 --> 00:01:59,200
the transformer is not that complicated an architecture.

25
00:01:59,200 --> 00:02:05,200
And the dataset is not a fancy or sophisticated, it's not like a highly, what am I saying?

26
00:02:05,200 --> 00:02:11,200
It's not like a high-effort structured dataset, it's just like a giant pile of text.

27
00:02:11,200 --> 00:02:19,200
I'm not sure it's good quality text, but it's just unstructured, just any text you could get from the Internet.

28
00:02:19,200 --> 00:02:25,200
And it's able to perform reasonably well, sometimes state-of-the-art, sometimes not,

29
00:02:25,200 --> 00:02:31,200
on all of these specific benchmarks for different natural language processing tasks.

30
00:02:31,200 --> 00:02:35,200
And that was very impressive.

31
00:02:35,200 --> 00:02:41,200
The thing, I think I said this at the time, is that the graphs were still curving upwards.

32
00:02:41,200 --> 00:02:45,200
You have these like, well not curving upwards, but they were straight, they weren't curving down.

33
00:02:45,200 --> 00:02:49,200
So generally speaking, you start to get diminishing returns, right?

34
00:02:49,200 --> 00:02:53,200
You can't just keep making the model bigger forever.

35
00:02:53,200 --> 00:02:55,200
It was plateauing as it got bigger and bigger.

36
00:02:55,200 --> 00:03:01,200
Yeah, that's the thing, it's like, you usually would expect it to plateau, but at the level of GPT-2 it was not.

37
00:03:01,200 --> 00:03:05,200
So not only was this model bigger than anything that came before,

38
00:03:05,200 --> 00:03:13,200
but also there was an indication that just scaling up is like, we haven't reached the limits of that yet.

39
00:03:13,200 --> 00:03:19,200
And that was kind of surprising, because you would generally expect these things to plateau, right?

40
00:03:19,200 --> 00:03:27,200
You would expect to hit the limits of the data, the information contained in the data, the dataset size,

41
00:03:27,200 --> 00:03:33,200
but also maybe the training regime, maybe the whole approach, right?

42
00:03:33,200 --> 00:03:35,200
What's the biggest airplane you can build? It's pretty big.

43
00:03:35,200 --> 00:03:39,200
But there comes a point where the airplane approach doesn't work anymore.

44
00:03:39,200 --> 00:03:43,200
And if you want to go higher, you need a rocket, right?

45
00:03:43,200 --> 00:03:45,200
You need like a whole different approach.

46
00:03:45,200 --> 00:03:47,200
You can't just scale up what you have.

47
00:03:47,200 --> 00:03:53,200
What they found with GPT-2 was, not only could you make the model much bigger, and it continued to get better,

48
00:03:53,200 --> 00:04:01,200
but also the rate at which it continued to get better is still pretty much a straight line with the scaling of smaller models.

49
00:04:01,200 --> 00:04:07,200
So since that time, various people have got on the bigger language models train

50
00:04:07,200 --> 00:04:11,200
and tried making new things, you know, bigger and bigger language models,

51
00:04:11,200 --> 00:04:17,200
and they keep doing better and better, kind of according to what you would expect.

52
00:04:17,200 --> 00:04:25,200
And then for GPT-3, what OpenAI has done is come along and said essentially the same thing they said for GPT-2,

53
00:04:25,200 --> 00:04:29,200
which is, okay, but what if we made a bigger one?

54
00:04:29,200 --> 00:04:32,200
And everybody's like, well, we did make a bigger one.

55
00:04:32,200 --> 00:04:34,200
It's like, no, what if we made like a bigger, bigger one?

56
00:04:34,200 --> 00:04:39,200
Well, I can see it now. Somebody's plotted this graph, and then there's somebody there at the back of the room going,

57
00:04:39,200 --> 00:04:42,200
that's still going up. That's still going up there.

58
00:04:42,200 --> 00:04:46,200
Yeah, right. Like, how far can we ride this thing? Let's find out.

59
00:04:46,200 --> 00:04:54,200
So you remember the GPT-2, they released the 117 million parameter model,

60
00:04:54,200 --> 00:04:57,200
and they didn't release the larger models immediately, right?

61
00:04:57,200 --> 00:05:00,200
Because there were some concerns about possible misuse.

62
00:05:00,200 --> 00:05:05,200
And over time, they steadily released larger and larger models.

63
00:05:05,200 --> 00:05:09,200
The largest GPT-2 model was 1.5 billion parameters.

64
00:05:09,200 --> 00:05:14,200
So GPT-3 is 175 billion parameters.

65
00:05:14,200 --> 00:05:17,200
Wow. Okay.

66
00:05:17,200 --> 00:05:23,200
Yeah, you need a lot of compute and a lot of money to run it.

67
00:05:23,200 --> 00:05:28,200
So yeah, they did have to do some clever engineering,

68
00:05:28,200 --> 00:05:37,200
because like GPT-2, you can put it on a single machine at inference time.

69
00:05:37,200 --> 00:05:39,200
Whereas I don't think you can do that with GPT-3.

70
00:05:39,200 --> 00:05:44,200
I think you need a sort of a cluster to run it.

71
00:05:44,200 --> 00:05:50,200
But yeah, so the big finding with this giant model, which is about 10 times bigger,

72
00:05:50,200 --> 00:05:56,200
it's 117 times bigger than GPT-2, and about 10 times bigger than the previous biggest thing,

73
00:05:56,200 --> 00:06:01,200
which was Turing NLG.

74
00:06:01,200 --> 00:06:07,200
And what they find is when they look at the graphs, they're still going up.

75
00:06:07,200 --> 00:06:12,200
Oh, no. That person in the back of the room is going to be thinking still.

76
00:06:12,200 --> 00:06:14,200
What if, what if?

77
00:06:14,200 --> 00:06:17,200
Right. We could still go bigger.

78
00:06:17,200 --> 00:06:21,200
And it does look like it would continue to get better.

79
00:06:21,200 --> 00:06:26,200
So how good is it?

80
00:06:26,200 --> 00:06:31,200
Some of the main takeaways are when you have it write an article

81
00:06:32,200 --> 00:06:39,200
and you ask human beings to differentiate articles written by GPT-3 from articles written by humans,

82
00:06:39,200 --> 00:06:43,200
they get it right about 52% of the time.

83
00:06:43,200 --> 00:06:45,200
I'm looking at the table here.

84
00:06:45,200 --> 00:06:50,200
It says human accuracy in identifying whether short news articles are model generated.

85
00:06:50,200 --> 00:06:52,200
These are articles of about 200 words.

86
00:06:52,200 --> 00:06:57,200
And basically they tried generating with all of the different sizes.

87
00:06:57,200 --> 00:07:07,200
So GPT-3 small, medium, and large on this are, I think, equivalent sizes to the GPT-2 ones.

88
00:07:07,200 --> 00:07:16,200
And then you can see how the accuracy with which humans are able to identify just steadily goes down, basically.

89
00:07:16,200 --> 00:07:23,200
The small model, they are 76% of the time able to tell correctly if it's human or AI

90
00:07:24,200 --> 00:07:29,200
and then just steadily drops down until you get to the 175 billion parameter model where they're at 52%.

91
00:07:29,200 --> 00:07:33,200
What I thought was it would be fun to run a little experiment with everybody at home

92
00:07:33,200 --> 00:07:37,200
because they had the thing generate some poems and there are samples in the paper.

93
00:07:37,200 --> 00:07:41,200
The way that you get this model to produce things is you give it some text and then you say,

94
00:07:41,200 --> 00:07:43,200
and now it's your turn to continue from here.

95
00:07:43,200 --> 00:07:49,200
So they gave it something which kind of looks like it's from a compendium of poetry.

96
00:07:49,200 --> 00:07:54,200
So it has the title of the poem by this person and then the full text of the poem

97
00:07:54,200 --> 00:07:56,200
and then the title of another poem.

98
00:07:56,200 --> 00:08:03,200
So what I thought it would do, because we know the poet that GPT-3 is trying to imitate,

99
00:08:03,200 --> 00:08:08,200
that I could try reading, like randomly picking one of Wallace Stevens' actual poems

100
00:08:08,200 --> 00:08:10,200
and randomly picking one of these.

101
00:08:10,200 --> 00:08:13,200
I think these aren't cherry-picked either.

102
00:08:13,200 --> 00:08:16,200
Yeah, uncurated completions.

103
00:08:17,200 --> 00:08:19,200
And then we'll see.

104
00:08:19,200 --> 00:08:23,200
So I'm going to randomly pick one of each and then I'm going to randomly decide which one I'll read first

105
00:08:23,200 --> 00:08:25,200
so that you don't get any clues.

106
00:08:25,200 --> 00:08:30,200
Okay, so this poem, they're both by Wallace Stevens.

107
00:08:30,200 --> 00:08:34,200
This first poem is called Shadows on the Way.

108
00:08:34,200 --> 00:08:36,200
I must have shadows on the way.

109
00:08:36,200 --> 00:08:42,200
If I am to walk, I must have each step taken slowly and alone to have it ready made.

110
00:08:43,200 --> 00:08:48,200
And I must think in lines of grey, to have dim thoughts to be my guide,

111
00:08:48,200 --> 00:08:53,200
must look on blue and green and never let my eye forget that colour is my friend

112
00:08:53,200 --> 00:08:55,200
and purple must surround me too.

113
00:08:55,200 --> 00:09:01,200
The yellow of the sun is no more intrusive than the bluish snow that falls on all of us.

114
00:09:01,200 --> 00:09:07,200
I must have grey thoughts and blue thoughts walk with me, if I am to go away at all.

115
00:09:07,200 --> 00:09:09,200
That's one poem.

116
00:09:09,200 --> 00:09:14,200
The other one is titled Fablio of Florida.

117
00:09:14,200 --> 00:09:17,200
Bark of phosphor on the balmy beach.

118
00:09:17,200 --> 00:09:22,200
Move outward into heaven, into the alabasters and night blues.

119
00:09:22,200 --> 00:09:24,200
Foam and cloud are one.

120
00:09:24,200 --> 00:09:27,200
Sultry moon monsters are dissolving.

121
00:09:27,200 --> 00:09:30,200
Fill your black hull with white moonlight.

122
00:09:30,200 --> 00:09:34,200
There will never be an end to this droning of the surf.

123
00:09:36,200 --> 00:09:38,200
Everybody place your bets.

124
00:09:38,200 --> 00:09:42,200
The problem is, people who know poetry really well,

125
00:09:42,200 --> 00:09:47,200
who would be well placed to decide which of these they prefer or whatever,

126
00:09:47,200 --> 00:09:50,200
the chances are they'll know the originals.

127
00:09:50,200 --> 00:09:52,200
So, it's hard to get a fair test.

128
00:09:52,200 --> 00:09:56,200
Without magical Google, I have no idea which is which.

129
00:09:56,200 --> 00:09:59,200
I mean, I don't know, should we reveal it here on Computefile

130
00:09:59,200 --> 00:10:02,200
or should we let people have a think about it or should we say it at the end?

131
00:10:02,200 --> 00:10:04,200
Yeah, maybe at the end of the video.

132
00:10:04,200 --> 00:10:09,200
Poetry is one thing and at the risk of offending some poetry fans,

133
00:10:09,200 --> 00:10:14,200
it can be thought of as kind of ethereal and maybe not so grounded in fact

134
00:10:14,200 --> 00:10:18,200
and therefore it's okay to predict that sort of stuff and to emulate a poet.

135
00:10:18,200 --> 00:10:21,200
But what about things like scientific papers?

136
00:10:21,200 --> 00:10:24,200
If you fed it enough science, enough scientific papers,

137
00:10:24,200 --> 00:10:28,200
do you think, could it come up with something that we've not really realised before

138
00:10:28,200 --> 00:10:30,200
or something new?

139
00:10:30,200 --> 00:10:35,200
Yeah, so my instinct is to say,

140
00:10:35,200 --> 00:10:39,200
no, it's just predicting the next word, right?

141
00:10:39,200 --> 00:10:41,200
It's just a language model.

142
00:10:41,200 --> 00:10:48,200
It doesn't have the ability to build the kind of abstract mental structures

143
00:10:48,200 --> 00:10:54,200
that you need in order to actually kind of synthesise new knowledge.

144
00:10:55,200 --> 00:11:03,200
But there's a kind of an outside view that says that we thought that about a bunch of things

145
00:11:03,200 --> 00:11:05,200
that it now seems to be doing.

146
00:11:05,200 --> 00:11:08,200
So I'm not going to say that it definitely couldn't do that.

147
00:11:08,200 --> 00:11:16,200
So one example of a task which it got better at, tremendously better at,

148
00:11:16,200 --> 00:11:21,200
is arithmetic, which is kind of an interesting task

149
00:11:21,200 --> 00:11:26,200
because again, it's a language model, it's not trying to do arithmetic,

150
00:11:26,200 --> 00:11:28,200
it's not designed to do arithmetic.

151
00:11:28,200 --> 00:11:34,200
So with GPT-2, if you put in 2 plus 2 equals

152
00:11:34,200 --> 00:11:38,200
and get it to give you the next token, it will give you a 4.

153
00:11:38,200 --> 00:11:42,200
But that's not very surprising, that's not very impressive

154
00:11:42,200 --> 00:11:46,200
because you would expect in its dataset to see that string 2 plus 2

155
00:11:46,200 --> 00:11:48,200
followed by the string 4 very many times.

156
00:11:49,200 --> 00:11:51,200
That's pure memorisation, right?

157
00:11:51,200 --> 00:11:55,200
The thing doesn't have to have any understanding of what numbers are at all.

158
00:11:55,200 --> 00:11:59,200
It can just see the sequence of tokens, give you the next one.

159
00:11:59,200 --> 00:12:02,200
And then the problem gets harder and harder,

160
00:12:02,200 --> 00:12:06,200
the more like 23 plus 48 or something,

161
00:12:06,200 --> 00:12:12,200
that's more difficult because it's less likely that that specific string

162
00:12:12,200 --> 00:12:16,200
has appeared in the training dataset.

163
00:12:16,200 --> 00:12:20,200
So this gets more and more difficult

164
00:12:20,200 --> 00:12:24,200
and more and more like actual reasoning.

165
00:12:24,200 --> 00:12:28,200
You couldn't see me doing big hair quotes there.

166
00:12:28,200 --> 00:12:31,200
The longer the numbers get, right?

167
00:12:31,200 --> 00:12:34,200
If you can reliably add 10-digit numbers together,

168
00:12:34,200 --> 00:12:37,200
then it's hard to deny that what you're really doing,

169
00:12:37,200 --> 00:12:39,200
you have to really be doing addition, right?

170
00:12:39,200 --> 00:12:44,200
There's no way you could memorise to that.

171
00:12:44,200 --> 00:12:47,200
But it's kind of interesting because GPT-3 does way better,

172
00:12:47,200 --> 00:12:50,200
but it can't add 10-digit numbers.

173
00:12:50,200 --> 00:12:54,200
So let me find the graph because they graphed this.

174
00:12:54,200 --> 00:12:57,200
So it starts to run out of steam effectively.

175
00:12:57,200 --> 00:12:58,200
Right.

176
00:12:58,200 --> 00:13:00,200
Much as a human does.

177
00:13:00,200 --> 00:13:01,200
Yep.

178
00:13:01,200 --> 00:13:04,200
So what I'm looking at now is a graph of performance

179
00:13:04,200 --> 00:13:07,200
on a bunch of different arithmetic tasks.

180
00:13:07,200 --> 00:13:11,200
And you can see that just going up to like 2-digit addition,

181
00:13:11,200 --> 00:13:14,200
GPT-2 does pretty poorly.

182
00:13:14,200 --> 00:13:17,200
So the 1.3 billion parameter model,

183
00:13:17,200 --> 00:13:20,200
which I guess is the closest equivalent,

184
00:13:20,200 --> 00:13:23,200
better than chance but not much at all.

185
00:13:23,200 --> 00:13:27,200
So the thing is, so 2-digit addition and 3-digit addition

186
00:13:27,200 --> 00:13:31,200
are things which, like by the time you're at 3-digit addition,

187
00:13:31,200 --> 00:13:34,200
you're not going to be memorising from the dataset

188
00:13:34,200 --> 00:13:38,200
because firstly, I think that the cleaning of the dataset

189
00:13:38,200 --> 00:13:40,200
made some attempt to remove,

190
00:13:40,200 --> 00:13:43,200
if there was something that was just like a giant table

191
00:13:43,200 --> 00:13:45,200
of the times tables or something like that.

192
00:13:45,200 --> 00:13:48,200
I think they tried to remove that from the dataset.

193
00:13:48,200 --> 00:13:54,200
And secondly, if you're doing 3-digit addition,

194
00:13:54,200 --> 00:13:58,200
that's a million different possible problems, right?

195
00:13:58,200 --> 00:14:02,200
That's like quite a lot of network capacity to do by memorisation.

196
00:14:02,200 --> 00:14:05,200
People learn multiplication tables.

197
00:14:05,200 --> 00:14:09,200
And this is like apparently the most effective way

198
00:14:09,200 --> 00:14:12,200
of teaching something that works like a human brain.

199
00:14:12,200 --> 00:14:16,200
And then you have some procedural rules for taking,

200
00:14:16,200 --> 00:14:19,200
you memorise that 3 plus 3 is 6,

201
00:14:19,200 --> 00:14:23,200
and then you have these procedural rules about like carrying

202
00:14:23,200 --> 00:14:28,200
and those kinds of things to do larger additions.

203
00:14:28,200 --> 00:14:32,200
And then you iteratively like systematically apply that.

204
00:14:33,200 --> 00:14:37,200
But yeah, the larger the numbers get, the harder it is to memorise.

205
00:14:37,200 --> 00:14:39,200
And they actually ran an analysis.

206
00:14:39,200 --> 00:14:43,200
So they searched for the addition problems.

207
00:14:43,200 --> 00:14:45,200
They searched for 2,000 of them,

208
00:14:45,200 --> 00:14:47,200
just looking through the whole dataset.

209
00:14:47,200 --> 00:14:51,200
Does 38 plus 49 exist anywhere in this dataset?

210
00:14:51,200 --> 00:14:53,200
And they found 17 matches, right?

211
00:14:53,200 --> 00:14:59,200
So 0.85% of these problems occurred in the database.

212
00:14:59,200 --> 00:15:07,200
But GPT-3's performance on 2-digit addition is extremely good.

213
00:15:07,200 --> 00:15:09,200
It's basically 100% of the time.

214
00:15:09,200 --> 00:15:11,200
2-digit subtraction, only slightly worse.

215
00:15:11,200 --> 00:15:14,200
And then 3-digit addition and subtraction.

216
00:15:14,200 --> 00:15:18,200
Again, it's getting like 80%, 90%.

217
00:15:18,200 --> 00:15:22,200
And it's a big jump from the smaller models.

218
00:15:22,200 --> 00:15:26,200
What they're kind of suggesting in the paper is

219
00:15:26,200 --> 00:15:33,200
that it has actually learned how to learn.

220
00:15:33,200 --> 00:15:34,200
Okay.

221
00:15:34,200 --> 00:15:40,200
Like that's the interpretation of this that they're pushing,

222
00:15:40,200 --> 00:15:50,200
that in order to perform sufficiently well at this language modelling task,

223
00:15:50,200 --> 00:15:55,200
the best thing to do is to actually, while looking at the context,

224
00:15:55,200 --> 00:16:00,200
learn specific rules for how the context behaves

225
00:16:00,200 --> 00:16:02,200
so that you can continue it more accurately.

226
00:16:02,200 --> 00:16:04,200
Okay. Wow.

227
00:16:04,200 --> 00:16:07,200
Yeah.

228
00:16:07,200 --> 00:16:10,200
And so I have an example.

229
00:16:10,200 --> 00:16:12,200
I have like a way of thinking about this,

230
00:16:12,200 --> 00:16:14,200
which is not that tight an analogy,

231
00:16:14,200 --> 00:16:16,200
but I think it might be helpful.

232
00:16:16,200 --> 00:16:17,200
Yeah, go on then.

233
00:16:17,200 --> 00:16:18,200
Okay.

234
00:16:18,200 --> 00:16:21,200
So suppose you're doing like Sim2Real.

235
00:16:21,200 --> 00:16:22,200
You have a robotics task.

236
00:16:22,200 --> 00:16:26,200
You're training an agent to do some thing with a robot, right?

237
00:16:26,200 --> 00:16:28,200
Running things on the physical robot is super slow,

238
00:16:28,200 --> 00:16:29,200
so you're using a simulation.

239
00:16:29,200 --> 00:16:30,200
But you have a problem,

240
00:16:30,200 --> 00:16:33,200
which is that the simulation is not exactly the same as reality.

241
00:16:33,200 --> 00:16:35,200
You have this physics model that you've built

242
00:16:35,200 --> 00:16:39,200
that is supposed to simulate exactly the robot

243
00:16:39,200 --> 00:16:40,200
and the environment of the robot

244
00:16:40,200 --> 00:16:42,200
so that it can learn in the simulation and transfer it.

245
00:16:42,200 --> 00:16:44,200
In practice, that doesn't work very well

246
00:16:44,200 --> 00:16:45,200
because it's really hard to know.

247
00:16:45,200 --> 00:16:48,200
You always have some uncertainty about just little variables.

248
00:16:49,200 --> 00:16:52,200
You know how much each part of the robot weighs or whatever

249
00:16:52,200 --> 00:16:53,200
because you built it,

250
00:16:53,200 --> 00:16:57,200
but what's the coefficient of friction on the ground

251
00:16:57,200 --> 00:17:00,200
at the spot where the robot is right now?

252
00:17:00,200 --> 00:17:01,200
You have to estimate, right?

253
00:17:01,200 --> 00:17:02,200
And it might not be right.

254
00:17:02,200 --> 00:17:03,200
And so if you train something,

255
00:17:03,200 --> 00:17:05,200
if you take your best guess, put it in,

256
00:17:05,200 --> 00:17:06,200
and you train a system,

257
00:17:06,200 --> 00:17:07,200
it might find a policy,

258
00:17:07,200 --> 00:17:09,200
which is a policy of doing some kind of leap,

259
00:17:09,200 --> 00:17:12,200
that that's the best way to achieve whatever it is you've told it to do,

260
00:17:12,200 --> 00:17:15,200
like get from here to there quickly or something.

261
00:17:15,200 --> 00:17:16,200
And then you have a problem

262
00:17:16,200 --> 00:17:20,200
because then it's relying on the current coefficient of friction

263
00:17:20,200 --> 00:17:21,200
being this specific thing,

264
00:17:21,200 --> 00:17:23,200
and then you run it on the real robot

265
00:17:23,200 --> 00:17:24,200
and the thing completely falls over

266
00:17:24,200 --> 00:17:28,200
because it's out of the distribution that it was trained on.

267
00:17:28,200 --> 00:17:30,200
So one thing that people do is,

268
00:17:30,200 --> 00:17:31,200
when they're simulating it,

269
00:17:31,200 --> 00:17:33,200
they randomly vary it, right?

270
00:17:33,200 --> 00:17:35,200
You say, we think that the coefficient of friction here

271
00:17:35,200 --> 00:17:37,200
is around this number,

272
00:17:37,200 --> 00:17:38,200
but we're actually going to give it,

273
00:17:38,200 --> 00:17:40,200
every time, every episode,

274
00:17:40,200 --> 00:17:42,200
we're going to give it a random value

275
00:17:42,200 --> 00:17:45,200
somewhere in the range from 0.9 of that to 1.1 of that,

276
00:17:45,200 --> 00:17:46,200
you know.

277
00:17:46,200 --> 00:17:51,200
And then the machine learning system

278
00:17:51,200 --> 00:17:52,200
is going to learn a policy

279
00:17:52,200 --> 00:17:55,200
that's able to handle any coefficient of friction

280
00:17:55,200 --> 00:17:56,200
within that range.

281
00:17:56,200 --> 00:17:58,200
So it's learning to adapt, right?

282
00:17:58,200 --> 00:17:59,200
Right, well that's the thing.

283
00:17:59,200 --> 00:18:02,200
So there's two different things that could happen here.

284
00:18:02,200 --> 00:18:05,200
One is, the model could learn,

285
00:18:05,200 --> 00:18:07,200
oh, if I do this kind of leaping thing,

286
00:18:07,200 --> 00:18:10,200
then some of the time I completely stack it

287
00:18:10,200 --> 00:18:12,200
and it's very embarrassing.

288
00:18:12,200 --> 00:18:16,200
So I'm going to just do a shuffling thing, right?

289
00:18:16,200 --> 00:18:18,200
That's much more reliable,

290
00:18:18,200 --> 00:18:22,200
that works across a wide range of friction values, right?

291
00:18:22,200 --> 00:18:23,200
That's one thing you could do.

292
00:18:23,200 --> 00:18:28,200
But there you've kind of sacrificed some performance, right?

293
00:18:28,200 --> 00:18:32,200
But if your model is more sophisticated,

294
00:18:32,200 --> 00:18:34,200
it could learn something like,

295
00:18:34,200 --> 00:18:38,200
okay, first, just slide your foot along the ground a bit

296
00:18:38,200 --> 00:18:40,200
to get a feel for what the friction is like,

297
00:18:40,200 --> 00:18:43,200
and then, if it's correct, do the leap,

298
00:18:43,200 --> 00:18:44,200
otherwise do something else.

299
00:18:44,200 --> 00:18:46,200
Or like, adjust how you're leaping

300
00:18:46,200 --> 00:18:47,200
so that it always works.

301
00:18:47,200 --> 00:18:49,200
So that's actually being adaptive

302
00:18:49,200 --> 00:18:50,200
rather than the lowest common denominator,

303
00:18:50,200 --> 00:18:53,200
which is the sort of prior idea.

304
00:18:53,200 --> 00:18:54,200
Exactly.

305
00:18:54,200 --> 00:18:58,200
And nothing necessarily changed for that

306
00:18:58,200 --> 00:19:01,200
except the power of the model, right?

307
00:19:01,200 --> 00:19:03,200
If your model is too small,

308
00:19:03,200 --> 00:19:06,200
then it's not going to be able to learn something

309
00:19:06,200 --> 00:19:08,200
as complicated as measure the friction

310
00:19:08,200 --> 00:19:10,200
and then do one of these five possible things

311
00:19:10,200 --> 00:19:12,200
depending on the friction, right?

312
00:19:12,200 --> 00:19:14,200
It's only going to be able to learn one thing

313
00:19:14,200 --> 00:19:15,200
that it could do,

314
00:19:15,200 --> 00:19:16,200
and so it has to just learn one

315
00:19:16,200 --> 00:19:18,200
that does okay on all friction levels.

316
00:19:18,200 --> 00:19:20,200
But if you have a larger model,

317
00:19:20,200 --> 00:19:21,200
you can learn a better policy

318
00:19:21,200 --> 00:19:25,200
which actually adapts.

319
00:19:25,200 --> 00:19:27,200
I don't know, this is purely,

320
00:19:27,200 --> 00:19:29,200
I'm not talking about a specific paper or anything,

321
00:19:29,200 --> 00:19:32,200
this is just a thing that I thought of.

322
00:19:32,200 --> 00:19:34,200
And so what they're suggesting, I think,

323
00:19:34,200 --> 00:19:35,200
in this paper,

324
00:19:35,200 --> 00:19:38,200
is that GPT-3 is doing something similar,

325
00:19:38,200 --> 00:19:40,200
that in order to perform really well

326
00:19:40,200 --> 00:19:41,200
at this language modeling task,

327
00:19:41,200 --> 00:19:45,200
it's actually learning online,

328
00:19:45,200 --> 00:19:47,200
from the context,

329
00:19:47,200 --> 00:19:50,200
what the task is that needs to be done.

330
00:19:50,200 --> 00:19:53,200
Are we getting into AGI territory here?

331
00:19:53,200 --> 00:19:54,200
Gradually.

332
00:19:54,200 --> 00:19:55,200
I mean, I think it's like,

333
00:19:55,200 --> 00:19:57,200
it's a step on the path.

334
00:19:57,200 --> 00:19:59,200
It's not like a,

335
00:19:59,200 --> 00:20:02,200
it's not dramatically closer

336
00:20:02,200 --> 00:20:03,200
than we would expect

337
00:20:03,200 --> 00:20:04,200
or anything like that.

338
00:20:04,200 --> 00:20:05,200
What they're interested in,

339
00:20:05,200 --> 00:20:06,200
mostly in this paper,

340
00:20:06,200 --> 00:20:11,200
is GPT-3 as a few-shot learner,

341
00:20:11,200 --> 00:20:12,200
which is,

342
00:20:12,200 --> 00:20:13,200
so you have,

343
00:20:13,200 --> 00:20:15,200
the standard machine learning model is

344
00:20:15,200 --> 00:20:18,200
you give the thing loads and loads of data.

345
00:20:18,200 --> 00:20:20,200
The more data points, the better.

346
00:20:20,200 --> 00:20:23,200
But sometimes you have

347
00:20:23,200 --> 00:20:24,200
a few-shot learning problem,

348
00:20:24,200 --> 00:20:26,200
which is where you have to learn

349
00:20:26,200 --> 00:20:29,200
using only a few examples.

350
00:20:29,200 --> 00:20:31,200
So let's say, for example,

351
00:20:31,200 --> 00:20:32,200
your phone,

352
00:20:32,200 --> 00:20:34,200
you want to unlock your phone, right?

353
00:20:34,200 --> 00:20:35,200
You can train a model

354
00:20:35,200 --> 00:20:38,200
that does all kinds of face recognition and stuff,

355
00:20:38,200 --> 00:20:40,200
but in this case,

356
00:20:40,200 --> 00:20:42,200
you want to train a classifier

357
00:20:42,200 --> 00:20:45,200
to distinguish this face from other faces,

358
00:20:45,200 --> 00:20:47,200
but it's only going to get,

359
00:20:47,200 --> 00:20:48,200
you know,

360
00:20:48,200 --> 00:20:51,200
you're going to give it like three pictures of yourself,

361
00:20:51,200 --> 00:20:52,200
whereas usually for a classifier,

362
00:20:52,200 --> 00:20:54,200
you would want to be giving them thousands.

363
00:20:54,200 --> 00:20:58,200
So that's like a few-shot learning problem.

364
00:20:58,200 --> 00:21:00,200
And so you can kind of,

365
00:21:00,200 --> 00:21:01,200
you can kind of imagine,

366
00:21:01,200 --> 00:21:03,200
this is all kind of fuzzy because

367
00:21:03,200 --> 00:21:07,200
it's like you can think of it as

368
00:21:07,200 --> 00:21:10,200
when you're giving the thing the context,

369
00:21:10,200 --> 00:21:12,200
you can give it examples.

370
00:21:12,200 --> 00:21:15,200
And how many examples you give it

371
00:21:15,200 --> 00:21:17,200
is a bit like just giving training samples

372
00:21:17,200 --> 00:21:20,200
to a machine learning system.

373
00:21:20,200 --> 00:21:21,200
But what's impressive is

374
00:21:21,200 --> 00:21:24,200
that GPT-3 seems to be able to do these tasks

375
00:21:24,200 --> 00:21:28,200
with what would be very, very few examples

376
00:21:28,200 --> 00:21:30,200
compared to standard machine learning methods,

377
00:21:30,200 --> 00:21:31,200
right?

378
00:21:31,200 --> 00:21:32,200
The thing that's kind of interesting is

379
00:21:32,200 --> 00:21:34,200
when you look at,

380
00:21:34,200 --> 00:21:36,200
we can stick with arithmetic,

381
00:21:36,200 --> 00:21:37,200
stick with addition.

382
00:21:37,200 --> 00:21:39,200
The number of examples that you give it

383
00:21:39,200 --> 00:21:40,200
makes a big difference.

384
00:21:40,200 --> 00:21:42,200
If you just say,

385
00:21:42,200 --> 00:21:43,200
you know,

386
00:21:43,200 --> 00:21:45,200
this number plus this number equals,

387
00:21:45,200 --> 00:21:49,200
it will get that right a certain percentage of the time.

388
00:21:49,200 --> 00:21:50,200
But if you say,

389
00:21:50,200 --> 00:21:51,200
you know,

390
00:21:51,200 --> 00:21:52,200
10 plus 10 equals 20,

391
00:21:52,200 --> 00:21:53,200
you know,

392
00:21:53,200 --> 00:21:56,200
25 plus 30 equals 55,

393
00:21:56,200 --> 00:21:57,200
you know,

394
00:21:57,200 --> 00:21:58,200
and give it a few of those,

395
00:21:58,200 --> 00:22:00,200
then the performance gets much better.

396
00:22:00,200 --> 00:22:01,200
There's various different ways

397
00:22:01,200 --> 00:22:03,200
that you could interpret this.

398
00:22:03,200 --> 00:22:08,200
Is it actually learning how to do addition

399
00:22:08,200 --> 00:22:11,200
from looking at your examples?

400
00:22:11,200 --> 00:22:13,200
Or is it just figuring out

401
00:22:13,200 --> 00:22:16,200
that what you want is addition?

402
00:22:16,200 --> 00:22:17,200
Is it learning addition

403
00:22:17,200 --> 00:22:20,200
or is it like locating addition

404
00:22:20,200 --> 00:22:22,200
in the space of possible tasks

405
00:22:22,200 --> 00:22:24,200
that it already knows how to do?

406
00:22:24,200 --> 00:22:25,200
Kind of unclear.

407
00:22:25,200 --> 00:22:27,200
For pretty much every task,

408
00:22:27,200 --> 00:22:29,200
they try it in the zero shot,

409
00:22:29,200 --> 00:22:30,200
the one shot,

410
00:22:30,200 --> 00:22:32,200
and the few shot settings.

411
00:22:32,200 --> 00:22:33,200
Right, okay.

412
00:22:33,200 --> 00:22:36,200
And they look at how well it performs.

413
00:22:36,200 --> 00:22:37,200
And it consistently does,

414
00:22:37,200 --> 00:22:38,200
you know,

415
00:22:38,200 --> 00:22:40,200
better the more examples you give it,

416
00:22:40,200 --> 00:22:42,200
up to the size of the context.

417
00:22:42,200 --> 00:22:44,200
Obviously you can't,

418
00:22:44,200 --> 00:22:48,200
I think you can only give it 2048 tokens,

419
00:22:48,200 --> 00:22:50,200
which actually is a very large,

420
00:22:50,200 --> 00:22:51,200
like that's much bigger

421
00:22:51,200 --> 00:22:55,200
than most other language models out there.

422
00:22:55,200 --> 00:22:57,200
But they find it does better.

423
00:22:57,200 --> 00:22:58,200
It does better the more you give it.

424
00:22:58,200 --> 00:23:03,200
But also the ratio seems to go up

425
00:23:03,200 --> 00:23:05,200
the larger the language model is.

426
00:23:05,200 --> 00:23:06,200
Right.

427
00:23:06,200 --> 00:23:08,200
So all of the models do better

428
00:23:08,200 --> 00:23:09,200
with more examples.

429
00:23:09,200 --> 00:23:10,200
Yes.

430
00:23:10,200 --> 00:23:12,200
But the difference between

431
00:23:12,200 --> 00:23:15,200
the zero shot and the few shot

432
00:23:15,200 --> 00:23:17,200
is bigger for the bigger models.

433
00:23:17,200 --> 00:23:20,200
So it suggests that perhaps

434
00:23:20,200 --> 00:23:22,200
the larger models are actually

435
00:23:22,200 --> 00:23:24,200
like making better use

436
00:23:24,200 --> 00:23:26,200
of that information in the context

437
00:23:26,200 --> 00:23:28,200
to actually learn things.

438
00:23:28,200 --> 00:23:29,200
Okay.

439
00:23:29,200 --> 00:23:32,200
Yeah, so it's more efficient, right?

440
00:23:32,200 --> 00:23:33,200
Right, right.

441
00:23:33,200 --> 00:23:34,200
It's less about just like

442
00:23:34,200 --> 00:23:35,200
using the context

443
00:23:35,200 --> 00:23:37,200
to find the relevant parts

444
00:23:37,200 --> 00:23:38,200
of the training data

445
00:23:38,200 --> 00:23:40,200
to sort of power it back

446
00:23:40,200 --> 00:23:42,200
that you've memorized.

447
00:23:42,200 --> 00:23:44,200
And more about actually learning

448
00:23:44,200 --> 00:23:46,200
what to do from the context,

449
00:23:46,200 --> 00:23:48,200
like recognizing that

450
00:23:48,200 --> 00:23:49,200
what's happening is addition

451
00:23:49,200 --> 00:23:51,200
and then actually doing addition.

452
00:23:51,200 --> 00:23:52,200
Yeah.

453
00:23:52,200 --> 00:23:55,200
Because you have to have some way

454
00:23:55,200 --> 00:23:56,200
to account for the fact

455
00:23:56,200 --> 00:23:58,200
that this thing is reliably

456
00:23:58,200 --> 00:24:00,200
doing addition when only

457
00:24:00,200 --> 00:24:01,200
a very small number

458
00:24:01,200 --> 00:24:04,200
of those addition problems

459
00:24:04,200 --> 00:24:06,200
actually occurred in the training data set.

460
00:24:06,200 --> 00:24:07,200
Yes, okay.

461
00:24:07,200 --> 00:24:08,200
The other thing that's interesting

462
00:24:08,200 --> 00:24:10,200
about it is apparently

463
00:24:10,200 --> 00:24:12,200
when it gets them wrong,

464
00:24:12,200 --> 00:24:13,200
it tends to get them wrong

465
00:24:13,200 --> 00:24:17,200
in sort of human plausible ways.

466
00:24:17,200 --> 00:24:18,200
Okay, like it's supposed

467
00:24:18,200 --> 00:24:19,200
to carry them on or something.

468
00:24:19,200 --> 00:24:20,200
Yeah, exactly.

469
00:24:20,200 --> 00:24:21,200
Exactly.

470
00:24:21,200 --> 00:24:23,200
And that is another indication

471
00:24:23,200 --> 00:24:24,200
that what it's doing here

472
00:24:24,200 --> 00:24:29,200
is something like actual addition.

473
00:24:29,200 --> 00:24:30,200
But that's pretty exciting.

474
00:24:30,200 --> 00:24:33,200
And there's a sense in which,

475
00:24:33,200 --> 00:24:34,200
you know, you could imagine it

476
00:24:34,200 --> 00:24:37,200
learning the rules of addition

477
00:24:37,200 --> 00:24:38,200
sort of in the same way

478
00:24:38,200 --> 00:24:40,200
that it learns the rules of grammar.

479
00:24:40,200 --> 00:24:41,200
In order to change something

480
00:24:41,200 --> 00:24:42,200
into a question,

481
00:24:42,200 --> 00:24:44,200
then you swap these two words around

482
00:24:44,200 --> 00:24:45,200
and add a question mark

483
00:24:45,200 --> 00:24:46,200
or whatever it is, you know.

484
00:24:46,200 --> 00:24:47,200
In order to do addition,

485
00:24:48,200 --> 00:24:50,200
then you pull out the thing

486
00:24:50,200 --> 00:24:51,200
that you've memorized

487
00:24:51,200 --> 00:24:52,200
for these two and add it

488
00:24:52,200 --> 00:24:54,200
and then do it again to this one

489
00:24:54,200 --> 00:24:58,200
or whatever.

490
00:24:58,200 --> 00:25:00,200
And that is the kind of thing

491
00:25:00,200 --> 00:25:02,200
that most people,

492
00:25:02,200 --> 00:25:06,200
I didn't, expect a transformer

493
00:25:06,200 --> 00:25:09,200
trained in this very straightforward way

494
00:25:09,200 --> 00:25:10,200
to be able to learn.

495
00:25:10,200 --> 00:25:12,200
Right.

496
00:25:12,200 --> 00:25:16,200
And the big takeaway is

497
00:25:16,200 --> 00:25:18,200
we've gone 117 times bigger

498
00:25:18,200 --> 00:25:19,200
than GPT-2

499
00:25:19,200 --> 00:25:21,200
and GPT-2, they started doing it

500
00:25:21,200 --> 00:25:24,200
because the curves aren't levelling off.

501
00:25:24,200 --> 00:25:27,200
They're still not levelling off.

502
00:25:27,200 --> 00:25:30,200
So, yeah, we don't know

503
00:25:30,200 --> 00:25:31,200
how far we can push

504
00:25:31,200 --> 00:25:33,200
this type of language modelling.

505
00:25:33,200 --> 00:25:37,200
But a little bit further yet, at least.

506
00:25:37,200 --> 00:25:40,200
In this case, the first one was GPT-3

507
00:25:40,200 --> 00:25:42,200
and the second one was white snow.

508
00:25:42,200 --> 00:25:43,200
Yeah, so the bluish snow

509
00:25:43,200 --> 00:25:45,200
and et cetera, et cetera.

510
00:25:45,200 --> 00:25:46,200
That was the only thing

511
00:25:46,200 --> 00:25:47,200
I was thinking about.

512
00:25:47,200 --> 00:25:48,200
And then I started thinking,

513
00:25:48,200 --> 00:25:50,200
well, no, it's kind of bluish white.

514
00:25:50,200 --> 00:25:52,200
Great. Well done, GPT-3.

515
00:25:52,200 --> 00:25:53,200
And also, you know,

516
00:25:53,200 --> 00:25:55,200
it's often under a blue sky and so on.

517
00:25:55,200 --> 00:25:56,200
You need blue.

