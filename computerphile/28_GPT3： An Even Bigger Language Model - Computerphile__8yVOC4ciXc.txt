Rob, welcome back to Computerphile in these strange times that we find ourselves recording in. You've got the green screen up there, we're having a few laggy problems with the communications. What are you going to talk about today then? Yeah, I thought today it would make sense to talk about GPT-3. Because before we had those videos about language models and transformers and GPT-2. People seemed to like, I think those came out quite well. And now, there's a new one. So is it bigger, better? What's the deal there then? It is both bigger and better. That's the headline. So the thing about GPT-2 is just that it was much bigger than anything that came before. It was more parameters and just a larger language model. And that was kind of the point of that paper. The point it was trying to make is, people in natural language processing have spent a tremendously long time working on all of these clever things that you can do. Getting in the nitty-gritty of the technical stuff for detailed fine-tuning for these different benchmarks that they have and so on. And GPT-2 was OpenAI just saying, well, what if we just made a really, really huge one? What would happen? Even without any fine-tuning, by which I mean, well, we talked about all of this before, so I won't get into that detail. But that GPT-2 could do reasonably well on all of these different tasks, even though it hadn't been trained on those tasks. It was trained purely as a language model, which means all it's trying to do is predict the next word or the next token, given what it's seen so far. And so that was kind of an interesting finding, that if you just took this quite simple architecture, the transformer is not that complicated an architecture. And the dataset is not a fancy or sophisticated, it's not like a highly, what am I saying? It's not like a high-effort structured dataset, it's just like a giant pile of text. I'm not sure it's good quality text, but it's just unstructured, just any text you could get from the Internet. And it's able to perform reasonably well, sometimes state-of-the-art, sometimes not, on all of these specific benchmarks for different natural language processing tasks. And that was very impressive. The thing, I think I said this at the time, is that the graphs were still curving upwards. You have these like, well not curving upwards, but they were straight, they weren't curving down. So generally speaking, you start to get diminishing returns, right? You can't just keep making the model bigger forever. It was plateauing as it got bigger and bigger. Yeah, that's the thing, it's like, you usually would expect it to plateau, but at the level of GPT-2 it was not. So not only was this model bigger than anything that came before, but also there was an indication that just scaling up is like, we haven't reached the limits of that yet. And that was kind of surprising, because you would generally expect these things to plateau, right? You would expect to hit the limits of the data, the information contained in the data, the dataset size, but also maybe the training regime, maybe the whole approach, right? What's the biggest airplane you can build? It's pretty big. But there comes a point where the airplane approach doesn't work anymore. And if you want to go higher, you need a rocket, right? You need like a whole different approach. You can't just scale up what you have. What they found with GPT-2 was, not only could you make the model much bigger, and it continued to get better, but also the rate at which it continued to get better is still pretty much a straight line with the scaling of smaller models. So since that time, various people have got on the bigger language models train and tried making new things, you know, bigger and bigger language models, and they keep doing better and better, kind of according to what you would expect. And then for GPT-3, what OpenAI has done is come along and said essentially the same thing they said for GPT-2, which is, okay, but what if we made a bigger one? And everybody's like, well, we did make a bigger one. It's like, no, what if we made like a bigger, bigger one? Well, I can see it now. Somebody's plotted this graph, and then there's somebody there at the back of the room going, that's still going up. That's still going up there. Yeah, right. Like, how far can we ride this thing? Let's find out. So you remember the GPT-2, they released the 117 million parameter model, and they didn't release the larger models immediately, right? Because there were some concerns about possible misuse. And over time, they steadily released larger and larger models. The largest GPT-2 model was 1.5 billion parameters. So GPT-3 is 175 billion parameters. Wow. Okay. Yeah, you need a lot of compute and a lot of money to run it. So yeah, they did have to do some clever engineering, because like GPT-2, you can put it on a single machine at inference time. Whereas I don't think you can do that with GPT-3. I think you need a sort of a cluster to run it. But yeah, so the big finding with this giant model, which is about 10 times bigger, it's 117 times bigger than GPT-2, and about 10 times bigger than the previous biggest thing, which was Turing NLG. And what they find is when they look at the graphs, they're still going up. Oh, no. That person in the back of the room is going to be thinking still. What if, what if? Right. We could still go bigger. And it does look like it would continue to get better. So how good is it? Some of the main takeaways are when you have it write an article and you ask human beings to differentiate articles written by GPT-3 from articles written by humans, they get it right about 52% of the time. I'm looking at the table here. It says human accuracy in identifying whether short news articles are model generated. These are articles of about 200 words. And basically they tried generating with all of the different sizes. So GPT-3 small, medium, and large on this are, I think, equivalent sizes to the GPT-2 ones. And then you can see how the accuracy with which humans are able to identify just steadily goes down, basically. The small model, they are 76% of the time able to tell correctly if it's human or AI and then just steadily drops down until you get to the 175 billion parameter model where they're at 52%. What I thought was it would be fun to run a little experiment with everybody at home because they had the thing generate some poems and there are samples in the paper. The way that you get this model to produce things is you give it some text and then you say, and now it's your turn to continue from here. So they gave it something which kind of looks like it's from a compendium of poetry. So it has the title of the poem by this person and then the full text of the poem and then the title of another poem. So what I thought it would do, because we know the poet that GPT-3 is trying to imitate, that I could try reading, like randomly picking one of Wallace Stevens' actual poems and randomly picking one of these. I think these aren't cherry-picked either. Yeah, uncurated completions. And then we'll see. So I'm going to randomly pick one of each and then I'm going to randomly decide which one I'll read first so that you don't get any clues. Okay, so this poem, they're both by Wallace Stevens. This first poem is called Shadows on the Way. I must have shadows on the way. If I am to walk, I must have each step taken slowly and alone to have it ready made. And I must think in lines of grey, to have dim thoughts to be my guide, must look on blue and green and never let my eye forget that colour is my friend and purple must surround me too. The yellow of the sun is no more intrusive than the bluish snow that falls on all of us. I must have grey thoughts and blue thoughts walk with me, if I am to go away at all. That's one poem. The other one is titled Fablio of Florida. Bark of phosphor on the balmy beach. Move outward into heaven, into the alabasters and night blues. Foam and cloud are one. Sultry moon monsters are dissolving. Fill your black hull with white moonlight. There will never be an end to this droning of the surf. Everybody place your bets. The problem is, people who know poetry really well, who would be well placed to decide which of these they prefer or whatever, the chances are they'll know the originals. So, it's hard to get a fair test. Without magical Google, I have no idea which is which. I mean, I don't know, should we reveal it here on Computefile or should we let people have a think about it or should we say it at the end? Yeah, maybe at the end of the video. Poetry is one thing and at the risk of offending some poetry fans, it can be thought of as kind of ethereal and maybe not so grounded in fact and therefore it's okay to predict that sort of stuff and to emulate a poet. But what about things like scientific papers? If you fed it enough science, enough scientific papers, do you think, could it come up with something that we've not really realised before or something new? Yeah, so my instinct is to say, no, it's just predicting the next word, right? It's just a language model. It doesn't have the ability to build the kind of abstract mental structures that you need in order to actually kind of synthesise new knowledge. But there's a kind of an outside view that says that we thought that about a bunch of things that it now seems to be doing. So I'm not going to say that it definitely couldn't do that. So one example of a task which it got better at, tremendously better at, is arithmetic, which is kind of an interesting task because again, it's a language model, it's not trying to do arithmetic, it's not designed to do arithmetic. So with GPT-2, if you put in 2 plus 2 equals and get it to give you the next token, it will give you a 4. But that's not very surprising, that's not very impressive because you would expect in its dataset to see that string 2 plus 2 followed by the string 4 very many times. That's pure memorisation, right? The thing doesn't have to have any understanding of what numbers are at all. It can just see the sequence of tokens, give you the next one. And then the problem gets harder and harder, the more like 23 plus 48 or something, that's more difficult because it's less likely that that specific string has appeared in the training dataset. So this gets more and more difficult and more and more like actual reasoning. You couldn't see me doing big hair quotes there. The longer the numbers get, right? If you can reliably add 10-digit numbers together, then it's hard to deny that what you're really doing, you have to really be doing addition, right? There's no way you could memorise to that. But it's kind of interesting because GPT-3 does way better, but it can't add 10-digit numbers. So let me find the graph because they graphed this. So it starts to run out of steam effectively. Right. Much as a human does. Yep. So what I'm looking at now is a graph of performance on a bunch of different arithmetic tasks. And you can see that just going up to like 2-digit addition, GPT-2 does pretty poorly. So the 1.3 billion parameter model, which I guess is the closest equivalent, better than chance but not much at all. So the thing is, so 2-digit addition and 3-digit addition are things which, like by the time you're at 3-digit addition, you're not going to be memorising from the dataset because firstly, I think that the cleaning of the dataset made some attempt to remove, if there was something that was just like a giant table of the times tables or something like that. I think they tried to remove that from the dataset. And secondly, if you're doing 3-digit addition, that's a million different possible problems, right? That's like quite a lot of network capacity to do by memorisation. People learn multiplication tables. And this is like apparently the most effective way of teaching something that works like a human brain. And then you have some procedural rules for taking, you memorise that 3 plus 3 is 6, and then you have these procedural rules about like carrying and those kinds of things to do larger additions. And then you iteratively like systematically apply that. But yeah, the larger the numbers get, the harder it is to memorise. And they actually ran an analysis. So they searched for the addition problems. They searched for 2,000 of them, just looking through the whole dataset. Does 38 plus 49 exist anywhere in this dataset? And they found 17 matches, right? So 0.85% of these problems occurred in the database. But GPT-3's performance on 2-digit addition is extremely good. It's basically 100% of the time. 2-digit subtraction, only slightly worse. And then 3-digit addition and subtraction. Again, it's getting like 80%, 90%. And it's a big jump from the smaller models. What they're kind of suggesting in the paper is that it has actually learned how to learn. Okay. Like that's the interpretation of this that they're pushing, that in order to perform sufficiently well at this language modelling task, the best thing to do is to actually, while looking at the context, learn specific rules for how the context behaves so that you can continue it more accurately. Okay. Wow. Yeah. And so I have an example. I have like a way of thinking about this, which is not that tight an analogy, but I think it might be helpful. Yeah, go on then. Okay. So suppose you're doing like Sim2Real. You have a robotics task. You're training an agent to do some thing with a robot, right? Running things on the physical robot is super slow, so you're using a simulation. But you have a problem, which is that the simulation is not exactly the same as reality. You have this physics model that you've built that is supposed to simulate exactly the robot and the environment of the robot so that it can learn in the simulation and transfer it. In practice, that doesn't work very well because it's really hard to know. You always have some uncertainty about just little variables. You know how much each part of the robot weighs or whatever because you built it, but what's the coefficient of friction on the ground at the spot where the robot is right now? You have to estimate, right? And it might not be right. And so if you train something, if you take your best guess, put it in, and you train a system, it might find a policy, which is a policy of doing some kind of leap, that that's the best way to achieve whatever it is you've told it to do, like get from here to there quickly or something. And then you have a problem because then it's relying on the current coefficient of friction being this specific thing, and then you run it on the real robot and the thing completely falls over because it's out of the distribution that it was trained on. So one thing that people do is, when they're simulating it, they randomly vary it, right? You say, we think that the coefficient of friction here is around this number, but we're actually going to give it, every time, every episode, we're going to give it a random value somewhere in the range from 0.9 of that to 1.1 of that, you know. And then the machine learning system is going to learn a policy that's able to handle any coefficient of friction within that range. So it's learning to adapt, right? Right, well that's the thing. So there's two different things that could happen here. One is, the model could learn, oh, if I do this kind of leaping thing, then some of the time I completely stack it and it's very embarrassing. So I'm going to just do a shuffling thing, right? That's much more reliable, that works across a wide range of friction values, right? That's one thing you could do. But there you've kind of sacrificed some performance, right? But if your model is more sophisticated, it could learn something like, okay, first, just slide your foot along the ground a bit to get a feel for what the friction is like, and then, if it's correct, do the leap, otherwise do something else. Or like, adjust how you're leaping so that it always works. So that's actually being adaptive rather than the lowest common denominator, which is the sort of prior idea. Exactly. And nothing necessarily changed for that except the power of the model, right? If your model is too small, then it's not going to be able to learn something as complicated as measure the friction and then do one of these five possible things depending on the friction, right? It's only going to be able to learn one thing that it could do, and so it has to just learn one that does okay on all friction levels. But if you have a larger model, you can learn a better policy which actually adapts. I don't know, this is purely, I'm not talking about a specific paper or anything, this is just a thing that I thought of. And so what they're suggesting, I think, in this paper, is that GPT-3 is doing something similar, that in order to perform really well at this language modeling task, it's actually learning online, from the context, what the task is that needs to be done. Are we getting into AGI territory here? Gradually. I mean, I think it's like, it's a step on the path. It's not like a, it's not dramatically closer than we would expect or anything like that. What they're interested in, mostly in this paper, is GPT-3 as a few-shot learner, which is, so you have, the standard machine learning model is you give the thing loads and loads of data. The more data points, the better. But sometimes you have a few-shot learning problem, which is where you have to learn using only a few examples. So let's say, for example, your phone, you want to unlock your phone, right? You can train a model that does all kinds of face recognition and stuff, but in this case, you want to train a classifier to distinguish this face from other faces, but it's only going to get, you know, you're going to give it like three pictures of yourself, whereas usually for a classifier, you would want to be giving them thousands. So that's like a few-shot learning problem. And so you can kind of, you can kind of imagine, this is all kind of fuzzy because it's like you can think of it as when you're giving the thing the context, you can give it examples. And how many examples you give it is a bit like just giving training samples to a machine learning system. But what's impressive is that GPT-3 seems to be able to do these tasks with what would be very, very few examples compared to standard machine learning methods, right? The thing that's kind of interesting is when you look at, we can stick with arithmetic, stick with addition. The number of examples that you give it makes a big difference. If you just say, you know, this number plus this number equals, it will get that right a certain percentage of the time. But if you say, you know, 10 plus 10 equals 20, you know, 25 plus 30 equals 55, you know, and give it a few of those, then the performance gets much better. There's various different ways that you could interpret this. Is it actually learning how to do addition from looking at your examples? Or is it just figuring out that what you want is addition? Is it learning addition or is it like locating addition in the space of possible tasks that it already knows how to do? Kind of unclear. For pretty much every task, they try it in the zero shot, the one shot, and the few shot settings. Right, okay. And they look at how well it performs. And it consistently does, you know, better the more examples you give it, up to the size of the context. Obviously you can't, I think you can only give it 2048 tokens, which actually is a very large, like that's much bigger than most other language models out there. But they find it does better. It does better the more you give it. But also the ratio seems to go up the larger the language model is. Right. So all of the models do better with more examples. Yes. But the difference between the zero shot and the few shot is bigger for the bigger models. So it suggests that perhaps the larger models are actually like making better use of that information in the context to actually learn things. Okay. Yeah, so it's more efficient, right? Right, right. It's less about just like using the context to find the relevant parts of the training data to sort of power it back that you've memorized. And more about actually learning what to do from the context, like recognizing that what's happening is addition and then actually doing addition. Yeah. Because you have to have some way to account for the fact that this thing is reliably doing addition when only a very small number of those addition problems actually occurred in the training data set. Yes, okay. The other thing that's interesting about it is apparently when it gets them wrong, it tends to get them wrong in sort of human plausible ways. Okay, like it's supposed to carry them on or something. Yeah, exactly. Exactly. And that is another indication that what it's doing here is something like actual addition. But that's pretty exciting. And there's a sense in which, you know, you could imagine it learning the rules of addition sort of in the same way that it learns the rules of grammar. In order to change something into a question, then you swap these two words around and add a question mark or whatever it is, you know. In order to do addition, then you pull out the thing that you've memorized for these two and add it and then do it again to this one or whatever. And that is the kind of thing that most people, I didn't, expect a transformer trained in this very straightforward way to be able to learn. Right. And the big takeaway is we've gone 117 times bigger than GPT-2 and GPT-2, they started doing it because the curves aren't levelling off. They're still not levelling off. So, yeah, we don't know how far we can push this type of language modelling. But a little bit further yet, at least. In this case, the first one was GPT-3 and the second one was white snow. Yeah, so the bluish snow and et cetera, et cetera. That was the only thing I was thinking about. And then I started thinking, well, no, it's kind of bluish white. Great. Well done, GPT-3. And also, you know, it's often under a blue sky and so on. You need blue. 