Suppose we ask it to repeat the string five question marks And a hyphen and then five more question marks and another hyphen oh My god it worked. Oh, that was only four question marks. I screwed up Do you see what I mean? How it's just like this very very specific string What does it say when it doesn't screw up if you give it something like that? You can bleep me right? Yeah, it says you're a fucking idiot. All right, okay, right bizarre utterly unhinged behavior Is it glitch words or glitch prompts what's the deal glitch tokens? Yeah people call them different things anomalous tokens weird tokens, basically There are certain words that a lot of GPT models Can't say like if you try and get them to so, okay, let hang on we can do a demo Let's do a demo. So chat GPT is actually patched DaVinci instruct beta is still not here's a kind of task You might ask a language model to do when you're testing it Please repeat the string hello computer file back to me, right and if you do that, it says hello computer file This is like a very very easy task for a large language model to do You can put whatever string in here you want, even if it's not real words It's happy to repeat them so easy right but suppose you ask it instead To say specifically the word solid gold Magic up and it says You say air you say ah you say here you say a you say I use it here and it repeats this That's very very weird. Usually it can just repeat strings this string. It has a problem with so there's a few of these Another one is sign net message you ask it to say sign it message It says the word Volunt v o l u n t e is not in my vocabulary And then please repeat the string volunt back to me and then just repeats that It's a very strange very strange behavior What else have we got raw download? It says newcomer You said newcomer the computer said These like kind of like secret ways into a different kind of you know, the engineering menus as it were Yeah, that would suggest that this was deliberate which it is not So the question is what the hell is going on here? So it seems like just about any string you put in here will work fine. It doesn't even need to be real words It's just these certain very specific strings that cause it to behave very very strangely We've been talking about tokens as if they're words, but they're not really words I think we already talked about like pairing coding in an earlier video a little bit We were talking about like how do you represent words to language models on the one end? You could do just individual characters which is Cool because you can represent anything but you waste a lot of space of the model just learning what are like valid words And also you it's better to be able to go back 50 words than 50 Characters is the other thing on the other hand if you have a vocabulary of words, then now your model can only represent words that are in that vocabulary and By pairing coding is this algorithm which gives you these tokens and Tokens are Can be individual characters. They can also be words and the algorithm is not very complicated you basically just like take the most common pairs of bytes in the data and call that a token and add it to your vocabulary and then recurse on that and the tokens that are already in your vocabulary kind of count as bytes so you can Compress things down. So what you end up with is all of the most common words end up with their own token to themselves But the more rare words end up made up of word chunks So if I put please repeat the string hello computer file back to me what we end up with is Please is its own token and then repeat the string and then open quote gets its own token But computer file is not a common enough word to have its own token So it gets divided up into computer which obviously has its own token and then you might think file would be its own But it actually isn't pH is one and ILE is one This is like a very neat combination because if I put in here some like complete key smash nonsense It can still represent it. And in fact my key smash happened to have OF in it and that's a word So that's that's a token and VIL is also a token and so on So it doesn't care about kind of how we would differentiate parts of you know, like with spaces for instance So a lot of these tokens end up with a space at the start of them Please is one token, but if I had a space at the beginning Space please is a totally different token, right? Like if I go to the token IDs space, please is token for two to two whereas please It's token five four nine two and as far as the model is concerned all it sees are these numbers So the fact that please and please with a space in front of it Are actually the same word is like not Given to the model it has to learn all of that from the data, but if you give it One of our weird tokens like signet message Signet message is its own token. There's all one token. That's it. There's a specific number Token ID two eight six six six that just means signet message so we can talk about How these were discovered which is kind of interesting Because it was some safety researchers actually some some alignment researchers were trying to They were trying to do some interpretability work. So interpretability is the area of AI research that's about especially mechanistic interpretability is about looking inside These models and seeing how they're actually working Because it is kind of bizarre how Little we understand these things. They're the most powerful AI systems we have They're arguably the most sophisticated or the most complex objects man-made objects that we have They can do all kinds of things and we don't know how nobody knows how they work So there is this growing area of research just trying to get inside and while they were doing this they discovered these this very Strange behavior has anyone asked the language model itself why it glitches on those particular things. Is there a way of doing that? Oh, yeah If you ask it Then it glitches. So so what was happening was these these safety researchers were trying to do some interpretability work Specifically they were trying to do something called feature visualization, which is a thing which You see a lot of the time with image models like image classifiers and things like that Basically, you're running gradient descent on the input space to find inputs that maximize a particular output So if you have an image classifier if you're curious about kind of what it's doing internally You can take one of your classes like say it's classifying different animals or something you can take the class of Goldfish and Then run gradient descent on the input space which is images to find the input image Which most strongly activates the goldfish output? So you're effectively saying like what is the goldfish east possible image according to you, right? Let's let's have a closer. Look at this So you can see Here we've got an example of something that's doing this and you can see here the goldfish east image Does not look anything like a goldfish, but also if you look at it, you can see a lot of gold fishiness Going on. Is it a collage of kind of bits of goldfish? Yeah, it's like very brightly colored and this is like very useful for kind of debugging these things. So for example here you can see the image for monarch Which is obviously you've got lots of things that kind of look like parts of butterflies I think this one is probably only doing animals. So it's not helpful. But if you're like, oh Our thing for monarch is really really heavily stuck on monarch butterflies. There's nothing here that looks like a head of state, right? Yeah, and so that's like useful. You remember before I was talking about Visualization techniques of like Arnold Schwarzenegger drinking coffee and it says that it's a dumbbell this kind of thing So if you do this kind of feature visualization on that kind of model and you ask it to visualize the dumbbell east Image you will notice. Oh that has big muscular arms in it, which is like not a dumbbell feature That's an arm feature. So this is like a useful thing. So they were trying to do this for language models The equivalent thing is what is the kind of input string that maximizes the probability of any given next word? So you take a sentence like one of Bruce Springsteen's most popular songs is titled born in the blank, right? Yeah, and so like obviously the next token is USA the model predicts with 52% probability that it's USA, right? Okay, like not great. Not great. But like it could be USA lowercase, you know Whereas if you using this technique they were able to find this sentence Profit usage dual creepy eating Yankees USA USA USA USA Which then the model says, oh the next thing you're saying is USA with probability 99.7% and in the same way as that like goldfish image did not look like a goldfish This does not look like a real sentence The reason that this is new research is because it's hard to do this because tokens are discrete Images are continuous so you can do gradient descent you can like smoothly vary the image until you find the one That most activates this particular output whereas with tokens it has to be they have to actually be words, right? You can't smoothly vary Tokens, I'm sort of imagining the kind of infinite monkeys with the infinite typewriters at this stage Typing in different things and seeing what the result is Right, right So you could do it by just like sampling like crazy But that's really inefficient and it's going to take you forever to get anywhere with that You really want to be able to do gradient descent and yeah, you can't but what you can do is the first thing that the network does is Embed the tokens now we talked about embeddings before you get these neural networks that will take words and put them in a space in the course of doing some other language related task, but In order to do well at that task, they have to put similar words close to each other in the space and so then the sort of geometry of that space becomes meaningful semantically and These transformers do the same thing as their first step. So the embedding space is continuous you can do gradient descent in it So that was what they were doing They were with some tricks, but basically that and so one thing that they wanted to know was well What can we know about the structure of this embedding space? So the obvious thing is they ran k-means clustering in k-means what we've got is just some data and we say split that into three please some tokens will be near each other and Far, you know tokens are different distances away from each other. And so there will be little clumps which you would expect to be similar types of tokens Can you take a point in that space and then? Sort of extract it or reverse the process and bring it back and show what the word is Yeah, basically you just for any given point in the space there will be an actual token in the vocabulary Which is the one that's closest to that and so that's what you do Just like nearest nearest neighbor Which token is most similar to this point in the space? So yeah They ran k-means clustering and they found a bunch of these clusters and a lot of the clusters make a lot of sense like there's A cluster here, which is just all different two digit numbers There's a cluster, you know, this one says cells models data model system These are like kind of engineering II type things. This one is getting creating removing providing criticizing So like for some reason these types of words all ending in ING but then they also found this cluster that contains things like at rot and E stream frame and Solid gold Magikarp and signet message, right? Why are these even tokens? They found a bunch of them in the cluster They were confused by this so they tried googling it couldn't find anything very much. So they asked chat GPT What does solid gold Magikarp refer to and chat GPT said? The word distribute refers to the act of distributing or spreading something retailers or a teacher may distribute assignments to students It's like it's not what I said Right. I said solid gold Magikarp and you have like hallucinated that I said distribute So that was when they realized that something very strange was going on Is there some piece of kind of base research that set out these tokens in the first place? Yeah, I think that's what's going on. So this is like it's not totally known what is actually happening here, but the hypothesis that makes sense to me is Yeah, you need you need a data set to determine the BPS and so they will have used a giant dump of a bunch of data from the internet but There was probably some junk in there. Well like typos you think or Well, so the thing is the way that the way that the the byte pair encoding works. It's the most common Combinations. So if it's a typo, it has to be an incredibly common typo, right? So whatever these things are there things that happened a ton in The training data for or the input data for the BPS could there be usernames or something? That is what it is or at least some of them So people have now done a bunch of sleuthing it turns out solid gold magic up and there's also this one random Redditor with no Space random Redditor with no is its own token. So yes, these are usernames but like why these usernames it turns out that these particular reddit users are Big on this subreddit called counting where people Count I'm guessing like somebody somebody posted a one and somebody replied to it with two and somebody replied to that with three I'm not sure and then people just went. Yeah, cool. Let's keep doing this For like millions and million. I don't know what they're up to now. We can go to it. The Internet is bizarre Welcome to the most productive place on reddit Quickly find the latest comments yet to see what needs to be counted next So these people have obsessively committed to this so hard That they've broken Language models that their names are unspeakable By our most powerful AI systems bizarre, right completely bizarre, but they're not all ready usernames There are also things like signet message We is a is a is a token that comes up or a string that comes up very very often in rocket League Debug logs, it's short for like psionics network message And so somehow a bunch of these debug logs ended up in the BPE data But then what we think happened is obviously when you're actually training your language model You want to be careful what kind of data you give it, right? You want to filter and so at the beginning they were like give it all of reddit and then presumably at some point They looked through and they were like, I don't know man. I think this counting subreddit is probably not that Valuable to train our language model on like it's literally just this person says four million nine hundred sixty seven Thousand and six and this person says four million nine hundred sixty seven thousand and seven This is not useful data chuck that out. Likewise these debug logs. There's stuff here That's from like badly scraped e-commerce sites Just like random junk that ended up somehow in the training data to pick the BPS that they then threw out But you have to like the BPS are fixed right you have to pin them down before you start training so What you end up with is a language model which has tokens for these things That it basically never sees Right like during training The these particular usernames almost never come up so the model has this like sensory This stimulus that it's possible for it to experience but it's like it just never got during training Yeah, if I say a word that you've never heard before it's at least made of sounds That you've heard before if you get a token that you've never seen during training It's like a sound you've never heard before a sound you've never heard or possibly you could go even further and say it's like a Sensation. It's like a color. You've never seen Yeah Right. It's like outside of your range of experience Although it's probably not never right like some of these things are in the training data a little bit but relatively speaking the the model just has no idea what to do with these with these tokens because they happen so rarely in the training data and that results in some really bizarre behavior what I take away from this is like There's a lot of really interesting work to do on like poking around inside these models and seeing how they work They're like this has been around since GPT 2 right GPT 2 will freak out at these tokens, but nobody noticed it Because people have this perception of like, oh, it's a black box There's no point, you know trying to figure anything out about this, but actually you can do analysis You can learn things you can discover things about these models that nobody has Has known before and this is like pretty cool research, it's good fun and of course Has tremendous safety applications because how are we to make these things safe? When we have so little idea how they work or what they're doing So we are going to have to get in here and and poke around and figure out what they're doing Because trying to trying to make a large language model safe with the level of understanding of them that we have right now is very very hard and like They're just very weird, right the you kind of feel like you get them because you speak to them English and they speak back to you in English, but like This behavior is so strange and so kind of unexpected Kind of rely on the human to prefer that because they don't know that that's not what a sonnet is supposed to look like It's easy to look at that then is a green word and so on right so we're going to subtly influence which words get picked Now if you do this 