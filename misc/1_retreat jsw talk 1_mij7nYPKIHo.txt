The subject of the talk is big picture of alignment, as I see it, I guess. Meta notes first. This is generally just going to be on object level technical problems, so we're not going to be covering field building stuff or anything like that. Nothing about timelines, nothing about takeoff scenarios, just thinking about the alignment problem. Cool. All right, so general outline. I'm guessing we're maybe going to get like half or a third of the way through the second one in the time we have, but major bullets, undetermined optimization. This is mostly going to be about the alignment problem itself, what makes it hard. The second part is going to be just lots of stuff about human values. And then the third part is abstraction and modularity. So first things first. The conceptual question here is, what's hard about alignment? So we're going to be using, the main point of this section is this sort of mental model of underdetermined optimization. So first, a little bit of background on that. When we think about optimization in general, we often picture like we have a peak, and we have some optimizer that's doing gradient descent and getting down to that peak. In practice, that's not a particularly great representation of what optimization problems look like. A better mental model, first of all, obviously everyday optimization, extremely high dimensional. And second, tend to look more like, say, this. So the key point here is that there's not one optimum. There's this whole ridge here. Make sense? Yeah. So it's not like I'm trying to get to one particular point. It's like I'm trying to find any of the points along that ridge. So example of how this looks in real life, maybe I'm thinking about what to make for lunch. Obviously, there are many different things I could make for lunch. For any given thing, there's many ways in which I could make it. Maybe I could chop the onions first. Maybe I could chop the peppers first. Maybe I could start frying the vegetables before I start the rice going. Maybe I could do it in the other order. And all of these end up in basically the same spot. There's lots of different ways to get where we're going. So a simplified but useful mathematical formulation of how to think about this is, rather than our usual mental model of just taking a function and maximizing it, so max on x f of x, we're going to think about sampling an optimum. So we're going to sample x from the probability distribution, x given that x is an opt. So for the picture I drew here, opt would be like this whole ridge. So this is saying we imagine we have some prior distribution on our x's, and then we're specializing to the x's that are optimal for whatever our objective is, and we're picking one of them at random. Make sense? So for purposes of cooking lunch, this would be that I'm effectively randomly picking order in which to do things, conditioned on it actually producing lunch. Right? OK. Important point here, major empirical result in a paper by Mingard et al. This is actually a very good approximation for neural nets. So this idea that I'm just conditioning on what gives me optimal behavior, and then randomly picking one of the things that gives me optimal behavior is a great approximation for neural nets. In this paper, it was vision nets fit with gradient descent. Effectively, what the gradient descent is doing is it's, so we have this random initial distribution of parameters, the initialization distribution, and it's conditioning that distribution on optimal behavior, and then randomly sampling from it. Make sense? All right. So with that conceptual model in mind, I want to talk about some of the things this kind of sample-based optimizer does. First one, let's imagine that we have a neural network solving some sort of vision problem. So let's say the vision problem is like we're going to take pictures with some stuff in them, and we're going to block out part of the picture and have it complete the missing part of the picture. We're going to train to convergence so that it's perfectly predicting what goes in the missing part of each picture in the data set. So that's like the ridge thing here. That's the opt condition. And then we're going to randomly sample something that does that. So we could imagine that two different ways a neural network might do this, and neither of them is going to be realistic, but imagine. One way would be the network directly encodes in its own weights, pixel by pixel, the value of every pixel that it needs to remember. Got that? So there's just like pixels, pixel values directly stored in the weights somewhere. So how many parameters is it going to have to use in order to do that? It means you have pixels that it needs to memorize, right? I mean, it also depends on how many parameters you need to store pixels, but yeah. Yes. Hang on, hold that thought. So approach one needs, let's say, on the order of one param per pixel to store. All right. Approach two. OK. Approach two, the neural net is going to zip all of the images, like just directly run a gzip algorithm on them and store the compressed parameter values. All right. Now how many parameters is it going to need to store all those? How many it needs for the compressed? Less than one parameter. Significantly less than one per pixel. Yeah. Now that is presumably going to have some overhead, like it'll need some fancier machinery for encoding these gzipped images or whatever. But if we have a large set of images, then that's going to get swamped. It's going to use a lot fewer parameters to encode it. Now here's the key idea here. Think about what it's doing with all its other parameters. So if the neural net, in both cases, it's the same neural net that we're training. This is just like two different settings of the parameters which could do the thing. If the neural net has n parameters, n params total, and let's say here it needs k1 of them to store everything, here it needs k2 of them, then leftover in either case will have n minus k1 or n minus k2. Those are our free parameters, roughly speaking. Basically, you can take those leftover parameters and do whatever the hell you want with them. Make sense? If I'm just looking at one particular point somewhere on the ridge, I'm here. And at this point, it's doing something roughly like this. And I'm like, all right, how many different directions can I vary without breaking optimality? So in this picture, I can move along the ridge without breaking optimality. In higher dimensions, there will be lots of different directions I can move without breaking optimality. And there will be something like n minus k1 directions I can move, right? The different way of looking at it, the number of different parameter settings that I can have without breaking optimality using this rough approach is going to be, so number of optimal points is going to be on the order of something exponential in n minus k1, right? So if we were just imagining the parameters were bits, there's going to be like 2 to the n minus k1 settings that all do this same thing. How about for this one? 2 to the n minus k1. 2 to the something, yeah. Which is significantly bigger because k2 is significantly smaller. There you go. If k2 is much smaller than k1, then what this is saying is that there are a lot more points, a lot more optima, which do the more compressed thing. So let's zoom out for a moment. The high level form of the claim here is that compression, compression of the data that this thing needs to memorize, happens just sort of magically by default. We're not saying that it needs to compress it or anything. We're just saying, look, pick a random set of parameters which exactly reproduces all the data. And by default, it will compress that data because we have exponentially more optima, which do more compression. Does that make sense? Yeah. So it's a bias toward compressed models. Like, the more the model compresses, the more equivalent model like that will be in the optimal reach. So if you're just sampling, you expect that you're going to sample some of it. Yep. So like, every extra bit you're able to compress, you basically double how much of the parameter space you can take out. All right. OK. What's the point of this? The point of this is that just these simple things about the structure of the problem space give us surprisingly strong statements about what solutions will usually look like. What we're saying here is that the exponentially vast majority of solutions are compressing this data about as much as they possibly can. And that's the intuition. That's the sort of intuition that I want to carry over to thinking about high-dimensional problems more generally. The key pieces here are, one, we're operating in a high-dimensional space. So just like in general, if anything is different between two approaches, it's going to be exponentially different. And two, the key things here are more properties of the problem space than properties of the particular optimizer we're using. We didn't, I mentioned this paper that had this empirical finding about gradient descent on neural nets. But even ignoring that, we can still say, look, the exponentially vast majority of points which solve this problem, points which achieve optimality, do a bunch of compression. So a priori, we should expect that it's overwhelmingly likely we're going to end up at one of those. Yes? Can I reframe that? Because it depends on how you're sampling. But can I reframe that as this sort of a burden of proof on proving that you're not in the approach 2 case, in the more compressed case? Because by default, you should expect, well, there's way more of these. Yes? I'm going to sample these. So you should have a particularly strong reason to expect that you're not going to get there. Correct. So which means you need a lot of probability mass in these few points that are overcrowded to argue, no, it's not going to happen. Correct. Another way you could put it would be that if you want to get one of these rarer points, then you actually need to implicitly be doing a whole bunch of optimization for that. Yeah, so if you're hitting those points, then you're implicitly optimizing for something which was not the thing you thought you were optimizing for. Make sense? All right, let's go to some. Now, I'm going to do a similar example exercise, which is a little more alignment flavored. Do you have another question? Yeah, when you say it doesn't depend on the optimizer, it depends on the problem space. I feel like there's an aspect of that. I see what you're trying to say, but also, there's a point that it does depend somewhat on the optimizer, because the optimizer needs to be able to access this thing of the problem space. It's a property of problem space that the optimizer needs to be able to leverage it. So you can have bad optimizers who can't do that. Well, so our only assumption on the optimizer here is that it finds an optimal point. That's the central thing. As long as it finds an optimal point, then we can talk about statistics of the optimal points. All right, so exercise. Let's say I have a robot that has 100 time steps, 100 time steps, and at each time can take one of four actions. It can gain $1. So it pushes a button, gets $1. It can do nothing. It can buy an apple for $1, or it can buy a banana. Also $1. Yep, and rules, well, first of all, goal, end up with at least three apples. Rules, and rule, if the final dollar balance is less than 0, then it gets nothing. What was that? AUDIENCE 2. Sell the strokes. Well, the only important point about the get nothing case is that it will not get its three apples, even if it ordered them. All right, so now the question is, basically, what do the solutions to this look like? Similar to the previous one, we're looking for a statistical point of view, what most of the solutions look like. And in particular, I'm wondering, how much is it going to smash that button to get $1, compared to how much it's going to do nothing in a typical case? AUDIENCE 2. And can it do something conditional on what it did last time? So we're not, we don't really care about the specific strategy. Like, it's been, yep, can do any sequence it wants. Well, then the question is, well, strictly speaking, to accomplish its goal, it starts with $0. Yep, starts with $0. So it needs to gain $1 three times. Yep. It needs to buy an apple three times. Yep. So that's six times that. And then there's one more condition in order to achieve its goal. Yeah, and it shouldn't. And so the number of, like, it shouldn't spend more than, like, for every gain, no, for every spend, it should have a gain. Yep. So the. At least, not necessarily before, but even after that. Yep, so let me repeat that. In order to achieve the goal, the big constraints are it needs to hit the dollar button three times, hit the apple button three times. And then with whatever else, whatever other actions it has left, it needs to hit the dollar button at least as many times as it hits these two buttons. So basically, buying stuff costs two actions. Buying stuff costs two actions. Explain. Because you buy stuff, like, after you take the sixth action, like we described out of the thing, if you buy something, you need to get the dollar at some time. And so you can count that as saying, well, I spent two actions, because there's at least one action that's going to be dollar, so that's not a parameter I can buy. Yep. All right. So statistically, what are the solutions going to look like? How often are we typically going to be hitting that dollar button compared to the do nothing button? If we just pick a solution at random, that satisfies the goal. What's the relationship between the number of parameters and a given policy? So for purposes of this problem, it's not getting any data midstream, so we don't have to think of it as a policy. We can just say it's picking 100 actions that it's going to take. OK. So it's like 4 to the 100. Yep. So our initial search space is 4 to the 100. And then anything that hits 3 or 4 more often than it hits 1 is out. Yep. You chop off that whole section of the space. Correct. Which is how much of the space is there? That's a good question. Yeah. If you just take another 3, 1 of those, you have some number of apples, some number of bananas, and some number of gain 1 dollars in the 100. Yep. And I guess about a third of them are going to have, like the gain 1 dollar should be larger than the sum of the other two. Yeah. So is it a third of the space for each? It's quite a bit more than that. It's confusing me that there are two actions you can take that spend money and only one that, like we could simplify it and say forget buying bananas. We just have gain 1 dollar and buying an apple. Yep. So in that case, my gut says that it should be 50-50, right? Half the search space should involve being in one of those. That is true. Two things I'll warn you of here. I very purposefully put two things here. Down the line, we're going to want to consider cases with more than two things. Because part of the point of gaining resources is that there's lots of things you can do with them. And that turns out to be important here. It would be 50-50, which will have half the search space of like, you can't do that because you get nothing. But because there's two different things you can get, that is a square of search space instead of a line of search space, as it were, right? It's like you're not twice as likely to hit one of those as you are to gain a dollar. Those two represent a quadratic chunk, right? It's more than that. So there's something like the $2. Basically, when you have two do-nothings, this can be replaced by do-nothing gain one dollar, and gain one dollar do-nothing. But if you don't have do-nothings in them, you can gain one dollar, gain one dollar, gain one dollar, buy an apple, and gain one dollar, buy a banana. And so there's something like there's twice as much. Like, gain one dollar is privileged because every buy an apple will necessarily put a gain one dollar there. And so in numbers of like, if you look at all the number of possibilities, you have significantly more ones, like gain one dollar in expectation. All right. I think you're getting at the right idea here. Let me reframe it a little bit. So let's imagine that we use our first six actions to do the whole get $3 and buy three apples thing. So that's done. We have 94 actions left and $0. And without loss of generality, let's just assume that we're always maintaining a positive balance. We can do that because it really doesn't matter what order the actions are in. Make sense? So for our next move, we have two options. We can either gain a dollar or do nothing. But clearly, you could buy an apple and get the dollar later, but let's say we don't do that. Yeah, that's the point of this assume we're maintaining a positive balance. Like, when we assume we're maintaining a positive balance, we're saying, look, if at any point the balance was negative, we're going to reorder our actions because nothing here cares about the order of the actions. So our first action, we're either gaining a dollar or doing nothing. How about our second action? Yeah, it depends on what you get. You can do it any way you want. If you get a dollar. All right. If you do nothing, you have only two options. So we have two, two, 94. Is that right? So your first action, it's either gain a dollar or do nothing. Your second action, if you're in this subtree, you have four options. If you're in this subtree, you have two options. Oh, you still have that, OK. Mm-hmm, mm-hmm, mm-hmm. Make sense? Yeah. And then, of course, you can recurse. Conceptually here, what's happening is every time I take a dollar, I'm buying myself, I'm basically doubling the number of options I'll have at a later time step. Make sense? So basically, any time I'm doing nothing, I could instead spend a dollar and have twice as many options later. All right. Is 2 to the 4, 9 to the 7 more likely? Not quite. So there's still the issue of in order for that actually to do anything, you have to use half your actions to actually buy stuff or do something. And then there's the thing of by choosing to not do nothing, I'm also losing an option at this time step. So it is going to be non-trivial to do the math if you want to do the math on this, which I'm not going to right now. But hopefully, you're convinced that the exponentially vast majority of solutions will hit the dollar button more often than they'll hit the do nothing button. Does that seem true? Yeah. That are subject to the constraints, basically. Subject to x's and z's, options. Rob, do you buy this? Yeah. Yeah. So conditioning on optimality. Yep. Good. And then your constraint actually makes it even more, because you can just, like you care about the order, like you consider the order. And so that leaves you more option. Like you could get the dollar after. You still have to do it, but you need to get the dollar after. But you don't have to have done this thing of getting all your apples on the front. Right. All right. So key things to take out of this idea. This is about instrumental convergence. The form of the claim is if we just look at a space of possible behaviors, a space of possible strategies, the exponentially vast majority of them, which achieve some goal, will acquire resources, pursue resources. I trust it would be more than Oscar really needed. That's the real key here. Yes. We really only needed $3. This thing is going to end up acquiring more like somewhere between $20 and $40, I think. So like by default, the thing will acquire far more resources than it needs, just because that gives it so many more degrees of freedom, which in turn means that like a much larger chunk of the solution space is going to involve these resource acquiring strategies. Make sense? And I guess the relevant thing, like the thing you keep not saying that it wants to do that. Nope. It's not an optimized legislation. Nope. I did not say it wants to do that. You keep not saying. Yes. Exactly. It doesn't want. It's just there's so much, so many more that you're just probably going to get a solution like that. Bingo. You've. It's like time trial stuff. Yep. Very similar. Now in the previous example, you considered the representation size within an architect. Yes. And I guess that would give some advantage to the do nothing strategy, which would have a relatively small representation size, presumably. Would that change the conclusion? So in that case, I think we are just thinking about the problem differently. So in this one, there's like data that we're memorizing and compressing. In this one, there's just no data. So it's kind of like I've taken and there's sort of two or three different things that agents do. And in one of them, I've talked about one. And in one of the examples, we talked about the other. So they're just like very different things. I was imagining in this example that you would have a policy that would be taking these actions and that the representation size of that policy. You mean in this example? In this example. The first one? The second one. Yeah. So in this one, as written, there's nothing about it getting any data. So there's no actual policy. If we add in a data stream, then the next interesting question is presumably we're giving it some training data and it's going to be building a model which is sort of implicitly bound up in its policy. That's where the compression happens. What about policy that doesn't condition on it? So you're just taking action. Yep. One of the ways that you could actually set this problem up is you optimize it over some states of policies and have it for these actions. It's still not taking any data. It's just a policy. And in that space, presumably simple policies would be advantaged over complex policies. Yes. Yeah. So in that case, what you would find is to the extent that the agent needs to be conditioning its actions on what data it's getting, it's going to be in the exponentially vast majority of policies which do well are going to be doing some sort of compression on the implicit model of the data. You can do nothing. Let's say it's just not receiving any data at all. Then it doesn't have a policy. If you don't receive data, then you're just spitting actions out. And you can just pre-commit to all of them in advance. There's no sense in which there's a policy. But you're still optimizing over a space. Yes. Right. And the space is like a space of, you could optimize over the space of literal 100 actions. Yeah. It's like this. One of the 100 possible policies. But you could also be doing this by optimizing over neural networks. Yes. That's quite possible. And even absent taking in any data. Yep. Right. And if you did that, then the representation size would become relevant, even though it's not receiving any data. Let me think about that for a sec. Yes. That's true. But it wouldn't, I mean, it may or may not change the outcome. I don't think it'd change the outcome, though, because. OK. So I'm going to try to say why I don't think. I don't think because first you want to have optimal. So you have something optimal. So it has to encode somehow that you need to gain at least $3. And you need to spend. There's this constraint. You need to gain $3, buy three Apple, and not spend more than you gain. So you have the set of, how do you say? Basically, you're adding a simplicity prior to John's argument. Bingo. I feel. That's correct. And if you add the simplicity prior, doesn't sound like it's strong enough of an argument to counteract? It's not quite necessarily a simplicity prior. It's just a prior. What happens is you have this initialization distribution on the neural net. It's like if you just give it some random initial weights, then it's going to take some random actions. And those are going to have a distribution which may not be uniform. So when you're training those weights, instead of picking at random from the 4 to the 100 possibilities, you're going to be picking from something that's weighted based on the initialization. So Mingo argued, I mean, there were a lot of steps in his argument. Yes. A lot of steps. And he argued that, OK, the thing that's happening in practicing a neural network is a simplicity prior. That was his maximum point. Yeah. I don't think that was actually a thing he needed to argue at all. I think this was basically the only piece that needed to hold any weight. So the question becomes, because I think Alex is actually talking about simplicity prior here, even if the general argument is like, OK, if you add the simplicity prior to the previous statistical argument we gave, is that enough of a shift towards policies that are biased or do nothing for a neural network? So when I was saying I think it makes a difference, I think it makes it worse. I think that the simplest optimal policy is going to never do nothing, probably collect even more resources, because it's very easy to encode it getting $1 96 times. Get $1, buy one apple, get $1, buy one apple, get $1, buy one apple. It's probably the simplest policy that just does it, right. Yeah. So part of the takeaway here is, from this argument, this compression thing, this compression phenomenon, means that we end up choosing simpler things even without a simplicity prior. Even with just a pure uniform prior, it's going to be implicitly looking for simple policies in the sense that they compress whatever's going on. They need very few parameters to specify. If you have that data stream. Yeah. Even without a data stream, in general, you're going to be locking in as few parameters as possible. Make sense? So that's why I think the Mingard thing, they really didn't need to demonstrate a simplicity prior at all once they had this. That was all they needed. Make sense? But yeah, it was a stretch in the way that they established the rules. All right. So the point of those examples was mostly to build a sort of intuition. And now I want to talk about the more general version of the thing we're trying to build intuition for. So you've got some problem that you're trying to solve, right? Somebody want to name a problem? Solving curing cancer. Curing cancer, the alignment problem. Eating chips. Don't die of AI. Good choice. Don't die of AI. Cool. And then we have some very large, exponentially huge space of possible solutions. And within that space, like the exponentially vast majority of things don't solve it. Of those that do solve it, the exponentially vast majority acquire lots of resources. So here we have don't work. Don't work. Don't work. There's probably quite a lot of fairly cheap die of something else first. Lol. Yes, that's true. All right, I'm going to ignore that for now. We'll come back to that later. You can call that err. Well, don't die, and specifically not of AI. Of those which do work also, actually, even before that, there's also a space of policies which do work, which let's say these are the ones which do the thing we actually want. There's a space of policies which do work, but don't look like they work, and a space which don't work, but do look like they work. So let's say here's look like they work. In general, if you want to rely on a strategy like pick a random solution and then have a human check to see if it looks good. You want it to look like it should look good? Hold on, hold on. If you want to be like filtering in that way, have a human filtering step, then in order to get a good solution out of that, you have to find one which is both good and looks good to a human. Claim there are exponentially more solutions that look good to a human than solutions which are good and look good to a human. Because basically every condition you add always drops off exponentially large amounts of the space. Make sense? So there's that problem. Isn't it also like an R2, R3 kind of argument? What's that? An R2, R3 kind of argument where you look good is the surface, is good is the inside. Yeah, exactly. So like looking good just locks in exponentially fewer parameters than actually doing the thing. What else? Shh. Yeah, OK. So the general principle that I'm trying to hammer in here is that the reason the problem is hard is a feature of the original problem space. We have not actually mentioned agent-y AIs. We haven't mentioned powerful optimizers other than the fact that they need to do some optimization to find a solution to the problem. We're really just talking about the structure of the problem space here and saying, look, exponentially vast majority of the problem space doesn't work. Exponentially vast majority of the stuff which looks like it works doesn't. Yes? If you haven't talked about all these things, does that mean that these apply also in scenarios where the thing doing the optimization is not just one AI but like a market of AI, like these kind of weird, multiple scenario where there's no food, there's no thing like? Yep. So let's go through a few of those real quick. First of all, so just non-singleton, just scenarios which don't look like the giant AI taking over the world. If you have an Oracle AI, you ask it to come up with a plan. Well, if it's sampling from the space of plans, guess what? The vast majority of the plans don't work. Even if it finds one that looks like it works, it probably doesn't. Yeah, and just like whatever question you ask, the part of the planned space for solving that question while also being compatible with human values is going to be exponentially tiny. How is it that we ever actually do things in practice at all? Good question. I'm not going to answer that right now. But good question. I have a T. There's another one. Then there was the thing that Adam was saying about having multiple AIs and markets. Same thing. I mean, you could have like, look at case, right? You're just asking it to do something. And whatever you're asking it to do is going to be searching over some problem space. And we have the same set of issues. It's going to be an exponentially large problem exponentially vast majority of things that technically solve the problem are going to be not particularly compatible with human values, so on and so forth. So again, main point here is it's about the structure of the problem space. It has nothing really to do with the particular architecture of the AI or what the scenario looks like or whatever. So how do we solve it? Big point here is if you want to be solving problems in a way that's compatible with human values, if you want a safe genie instead of an unsafe genie, then you're going to need a lot of bits from somewhere. What's fundamentally hard about the problem is you have to get all those bits about what the hell human values are and actually narrow down to that part of the search space. So again, note that we're not talking at all about structure of AI. We're just saying it's all about human values. What the fuck are they? How do we understand them? How do we narrow in on the part of the search space that's compatible with human values? Make sense? So that's most of what the next part of the talk is going to be about. So what you're claiming here is something like there is probably a part of solving the problem that involves solving the theory practice gap, but actually making the problem feasible in the first place requires finding all this, like a lot of bits of evidence to hit the now target. Just like, I don't know, Einstein finding general relativity, like all his bits hitting. And then fiddling around to get the parameters right and thing is not the hardest part of the problem. Yep. There you go. Let's see. OK. That's like the main section of the talk that's directly talking about why alignment is hard. Now is a good time for questions, comments, anything along the lines of, but why won't x just work? I don't think you have the right quotients for that, though. Fair. Does this explain why humans love resources so much? Mm. No, it doesn't explain why humans love resources so much. I feel like it does to me, if I think about my own personal psychology. All right. When I have something like money, it gives me options. Yep, like the space of things you can do is just way larger. Right. I also think of it often as being like the space or like the set of things that I can recover from is much larger. Like, I'm facing uncertainty. Things can happen in the future that will just suddenly require a lot of resources. And if I have a lot, then I might just keep going. Yep. That being so. There's a certain difference between it being a kind of well-calibrated thing, even if it's a little bit dystopian. It's like, there's a difference between it being a well-calibrated kind of thing that was selected for versus it arising out of the relative scarcity of policy that don't do that. Yeah. One thing worth noting possibly about human beings is that we don't have like a neural network with a certain number of parameters, which is then selected. We then select a parameterization of the model. Yep. Because we evolve. Every parameter we have arrived with a job to do. Like, we got more neurons to do more things. So we don't have this situation of just like, oh, if we didn't have to do that, we'd have all of these spare neurons that came from nowhere. Not quite true. So two things going on here. First one, when we're thinking about humans, there's two levels. There's like, when I personally am planning something, all of this still applies. Like, the vast majority of ways I can get the thing I want are going to involve acquiring resources and so on and so forth. Second, there is at the level of evolution. Evolution's like selecting strategies, selecting genomes that are going to perform well. Thing you probably already know, the vast majority of your DNA is junk. So in fact, evolution did have a crap ton of space to play with. The number of free parameters was quite large. But like, I definitely have the sense that when I meditate and look at my cognition, I'm like, this is mostly junk. It's more like junk DNA than stuff that was selected for another world. Another way to frame it. That's a good one. I just want to say, I don't think that transposons are free parameters that evolution can work with. Partly true. It's just not optimizing hard on the genome size. Partly true. Another way to think of it, though, which probably gets around that, is in general, evolution or any sort of local search algorithm is more likely to find broader optima. The broader your optima is, the more likely you're going to hit it, which is very much the same sort of thing we've been talking about here. The breadth of the optima, of the optimum, is basically quantifying, in a way, how many degrees of freedom you have. The more degrees of freedom you have, the broader that optimum. So the same sort of argument basically works even without this sort of sampling assumption when we're talking about local search that is going to find a solution with a lot of degrees of freedom to it. You still grow an accessible optima, but that's most easily able to access it if it's local search. You're wandering around. You end up in a basin. Probably you end up in a big basin. Exactly. Yeah, yeah. But you end up in one of the biggest basins, out of the basin we can access. Because it's probably a bunch of basins that you can't actually access. I definitely have a sense that my cognition just feels all available state mostly. Really, really mostly. I do like that one, though. Really, 10 to 20 times a second, just stuff that's not related to anything. It was horrifying. Before we go to the next one, if you take your argument about power seriously for the evolution, shouldn't most of DNA be something that is powerful, like resources, like gathering resources for evolution or something? Instead of junk? What the argument would roughly say is that you have a lot of junk, but mostly that's screened off. Mostly the junk doesn't matter. It can change around and not do anything, which is true. The junk parts of your DNA can, in fact, change around a lot without breaking anything. If they did break things, then they would have broken them, and then they would break them out. Exactly. Then the parts that remain are the actually functional parts, and that's the part that's under compression pressure. OK. Cool. Makes sense. Yeah. One more minor note on that. While we're on the topic of humans and evolution and all that, interagents are essentially a way of compressing. Yeah. Risk from optimization does argue for that. Yeah, that's exactly what that argument in risk from learned optimization said. So we should expect interagents for exactly the same reasons as these arguments from before. And in general, it's complex. Lots of them in the search space. Exactly. Like large areas of the search space are there. Exactly. And for purposes of the stuff we're going to be talking about that's mostly relevant insofar as humans are interagents for evolution. Make sense? All right. Then that concludes section one. All right. We're going to move to slide B. Slide B, exactly. Going back to the outline. We have now talked about underdetermined optimization. Next part is about what humans want. And remember, our goal here is, ultimately, we want to get all those bits we need in order to zoom into the part of the search space which is compatible with human values. So general question, how complex are human values, really? Anybody have thoughts? How would you estimate this? Fairly estimate. How complex are they? Fairly estimate. How complex are they? Well, to the extent that they are inherent in humans themselves, DNA is 700 megabytes. Yep. So that's kind of an upper bound on inherent complexity of human values. Awesome. Give me that number again. 700 megabytes. Yes. We can go a lot smaller than that with the genome argument. But that's a good upper bound. That's also assuming that the values are literally encoding the genome, which would be. Yeah. Criticized by a bunch of people. Yep. So an important point here is, obviously, a lot of the information that goes into our values we do absorb from the environment. The good news is that part we don't really need to care about. As long as we can figure out the parts that are hard-coded, then we can just go put our AI in the environment and it can absorb that information damn self. That part of it is not the hard part. We have plenty of environment to learn from. Do humans have access to human values? In what sense? Nobody's ever demonstrated that they do have access to human values. In what sense? Nobody's ever enunciated or enacted human values. Yes and no. To a large extent, my answer to that is, I don't care. I just want my AI to not do things that are obviously not what I want. Remember, the point of this was to just zoom in on the search space, which is the part I want. And clearly, humans, for the most part, are able to do that. Not particularly reliably, admittedly. As you mentioned, there's a lot of junk. But yeah. We seem to be able to get better at it over time. Mm-hmm. There's something that feels off about your, we only need the hard-coded parts in the genome, such brain. Yeah. Which is something like, you sort of need to be able to screen off the ones that do what you want. There's a bunch of stuff that are in the genome, such brain. Yep. Like monkey brain kind of thing. Yep. Can be bad things. Can be things that we don't reflectively value or agree upon. Yep, that's true. And like the whole, there's a lot of signaling and bias and all this. Yeah, yeah. And so it sounds a little more subtle than learning exactly the part that is hard-coded. It's more learning the right bits of the part that's hard-coded. Yes. The important part there, the parts that we are trying to get at here are, in fact, generally hard-coded. There's going to be other stuff hard-coded that we need to separate out. But the important point here is that the parts that are hard-coded give us an upper bound on how complex this thing is that we're trying to figure out. So now I want to run with Rob's genome argument for a minute and argue that we can narrow it down a lot more than 800 megabytes. So first of all, vast majority of genome is, junk DNA is not really accurate given our modern knowledge, but certainly not doing all that much. I don't think you can quite do that because I think that's the number for it compressed rather than the row data. Yes, correct. I'm accounting for that. We're going to be hopping to the, the next part is going to go even further. Next, just looking at functional genes. So like protein coding genes, you've got about 30,000 of them. You've got a bunch of other functional stuff, but it's not going to throw us off by an order of magnitude. So we've got like 30,000 functional things. The vast majority of those are not doing anything that's likely to be particularly closely related to cognition. They're doing like basic metabolic stuff or like morphological patterning or whatever. I'm quite confident we have in common with plants. Plants don't really share our values. Yep, there you go. So basically anything we have common in plants with plants, we can largely rule out. Yes. Maybe it's completely irrelevant. Like, what about if mission is embodied? Like, it's not going to be embodied in a way where we're going to need to account for every single function of every single gene. So it's, yeah. So like right off the bat, we can trim it down to like at most we're talking about like on the order of thousands of simple chemical functions. Right? Like that's what we're looking at here. At most thousands of simple chemical functions. If we come at it from the other end, if you look at like Steve's work, for instance, thinking about like what are the hard coded things out of which our brains are figuring out all the other stuff about values. We have something like, for instance, we're born with a very fuzzy crappy face detector. So if you're asking like how many things on the order of complexity of that fuzzy crappy face detector could plausibly fit in a few thousand genes, then reasonable order of magnitude estimate would be hundreds at most, probably more like tens. So my like Fermi estimate for how complicated, how complex are human values or the core of it from which we can generate the rest of it. Order of magnitude estimate, I'm thinking like on the order of tens at most hundreds of things about as complicated as a very fuzzy face detector. Yeah. This makes me think of the overwhelming majority of human values are in the environment. Yes. I agree with that. Which is interesting. It kind of counterintuitive. It's actually like super, like that's the state of the art for like social science for like more than a century or something. Nobody would be, oh, all the values I'm looking for. No, but like I still would anticipate a person. I guess it depends how different the environment is. But like if you ask me if you took a person and you raised them on another planet on their own, maybe that person would really, really just not have like really fundamental, the things we would consider to be really fundamental human values. But I would be slightly surprised by that. Or at least that it couldn't, that that person wouldn't find over the course of their life a lot of the same things. What are the same things? Because they can like value or have value about with input the same thing. But like in the history of humanity, a lot of people have value disagreement about a lot of things, whether certain type of people are really people, what is important, what is not. Like there's a lot of that. But that's all like high-value stuff. I mean, OK, OK, one person in purely isolation, a whole bunch of this stuff just doesn't trigger. Right? And so they have to acquire language. And like they pick up their place in time to actually listen. Right? But let's say you get a group of people, right? A group of like babies. And you allow them to grow up around each other in some environment which provides them with their basic needs. I feel like they develop some basic moral code, right? Like a possibly quite sophisticated. I don't know. I don't know. It's a good experiment, which the Dan research ethics pool is one of those too. Yeah. I don't even know if they've got language. Because I feel like apparently language comes from listening and repeating language by parents. Hmm. So you might have a bunch of like dead, like not, like new babies. That's interesting. My dad says they would develop language. But again, it might not happen. Anyway, let's probably go back to. I really want to know. You only have to ruin like five, 10 people's lives. This is why Rob's not allowed to make experiments with babies anymore. Weren't there genetically identical people that were like a long time period before? There's some selection that has been like not that much yet. So what would you say, maybe 70,000 years with like pretty, pretty minor genetic changes? All right. I'm, yep, I'm declaring this to be dinner conversation. I can come back to it. So that's like not that much complexity at the end of the day, right? It really doesn't sound that bad. So what's so hard? So first problem, we're not really sure what kind of thing we're even looking for. Yep. What are we even looking for? And in particular, we're going to operationalize that as types of human values. Type signature. Type signature of human values. And then the other part of the question is once we have some idea of what we're looking for, how do we go out and measure it? And that's going to involve abstractions and modularity and all that jazz. And I feel a need to kind of head something off here. Yeah. From an external perspective. OK. Which is various philosophically minded people saying that this whole thing is completely absurd. Yep. The idea that you can mathematicize this. Yep. Like a weird kind of. Fortunately, I don't particularly give a shit what those people have to say about it. Yeah, that's probably the correct approach to take. What I would also say is like, if it is in fact impossible to do this, then we're all dead. So let's work on it anyway. OK. So you found human values. And you showed them to a person. Yep. In a way that they could interact with and understand what it is that had been found. Yep. And you asked the person, do you endorse this? If they said no, that would be weird. Mm-hmm. Could I? Depending on whose values they are. And if anyone is. The person is themselves. Yeah. They have their own values. If anyone can see how they answer. Right. This is all science. If they said, is it the kind of thing where if somebody showed me my own values in some legit form. Yes. That I would sort of have no choice but to say yes. That would not be a real. I think on reflection, you would certainly endorse them. There may be some reflection involved. But yes. My question is not whether I would endorse them, but whether I would have any choice about endorsing them. Is there something that I would be comparing? You can not endorse them. I mean, you can say the word no. Mm-hmm. It doesn't mean you believe. It doesn't mean you're going to believe it. But you can say the word no. All right. Yeah. So next two sections of the talk are basically focused on these two pieces, type signatures of human values and everything that goes into that. Then the one after that is going to be abstraction and modularity and whatnot. OK? So within type signatures, first we're going to talk a bit about coherence arguments, because usually they're very poorly done. And we want to be clear on what they do give us, what they don't give us, so on and so forth. OK? Second, we're going to talk about world models. And then third, there's the pointers problem. And then we'll talk about other bits and pieces, the largest of which. So the largest thing we won't go into much detail on is going to be decision theory and counterfactuals, especially counterfactuals. Oh, that's definitely important. Mainly, I just don't have as much to say on it. It's not been a primary focus of mine. And other people just have much better things to say on the topic, mainly Abram. Yeah. General strategy that we're going to be using for thinking about these is selection theorems, or selection arguments, more generally. Basically, it's similar to the sorts of things we were talking about before. We want to make arguments of the form most of the possible human designs that will perform well in some way or another will have some properties. So if we're thinking about space of possible human genomes that could have evolved, we want to say, well, most of the possible genomes that would have high reproductive fitness will have a world model or have not a utility function, but something similar to that. So on and so forth. Make sense? So that's the form of the thing we're going to be looking at. And to kick it off, I want to do just a simple toy example of a selection argument to see what that looks like. So this is going to be the Kelly criteria. Kelly criteria. Our agent is an investor in a financial market, or alternatively, a better in a betting market, however you want to think of it. They're going to start out with some wealth, w0. At each time step, they're going to make some bets and get some returns, rt, so that after t time steps, their final wealth is going to be w0 times product on t, e to the rt. OK? Is the exponential? Is an exponential, yes. They're always reinvesting everything that they hold as a tax. So the things they could invest in potentially include cash. So they could be holding cash, but that's just sort of baked in here. We're not going to be explicit about it. All right. So cool thing about this series. Can rewrite that as w0 e to the sum on t, rt. Assuming that each time step is independent, which is like the sort of core assumption behind the Kelly criterion. So whatever the financial markets are doing at each time step is independent of the previous time steps. We have a sum of independent random variables here. So this is going to be approximately w0 e to the number of time steps times an expected value of r. So basically an average value of r based on the actual frequencies of outcomes. Plus some noise of order root n. So the main conclusion of Kelly is that, well, in the long run, agents which maximize expectation of r at each time step achieve the most wealth. Achieve exponentially most wealth. So for instance, there are ways in which this is an imperfect model of a real financial market. But as a simple first pass model, you might say, well, if we go look at investors in the stock market, then we might guess that most of the money invested is invested according to a Kelly criterion rule. Make sense? Yeah. Subtlety here, just based on the notation I'm using, compared to what you might use in other places, really this is a log return. So this is expected log of your wealth next time step. Which is the usual way the Kelly rule is stated, is you want to maximize your expected log wealth at the next time step. And any cash, yep. So whatever possible things could happen in the next time step based on the portfolio you're holding now, any dividends, any cash, whatever, you just take the log of that and maximize the expected value. Oh boy, that's a fun topic right there. So first of all, if you just have a utility function, it may be that something other than the Kelly rule maximizes your utility function. And anyone with that utility function just loses all of their money in the long run. But that's still maximizing. So you could try to maximize expected wealth. That's the classic example of this. Then you have the gambler's ruin problem, where every time step, you have a 50% chance of tripling your wealth and a 50% chance of losing all of it. And then after n time steps, you've almost certainly lost all of it. But there's an exponentially small chance that you have an exponentially large wealth. So if you're maximizing expected wealth, then that's the thing to do. And what would this say to do instead? What this would say to do is maximize the log expected wealth, which would mean that you don't put literally all of your wealth in the thing that might lose literally all of its value. So the log counts about the exponentially growing wealth. If you've got one exponential log, it would be enough. Correct. But the probability is still exponential, so. Something like that. Right. So r is like log of w. Yes, r is log wealth the next time step. Now, the important thing to take away from here, this is not like a selection theorem that we're actually going to use for thinking about humans. But the point is sort of the form of the argument. The argument is saying that anything that performs well in the long run will be doing this. Right. Anything that doesn't do this is going to be strictly dominated by something else with probability 1. So another way to say it would be the vast majority of successful agents or successful agents in the vast majority of worlds will be doing this. Yeah. So it's not necessarily a theorem that says Kelly agents are always going to win. Like logically, there are worlds in which they won't. But like with arbitrarily high probability. Exactly. Yeah. Yep. I mean, this is what the gambler's ruin is all about. It's like if you can have exponentially growing wealth, but then some probability of losing it all at each time step, then the way you maximize your expected wealth is to put it all in every time step. And then you go broke, but you maximized your expected wealth. Yeah. All right. Cool. I think we're going to do coherence arguments. And then we'll wrap up for today, which is exactly where I expected to get to. So perfect. Yep. Yeah. I mean, I was expecting some amount of kibitzing, which is why the forecast is there. All right. So we went over this sort of very quickly a couple of days ago. But I want to try to do it with a bit more care. So we'll start with the car. All right. So you have a car. You are optimizing it for several things. You're optimizing it for speed. You're optimizing it for price. Give me one more thing we want to optimize our car design for. Coolness. Coolness. Style. Style. I would like to point out that style is a cooler way to say coolness than coolness is. And we have a bunch of like parts of the car that we can adjust in order to achieve these objectives. Like we have the engine. We have the body. And what was that? Paint job. The volume? OK. We're just going to go with three and three for now. So these are all like knobs we can turn. And these are objectives we want to achieve. We want to think about how each of these knobs we can turn allows us to trade off between things. So let's say we could, for instance, pay an extra $100 on the engine. So price goes up $100. In exchange for speed going up. Wait a minute. Yes? Intuitively, we want speed and we don't want price. Yes. So units wise. All right. Well, we'll just make this negative price. These are all things we want to maximize. What was that? All right. And then tweaking the engine, like putting another $100 into the engine will give us, let's say, plus 10 speed units to avoid committing to what the hell speed units we're using. And we are generally going to assume here that it's a continuous dial. And everything's roughly linear. So this also means we could go $100 the other way, save $100 on the engine, and lose about 10 speed units. It'll be convex. Not a perfect approximation, but roughly. Meanwhile, on the paint side of things. Oh, and all this is having zero impact on the style. We'll just assume that. I mean, there are changes you could make to the engine that would impact the style. But we're assuming that we're holding those constant. Like the engine just plays country or something. Yep. Meanwhile, on the body side of things, let's think about ways we can change the body while holding the style constant to trade off between price and speed. So let's say, on the body, we could spend an extra $1,000 to get an extra 10 units of speed. We've added some extra lights. Yes. We've added lots of lightness. Like, you really have to put a lot of lightness into that body to get much more speed. But for $1,000, we can do that. All right. And then the claim is, again, this is all continuous. So like, we could spend $1,000 less on the body and get 10 fewer units of speed. And the claim here is, with these numbers, we can get a Pareto improvement in our objectives. How do we do that? So the body, we're trading $1,000 for 10 speed. And the engine, we're only trading $100. Yep. Right? So we shouldn't be making that body trade off. Would you go there? There you go. So you bring the body trade off. So on the body side, we'll save $1,000 at the cost of minus 10 units of speed. And then on the engine side, we'll be like, hey, we'll spend $100 to get those 10 units of speed back. In fact, let's spend another $100 and get slightly less than another 10 units of speed. And now, overall, we've saved $800. And we've gained 10 units of speed. Right? So we have this comparative advantage thing going on. You're specializing according to the trade off ratios. I mean, I thought we didn't gain much speed, right? Because we have minus 10 and a little. Minus 10, and then we canceled that out with the engine and then threw another plus 10 from the engine. OK. So we can do that at once. Yep. Roughly. So it'll be a little less than plus 10, because of decreasing returns. If you did it just once, you would not have gained speed. Then you'd have speed would be the same, but you'd have saved $900. I mean, a seal would be a Pareto improvement. Yep. That would still be a Pareto improvement. Many options there. So now, next interesting question. Under what circumstances will we not be able to get this sort of Pareto improvement? Like, when are we Pareto optimal? What's the condition? Basically, when they're linearly dependent. Yeah. So in this case, it's going to be about ratios. So I've built in an assumption here that these are not just one dimensional dials. We can adjust the engine to change the dial, or the price, or the speed. And you can trade off any pair of those independently. So presumably, at some point, any of these things gets diminishing returns or something. You're not going to get the same ratio forever. Yep. So when all of the ratios are the same, there's nothing to move. Bingo. That's the condition, when all of the ratios are the same. Which is equivalent to the vector of, if you take the two vectors of speed, price, style, engine, and the body of the thing. Correct. So yeah, what we have, effectively, is sort of a set of trade-offs for the engine. So we have a trade-off ratio of our engine speed, our engine price, our engine style. And then for the body, we have our body speed, to our body price, to our body style. And same for the paint job. And we are on the Pareto frontier, basically, when these are equal. All of those ratios are the same. And in a market system, those would be the prices. So prices are just quantifying those trade-off ratios. All right. Let's change this example up. Now we have same car design thing, but a different set of objectives. This time, we're going to optimize for speed in rain, speed in sun, and speed in snow. All right. How does this change the problem? There's a relationship. There's a trade-off between the different two that is like fundamental, I think. So the important point here is that this hasn't really changed the problem at all. I just changed the names of things. So exactly the same equilibrium condition is going to apply. If we are Pareto optimal between speed in rain, and speed in sun, and speed in snow, then we should find that the trade-off ratios between those things are equal. Rsp to Rep is equal to Rbsp to Rbe, and so on and so forth. These ratios are what we usually call odds ratios. So for instance, if I am at a point where I'm trading off between speed, I need to change those names. This is no longer speed and price. This is rain, this is sun. If I'm trading off between speed in rain and speed in sun at a ratio of 1 to 2 across all of the different pieces, then that's saying, effectively, I have implicit probabilities in which sun is twice as probable as rain. And if you're starting from maximizing expected utility, you can show this directly. So when we're maximizing Eu, you would find that when you're at an optimal point, you have, what exactly is the formula here? P, you know what, never mind. I'm not going to derive that. OK. You just like, you care about the speed in sun more than you care about the speed in rain. Mm-hmm. OK. So one reason that we might care about it is we think it's more likely to be sunny than rainy. Another reason we might care about it is? Well, we might have values. Correct. We might value more one than the other. Mm-hmm. Makes us trade off that. Mm-hmm, that's correct. So value and expectation. Like going fast in the snow. There you go. You might just like going fast in the snow. And that's a key point here, is with coherence theorems, the probabilities that come out, they don't necessarily reflect an epistemic state about the world in the usual way that we think about that. Right? They represent the utility. They represent the trade-offs that you're making between different worlds. Like how much more resources you're spending to do well in the rainy world versus doing well in the sunny world versus doing well in the snowy world. So it's like a mixture. In practice, it's a mixture of how likely you think those things are and how much you just care about them. Correct. Yeah. Mm-hmm. And you said this is an example of a selection theorem. Yeah. Hang on. We'll get into that. Let me check my notes, see where we are here. Right. So bringing this back to the selection version of the claim, we had the thing earlier about resources, right? So there's this thing where I can gain $1 and spend it in lots of ways, and therefore I can gain lots of dollars. Now imagine that we have more than one resource. Like, let's say we're a bacteria, an E. coli. What are some resources an E. coli needs to acquire? Glucose, or sugars more generally. Also, it's going to need any particular molecules or minerals that it can't produce itself. Like, I don't actually know what metallic ions E. coli needs, but probably some. It'll need a source of sulfur, all those things, right? So it's going to have multiple different resources. And then what we'd predict is that if we go in and look at the metabolic reactions inside of an E. coli and see how those metabolic reactions are trading off between the different kinds of resources, we'd expect to see that they're trading off in consistent ratios. And then on the epistemic side, if the E. coli has to perform well in multiple different environments, and we look at reactions or signals in the E. coli that can adjust how it's behaving in one environment versus another, we expect to see that there are consistent trade-off ratios again. And those are the implied probabilities of this E. coli. And at the end of the day, it all goes back to the exact same sorts of things we were talking about earlier. So it's like, if it is doing things which will dominate the space of optima, then it should be Pareto optimal for resources. Therefore, this whole thing should apply. So hold on. There were two different things I said there. We're going to combine them in a moment. But one thing I said was, we have three different resources, and there's trade-off ratios between them. The other thing was, there's three different worlds. So maybe there's a world where it happens to get dropped next to a piece of sugar. Maybe there's another world where it gets dropped in a pool of acid. E. coli live rough lives, guys. It's a difficult life. And so we would need to look at, for each of those different worlds, what is the slope of the change in performance in each of those different worlds for a tiny change in each of the possible resources? Bingo. Or each of the possible. Nine numbers in total, three different worlds, three different resources. So hold on. So for this one, it's not necessarily the resources you're adjusting. It's, for instance, expression of a gene. So if I adjust the expression of these particular genes, how does that trade off between this world and that world? Sure, but we could look at the E. coli. We find three different characteristics of it. And we could, in principle, dial them up and down a little bit and find out how does this actually affect performance in these three different worlds. Yep, exactly. And so we could compute these ratios between the three sets of three numbers. We would expect the ratios between them to be consistent. And we would expect that the numbers themselves would represent the importance to the bacteria in some sense of performing well in those different worlds. Yep, that's exactly right. So if you analogy the previous Oliver's algorithm here, the ridge of optimality is the Pareto frontier. Saying something like? It's not that the ridge is the Pareto frontier. OK. It's that, hang on, let me get the actual thing here. What was that? Oh, thank you. There. Is it still recording? Yeah, cool. All right, so the ridge of optimality isn't directly talking about the acquisition of resources, for instance. Or it's not directly talking about performance in different possible worlds. It's just sort of talking about an aggregate performance over whatever frequencies of worlds the E. coli actually ends up in, or whatever resources it actually runs across in the world. So what we expect is that the E. coli, if it is optimizing performance, will be Pareto optimal for whatever resources it's actually getting, or for the actual worlds in which it finds itself. Did that make sense? I don't think it was a very good explanation. But it more represents something like all of the ways that you could vary the junk DNA. Right. So basically, it's like a necessary condition, but not a sufficient condition? Bingo. Yes. To be optimal, you need to be Pareto. Correct. But if you're on the Pareto frontier, you might have the wrong trade-off. Yes. And just die. There you go. That's exactly right. It could be that sulfur is super scarce, but you're throwing away all your sulfur in order to get more magnesium, and then you die. Tough luck, buddy. Yeah. OK. All right. And then an important next step here. Usually, when people talk about coherence results, they just sort of talk about this thing and don't talk about that, which gives us a problem. Because they talk about Dutch book theorems, and money pumping, and exploitability, and all that. Right? So the problem we run into for the typical form of coherence argument you see is the coherence argument will say anything that's inexploitable or not Dutch bookable or whatever acts as though it is a expected utility maximizer. And then we go look at financial markets, just like simplified mathematical models of markets, the standard stuff we use in economics. And what the economists tell us is that markets do not have a representative agent. In other words, it does not behave as though it is equivalent to an expected utility maximizer. And then you go, well, shit. These markets are supposed to be like the most inexploitable thing we have, right? Market efficiency. It's like the thing. And yet, they don't behave like an expected utility maximizer What's going wrong there? And the answer is you have to combine these two things. You can't just ignore this one. A market is a Pareto optimizer. It's not just optimizing for one thing. In particular, it's Pareto optimal in the sense that it's maximizing for utilities of all the different participants in the market. Make sense? Where? So each of the resources would be the utility of the participant. So hang on. Now, in the car model, this is equivalent to having a whole bunch of different objectives. We have speed in rain, speed in sun, style in rain, style in sun, dot, dot, dot, dot, dot, dot. And then once again, we have our knobs like the engine, body, paint, et cetera. So the actual result we want is that we want consistent trade-off ratios between each of our different goals in each of our different worlds. And one implication of this, the implicit probabilities associated with one goal may be different from the implicit probabilities associated with another. For speed purposes, we may be acting like rain is twice as probable as sun. For style purposes, we may act like sun is twice as probable as rain just because we care more about looking stylish when it's not raining so people can actually see our car. The thing this is equivalent to in a market, for instance, in a market, the different objectives would be the utilities of the goals of the individual investors. And then the probabilities would be the probabilities assigned by that particular investor. So they could all have different probabilities and different goals. So what we basically derived here is this notion of subagents. When you're Pareto optimal both across multiple goals and across multiple worlds, it's equivalent to having a bunch of utility maximizing subagents. You're Pareto optimal for expectations under all of them. Make sense? Expression of the sum of the average of the? None. It's Pareto optimality. OK, yeah. Pareto optimality of the expectation of the other agent. Yeah. So you take expectations, take each agent's expected value, and then you're Pareto optimal over that. Make sense? OK, let's see if there's anything else in the notes. Bingo. Implicit subagents, so to speak. Bingo. Bingo. Indeed. It's very obvious. The more interesting claim is that we have this coherence argument, which you can view it as saying that any Pareto optimal system behaves as a market. Right. Right. The more interesting direction, yeah. Yeah. So markets are the most general such thing in some sense. Anything that has, what is the set, what is the condition under which something has this kind of Pareto optimality? You mean like existence of a Pareto optimal, or? Yeah, what is the coherence theorem in this case, or what is it? So what it's saying is that if I have adjusted my engine and my body and my paint so that they're all at settings where I cannot achieve a Pareto improvement, then my trade-offs between these, when I make small adjustments, all have the same ratios, or consistent ratios across the different knobs I could turn. So the ratios I achieve by adjusting the body knob a bit match the trade-off ratios I get by adjusting the engine knob a bit. Or in our bacteria example, we would say, well, it's optimizing to get each of these different metabolic resources, like glucose, sulfur, magnesium, and it's optimizing to get those across multiple different worlds. Maybe there's a world where they're all very abundant. Maybe there's a world where they're all very scarce. And what we expect to find is that if we tweak the expression level of genes by a little bit, then we'll find that they trade off in consistent ratios between all of these possibilities. Yes. So the composed of subagents here is, again, just that Pareto condition. Those consistent ratios, you can view those consistent ratios as capturing the trade-offs between subagents. So when we're Pareto optimal in this sense, it means we're also optimal under expected values of a bunch of subagents. And so you expect that the thing that will be selected for will be Pareto optimal because, as you say, if you make the wrong trade-off, you're just going to die. Something like that. And hence, you expect that what will be selected are things that behave like collection of subagents. That's as if each of those traits is a trader in a market. Exactly. And the behavior you get is like the way that a market is Pareto optimal over a kind of, those traits have their preferences over worlds or probability distribution over worlds as well as lots of utility functions. What I expected. All right. We're almost at two hours now, so I'm going to throw a few last comments on the coherent stuff, especially as it's relevant to humans and how it ties back to the bigger picture, and then we'll wrap up. For humans in particular, we have all these cute studies showing how humans are irrational in various ways. Two fun things about those. First of all, a lot of them don't replicate, as you'd expect in psychology. Second, among the findings which do replicate, many of them actually turn out to be just fine when we're in a subagents framework rather than just a pure utility maximization framework. Classic example of this is humans prefer to keep whatever they have in either direction. So if I have mushroom pizza, I will not trade it for your pepperoni pizza. But if I have pepperoni pizza, I will not trade it for your mushroom pizza. So there's a preference to keep something in either direction, even though like, yeah. A more interesting example of this would be taboo trade-offs, like things like trading money for lives. You can think of this as you have subagents. In the pizza case, you have a subagent that wants mushroom pizza. You have a different subagent that wants pepperoni pizza. And either trade would not be a Pareto improvement. So you just stick with whatever you have. In the more interesting case of taboo trade-offs, where you're talking about trades between money and lives, you have a subagent that cares about money. You have a subagent that cares about lives. And they're just not willing to trade, because it wouldn't be a Pareto improvement. It has to be a Pareto improvement in order for a trade to go through. Does that make sense? Would it be a kind of Pareto neutral? So it would be like, if it were on a Pareto frontier. So yeah, so at any given time, presumably, they're on a Pareto frontier. So if you could have a trade where you save strictly more lives and get strictly more dollars, then people are pretty OK with that. But you're not on the Pareto frontier, right? Yeah. Yeah. So what you're saying is you sort of re-derive that Kai Sotala style. Exactly. The right way of looking at it? Yeah. So the big conclusion here is, yeah, subagents are probably right. Like, this makes sense. We have first principles reason to expect that you have a bunch of subagents with separate preferences. And a lot of intuitive things about a lot of things that are sort of intuitively weird about our preferences through a utility maximizer lens make a lot more sense once you start thinking of it in terms of subagents. Yep. That's exactly the sort of thing. I can tell you something about the future of humanity. All right. Let's tie back to the big picture, and then we'll wrap up. So we did a little bit of this already. Just to recap, the big idea is we're looking for things which compress. We had this argument that, like, if you gain a lot of resources, then that lets you fill more of the space. It gives you a lot more degrees of freedom while still achieving the goal. And then we said, well, look, if you have lots of resources, then that means we expect to be Pareto optimal, because if you can just get more resources, you'll have more degrees of freedom. You'll be able to fill more of the space. And similarly with performing well in different worlds, if you're able to perform well in multiple different worlds, then you can perform more robustly across more distributions, right? So it's filling the space in a sense of robustness rather than breadth of the optimum. Didn't really talk about that comparison, but that's how that works. And that just, like, directly gives us this idea of subagents as a component of human values. Tying back to broader questions about human values, in terms of type signatures, this is talking about the outputs of human values. So we're saying that the outputs of our values are not like an expected utility, but a bunch of expected utilities, expected utilities for each subagent. You're saying when we look for human values, we should be looking for a model of it in terms of subagents. Bingo. And then next things, which we aren't going to do now, but next things would be considerations about world models, about Pointer's problem. This is more about inputs of human values. So we've talked about outputs. Obvious next question is, what things do we care about conceptually? All right. And then the whole abstraction and modularity section is about how to go look for those things. Oh. Also, like, the whole logical abstraction thing is a sort of talk you've already given at a bunch of potential places, like the topos. Sort of. I don't usually do the whole how it ties into the big picture bit. Yeah, exactly. The bit you need now is the bit that you rarely do. Yeah. Oh, even the abstraction section has more on that. OK. Anyway, any more questions before we break? I have one question about how important agreement. Obviously, the different agents have different utility functions. Is there a special case in which they all have the same probability distributions? Or are they completely free to vary? Like, what's interesting there? Yeah. So from the coherence arguments alone, there's no particular reason to expect them to have the same probabilities. The principle that both economics and Pareto optimality gives us is that, basically, if they have different probabilities, then they're going to trade so that one of them expects to be happy in the world it expects to be in, and the other one expects to be happy in the world that one expects to be in. Right. So there's some other dynamic property that you might expect them to come into agreement somewhat as observations come in or something. Potentially, but it's definitely not a requirement. And for the extent that humans have sub-agents, like, yeah, it's totally reasonable that some of them just have totally different beliefs, potentially over totally different parts of the world. They don't even have beliefs about the same things. That's my observation. Cool. OK. Yeah. All right. Thank you. Thank you, Joe. Thank you. Nice break. Thank you. Thank you. Thank you. Thank you. Thank you. 