So, where we left off, we had talked about underdetermined optimization. We had talked about a couple of ways in which the structure of optimization problems in general leads to things like compression and efficient use of resources and that sort of thing. Then we were mostly in the section on what humans want and this notion of selection theorems. So, we believe that the principles of behaving in optimal ways in general, so like the sorts of things we expect to evolve, imply certain structure. The big one we talked about yesterday was coherence arguments, implying Pareto optimality and this idea of Bayesian utility maximizing sub-agents. All right. That is the main piece which we currently know how to derive from something like a selection theorem. The next couple of pieces are going to be things where we don't yet have the full mathematical arguments, but we can make a pretty good guess as to what the shape of them is gonna be by looking at the things we do have, like coherence or information theoretic arguments and then cross-referencing that with how humans actually think in practice. Make sense? So, the two big pieces here are going to be world models and then latent variables. So, world models. In general, markers, where are they? There. In general, we're gonna be assuming from here on out a Bayesian viewpoint. We already saw, so the coherence arguments gave us one reason to expect that, like we expect Bayesian sub-agents from the coherence arguments, but they're somewhat underdetermined from the coherence arguments alone. Generally, coherence only really tells us about probabilities over actual observables or actual worlds. It doesn't really tell us much about the internal structure of models. We can get more of that from information theory, particular in order to compress efficiently, you have to be doing something Bayesian-ish, but we're not actually going to go through the math on that. Mainly because the math on that, I don't know of anyone who spelled it out particularly well yet. Although it is certainly something that should work in principle. The other big thing here is gonna be causality. So in general, we live in a world where cause and effect are things. And this has a big influence on what sort of world models we use in practice, what sort of world models work well in practice. So to start with, gonna just give a little bit of a primer on causal models, keep that pretty short. And then we'll talk about how to use causal models to model worlds that are bigger than us. Because like our brains themselves are embedded in the world. So necessarily we need to be modeling worlds that are bigger than our brains. So first things first, standard causal model. This is like the example you'd find on page one of Perl or whatever. Actually, no, that's not quite the example. This is the example. These are both good causal models. There we go. Is that it? No, it was this one, cool. Two causal models, sure. Let's say, yeah, this is it. So let's say we have a season. Season influences how likely it is to be raining and how likely it is that the sprinkler runs. So like in the summer, I turn on the sprinkler, in the winter, I don't. And then rain will depend more on your region, but whatever. And then these influence whether the sidewalk is wet, which in turn influences whether the sidewalk is slippery. So causal model, five variables. Main things to emphasize here. The arrows indicate like direct cause and effect. We also have a notion of indirect cause and effect. So like the season influences the chance that the sidewalk is wet, but it only does so indirectly via other things in the model like sprinkler or the rain. Symbolically, we could show that in a couple of different ways. I'll draw it like this. This is a cut through the graph. We call it a Markov blanket in this context. It says that things on this side only interact with things on this side via the variables it's cutting through. So in this case, the variable it's cutting through is this outgoing edge carrying the value of rain. So this edge says, is it raining or not? And this outgoing edge carrying the value of sprinkler. So this says, is the sprinkler on or not? So the mathematical statement that corresponds to this cut is that season is independent of wet and slippery given rain and sprinkler. Yes, exactly. And this doesn't have to be like a cut through what we'd call a time slice like this. We could also have a cut that goes vertical. Wouldn't be very interesting in this particular graph, but you could have something like that. You can have cuts that are like circular around a chunk of the graph. However you wanna do it, as long as you're cutting the graph into two pieces. Let's see, is there anything else we need to say about that? So physically, the way to think about causality for our purposes is mostly talking about locality. So this is about which things directly interact with each other things and what the structure of that is. So like, if I'm interacting with this chair, I'm not really directly interacting with it from here. There's like photons propagating between us. If I stomp, the waves will travel through the floor to the chair. And that's like, all that stuff is mediating the interaction between me and the chair. And I could imagine a Markov blanket, which is sort of like a shell of space time around the chair and all of my interactions with it are mediated by that Markov blanket. That's exactly the sort of thing we're gonna do. We're gonna be imagining as we go forward. That was causal models 101. Now for causal models one of two. The interesting question here is gonna be, how do we use these sorts of models to represent a world which is bigger than us? So I'll start with an example. We'll Python function. The, let's say, fact. That's an n equals zero. One. Turn n times fact of n minus one. All right. How would we represent this Python function as a causal model? What causal model would correspond to the Python function? Anyone wanna take a guess? A chain. Wanna try drawing it? How much are you about to do with arrows? Can you just have like input and arrow output? Good question. As a general rule, the key thing is that the arrows should be local. So they should have small number of inputs, small number of outputs. What we're actually going to do for something like a program is just have them do basic operations. So things like comparison or multiplication. All right. We're going to unpack it more than that. So it's not that fact one causes the value. Well, it is true in some sense that fact one causes the value of fact two causes the value of fact three, but we're gonna unpack more of the operations. So if I'm like just following through what happens in this program, the very first thing that happens is I have a value of n, goes n, I compare n to zero. So here's n. I say, is n equal to zero? If it is, then I return one. Let's say that's the result. If not, then I return n times fact n minus one. So that'll be n times fact n minus one. Something. Also take n, and then that'll be a return value. All right. Now, how do I get this? Well, I'll be taking an n minus one. Okay, where's the n minus one go? Let's add one more node for that. Okay. And then I'm gonna have basically a copy of this block. So this guy is gonna go there. I have n minus one here. I'll be comparing that to zero. Zero. I'll have a return value here. And then n minus one times like. Well, sorry, that's not right. Here, n minus two. Okay, did that get everything? And that arrow. There we go. And then I'm gonna have another copy of the same block. So we're sort of unpacking the whole thing as a circuit. And as a circuit, what's going on is we have a sequence of identical blocks where each block sort of feeds some stuff into the next block and then takes answers back and does a little bit of computation inside of it, right? And if we unroll this whole thing, it's going to be an infinite line, but in practice at some point, n is going to be equal to zero. So even though the whole circuit is infinite, it is going to like terminate, right? In the sense that we don't have to calculate the whole thing to get the result. And we get effectively an infinite circuit with a symmetry to it. The symmetry here would be, if we look at this whole thing, there is an exact copy of it here. So that's the symmetry. So the idea here is, what's that? It's a screen, a symmetric screen where you can take something out of the screen and then what's left is exactly the same thing. Take something out of the stream and what's left is exactly the same thing. Yes. Yeah, that's a reasonable way to think about it. The important point here is like, in general, we can write programs which are doing some standard computation, same way you would in any C-like programming language, set variable equal to function of other variables, so on and so forth. And then for simple operations, it's just basic circuits. For recursion, we have this sort of symmetry thing going on. And then that's all you really need to define most functions, right? You can go from there all the way to Turing completeness if you have the right data structures in your language. Make sense? So this is the basic trick that lets us take a causal model or a probabilistic causal model and use it to represent a world much larger than the model itself, right? So you could imagine like, this could be literally the world that you're thinking about, right? There's this infinite world, which is just consists of this giant factorial circuit. And we can represent it with something nice and compact like that. Does that make sense? But then this gets in, then sort of the natural next question is like, okay, so you can represent very large worlds efficiently in this way, but then how do you actually compute with them? Like, this is a model, I want to query it. I want to be like, well, I've observed something over here in the world. What does that tell me about the probabilities of stuff over here, right? So first of all, it presumably has to be a lazy data structure because we definitely don't want to compute everything. And in general, if we're using this to model the world rather than just a way to write functions in Python, these things don't necessarily terminate. They can just be infinite data structures and that's fine. So yeah, how do we do that? Basic idea here is what we're gonna talk about in the next section, abstraction. In general, abstraction lets you calculate small summaries from which you can calculate probabilities of things far away in the model. So that generally lets you get around dealing with very large models in principle. Make sense? All right, cool. Yeah. Two more things to cover here. I'm going through this section a little faster than originally planned because I want to do the next section in detail. In terms of like how we would imagine this actually being implemented in something like a human brain, most of this structure you could find is basically equivalent to something like predictive processing. Like that's how you would do it in a distributed way. It's pretty straightforward. Predictive processing basically tells you how to take any generative model like this or like this and turn it into a sort of distributed implementation. Make sense? Yeah, I'm not going to explain how that works, but yeah, that is conceptually, like if you want to go into how you would implement something like this, that's the direction to go. Then the other thing to mention is if we're using this to model the world, then a lot of our variables in here are not going to be things that we can directly observe in the world. So like if I'm thinking about my mental model of this room, my mental model includes things like that chair is made of wood. The wood is not hollow. There's an inside to the wood, even though I can't directly observe that. The wood is made of cells. At a lower level, it's made of atoms. None of that I directly observe, right? I just directly observe some photons coming from the wood. In probabilistic model terms, you can think of that as we have this great big causal model and like somewhere in there, there's a bunch of stuff that I observe. Maybe observe that, observe this, observe this guy, but like most of the stuff in this model is not stuff that I observe directly. Everything that I don't directly observe and can't directly observe in general is latent variables. And the key thing there is whenever a variable is latent, there's not necessarily any way for me to say that it exists in an objective sense. So for instance, let's say I have a clustering problem. Okay. And I'm like, okay, there's a cluster here and a cluster there. And I can talk about the shapes of these clusters and their positions. Okay. The high level cluster parameters, like the shapes and the positions of the clusters, these are latent variables. And in principle, there's not really an objective sense in which like these are the right ways to model it, right? So like the shape and position of this cluster is not a thing which exists in the world, is the thing which exists in my mind. And that's what we're talking about when we're talking about latent variables in these models. In fact, the causal structure for, if we just look at one cluster, would look like this, where these are all the points and this guy is the high level cluster stuff. Okay. So we would say, in this case, we're observing all of these and then this guy is latent. Yes. This question, like your last statement, that it's not really there in the world. Do you mean something like it's a generator of the thing that you see in the world? That's exactly what we're about to address. So the main reason I'm bringing up this example is because it's sort of the setup for the pointers problem. Claim inputs to human values are latent variables in human world models. And that's the pointers problem. All right. Anybody wanna tell me why this would be a problem? Go ahead. Can't observe them. In what sense? There we go. There you go. Good. So let's do an example. Pick some objects. What's that? That's the cute way to say it. Yes. All right. Let's do an example with some markers. So here I have four markers. You could imagine that these were the only four markers someone had ever seen in their life. And from their standpoint, they're like space of markers. Looks like one, two, three, four markers. All right. So in their mind, their notion of marker, maybe they have like a little cluster around that and they're like, ah, this is what marker is. And if this person comes along and says, I would like an orange marker, please. Then they're imagining that I'm going to generate an orange marker somewhere like there. All right. On the other hand, we could now imagine someone who had seen slightly more markers. Here we have slightly more markers. So they've seen these little Sharpies here. They've seen these big markers. Maybe they've seen some of the whiteboard markers over there. And now this person, they still have these four points in their marker space, but now they also have some points that are like over here. And maybe they have some markers over there. And if this person says, bring me an orange marker, they're hoping for something more like, I don't know, over here-ish maybe. And it's not really clear that either of these people is wrong, right? Like they're both, they both have a notion of marker, which is like, you know, valid in some sense. They both have a clear thing that they want and yet they're like disagreeing on what the thing is that I want an orange marker is asking for. And you can imagine this would be a much bigger issue if like you're building an AI and is confused about what you even mean by I would like an orange marker, please. That could get screwy real fast if it's working in a really different world model. Okay? So this is the basis of the pointers problem. And from a selection theorem standpoint, the big question is basically what sort of latent variable structures are selected for? If we know what kinds of latent variables are useful in whatever ways, then we can make predictions about what kinds of latent variables humans probably use and thereby make predictions about like what kinds of things we typically recognize as things in the world or what types of objects we typically recognize as types of objects, right? Let's see, other comments on that. One more thing on type signatures here, throwing a little bit more math on it. If we just think about a single sub-agent wants to maximize on X expectation of utility of let's say model.y given, I'll write it as do model.x equals X. This is like the standard utility maximization problem on a causal model where do model.x equals X is basically saying we go into the causal model and we make the thing we're calling X equal to this value. In terms of type signatures here, if Y is a latent variable in our model, then it's not clear how Y maps to the world. Like in terms of type, it is a thing which lives in the model and we're able to work with it because it's inside of this expectation symbol. We're taking an expectation over it. We don't really have a way to take it outside the expectation symbol. If we're just like, ah, what's Y? Go look in the world and tell me what Y is. That doesn't really work. There's just not necessarily anything in the world that it corresponds to. So for example, in the 17th century, people thought that a lot of illness was caused by miasma, right? Which didn't really map particularly perfectly to any particular thing in the physical world, right? They had this concept, this thing in their world model that didn't perfectly correspond to anything. Now that's an issue if you're like building AI and you're like, well, I want you to go get rid of the miasma, please. And the AI is like, what's a miasma? There's not actually anything in the world that corresponds to that. So there's just like not really a way for it to map the thing you want onto the thing in the world, right? And going through the world, we experience different things. We try to come up with explanations of these things. Yep. With these causal models, we cluster things together. One example of coming up with an explanation. And we kind of somehow have a sense of this is the way things should be. Yep. In terms of the pattern, stories, explanations, clusters of sound in the world. This cluster should exist or should be more or should be changed more like that. And we take actions that can, the world such that the thing we experience is more like the way we think the abstractions should be. But the abstractions are fundamentally not real. They're fundamentally an explanation. Right. They're also not exactly unreal, but they're not, they don't have the status of a thing in the world. And therefore, we can't just go ask something else to change them separate from the thing in our heads. Right. Any obvious operationalization of this thing is probably going to go through the concept in our heads. And then you risk the AI, like manipulating the thing in your head rather than manipulating the thing in the world. We can use tools. We might build a bulldozer or we might build the internet. Or we might even build a clever market regulation system or a high frequency trading system. Or we can build these complicated and kind of intelligent systems. Yep. But now we're trying to do that without having to be involved with them. So we can do that autonomously. Yep. Without getting some kind of direction or some kind of correction from us. That's where it's hard. Yep. Bingo. Yes. But why is it that we can take an expectation of this and we can't just find out what it is and then use it? Great question. Let's use the clustering example. That'll work. So if we have a probabilistic clustering model, part of what goes into that model is we have a prior P of mu sigma. Right? And then we also have in the model something like for each I, so like for each data point XI, we're going to be taking a product on those. What's the probability that we observe XI given this particular mu and sigma? Like in this case, I'm just sort of ignoring the other cluster, but whatever. So the only part of this, which is like in the world, so to speak, are the XIs themselves. The rest of this is all stuff that lives in the model, the mus and the sigmas. And in particular, the prior and actually the shape of the model itself. So this factorization. All of that is stuff that's essentially prior information. Like we talk about the prior distribution, but the shape of the model itself is also really part of your prior information. So when you take this expectation, that's all stuff that you're effectively summing over. And that's sort of the magic juice that lets you take the expectation over a thing that doesn't live in the world. Is you have all this sort of model that you were implicitly or explicitly assuming from the start. Does that make sense? Exactly. Cool. All right. And so this all motivates yet another thing that motivates the next section on abstraction. Before we get to that, I'm just gonna like throw out a quick like bullet list of things that we're not talking about more in the selection theorems department. So other things. Some of these are more important. Some of these are less important. The common theme on them is that I'm not the right person to talk about them. So top of the list, most important things probably are decision theory and counterfactuals. So here's a question I don't know the answer to. What decision theory does evolution select for? That really seems like a question we should know the answer to by now, doesn't it? But I don't know it. Also in this department is just sort of general principles for linking actions to world models. Or I guess Flint would call it the knowledge problem or part of the knowledge problem. This I think of as a subset of the decision theory and counterfactual problems. Like if part of figuring out what decision theory evolution selects for is figuring out like how the actions that something can take are going to be wired into its decision-making process. So that I would say is probably the biggest thing that we're not gonna be talking about much. Other than that, another big one is logical uncertainty. Yeah, you guys probably have heard stuff about that before. And then neurological bases, that's an interesting one. We briefly mentioned predictive processing, but like in general, tying this all to, yeah, neurological, tying this all to like the actual physical mechanisms in the brain is just sort of wide open territory, very useful. Like that would be great stuff to have in general. Similarly, there's a lot of pieces where we don't yet have selection theorems. We don't have selection theorems for the parts we need in this. We don't have selection theorems for like causal world models in particular. We don't have selection theorems yet for abstractions. Like really the, sure. Like really the only part we do have selection theorems for at this point is coherence. So we really like building out all that is important. The selection theorem is an explanation about the way that, in this case, the way that agents work in terms of, in terms of the algorithm goes in the long run, all the agents that survive will look like this. So the agents that dominate the marketplace of evolutionary landscape, the agents that appear in the long run look like this. Yep, exactly. The agents that are in the world look like this. Yep. Other than that, just like more generally, knowing what else to ask is always an important one. We don't necessarily know that, we don't know that we know all the things here. There are probably unknown, unknown still. Welcome to alignment. Yeah. Cool. Yeah. Yep. Yep. Yeah. Yeah, I mean, I think that's straightforwardly a mistake on their part, but like there are still useful things there that like are, some of them are close enough that it would probably produce a selection theorem if you tried. Like Abram's thing on Dutch booking agents where the CDT isn't equal to EDT is one that you could probably turn into a selection theorem pretty easily. All right, so I kind of rushed through that last bit to get to abstractions. Any questions before we move on? Cool. All right. All right. This section is stuff we've talked about a little bit before, but I wanna flesh out the math a bit more and also talk more about how it ties into the bigger picture. So first things first. Reminder for why we're talking about abstraction. First of all, there's the pointers problem. We just covered that. More generally, we want to be able to measure all this human value stuff. Like we talked a bunch about type signatures. We talked a bunch about like what kind of thing we're looking for, but we still need the tools to go out and look for it. So the question is, how do we go measure any of these parts of an agent or things an agent cares about or any of that? And as a sort of corollary of that, if we can go measure these things, then we can get feedback signals on them. Like once you can go start to measure, like is there an agent here at all, or measure values, measure utilities, measure probabilities within the system in which they're embedded, then we can start to do empirical work on the agency theory, right? We can start to go directly check how all this very abstract stuff actually plays out. Make sense? So a big part of the goal here is just generally being able to go from all this crazy theory stuff to actually doing science. That's the happy world. So general version of the problem, what are abstract objects? Why do we recognize the things we do as objects? For instance, we have this coconut cup. Why do we recognize this coconut cup as a natural thing to think of as an object rather than thinking of, say, this half of the coconut cup as an object and this half of the coconut cup as a separate object? Or why do we think of this coconut cup as an object rather than this coconut cup plus that chair plus this particular floorboard as an object? What makes this thing more a natural object than those? Interesting piece of evidence here. Whatever principle it is that we're using to recognize these things as objects, it can't be that complex in some sense because when you have a baby and the baby's learning the concept of what an apple is, how many examples does it take? Like maybe two, three tops, right? It takes very few examples for a baby to learn to recognize apples. And yet, if you think about, let's say we're building an apple classifier. I feed it an image. It tells me, is there an apple here? Well, the space of functions which take in a megabyte image and spit out a bool, there's something like two to the two to a million possible functions that do that, right? Like just absurdly huge space. There is no way in hell that you're going to learn to recognize apples by brute force given two or three examples. Brute force figuring that out would require like two to a million examples or some ridiculous number like that, right? So clearly, there's something going on here which is baked in. It's baked into the way we think and it requires very few examples. So like most of the work of recognizing what an apple is, the baby must already be doing before it hears the word, right? And then the key question is, well, how is it doing that? So we're gonna go through three different mathematical views here with different intuitions underlying them that end up all pointing in the same place. The first one is gonna be about information relevant far away. So this is the idea that my concept of this coconut cup is basically a summary of all the information about this cup, which is relevant in lots of other places far off in the world. Second concept we'll go through is that my concept of anything, concept of the coconut cup, for instance, is a summary of information which is redundant. So for instance, we have the coconut cup now, we have the coconut cup now, we have the coconut cup now, we have the coconut cup three hours ago. We have the image of the coconut cup, which is hitting Flint's retina. We have probably, probably similar coconut cups available for sale online. All of this is information that's redundant in the sense that like, if I hide the coconut cup by my back right now, you can make a pretty good guess as to what's there. You can reconstruct it with the information you have about the coconut cup earlier and the coconut cup later, right? Object permanence, it's a thing. So that's gonna be the second approach. The third approach we'll take is going to be to think about sort of the universality of abstractions. Like the simple fact that we're able to learn similar abstractions suggests that there's some sort of universality principle going on here. And it turns out that that basically narrows it down to one possible way of doing it, okay? Whew. So, we'll start off with a causal model and we're gonna assume that this is some ridiculously large causal model. I'll draw a bunch of arrows and then some dot, dot, dots. Okay. So, to talk about information far away, we're going to look at sequences of Markov blankets in this causal graph. So maybe here's one sequence. Okay. We'll call this one M1, M2, M3, M4, dot, dot, dot. And just assume that this model goes on for a while. Okay. So, we're talking about information far away. The way we're operationalizing that, we're not necessarily talking about far away in like space time. We're talking about far away in this causal graph. So, like there are many causal intermediates between me and the thing. And we're representing that using these sequential layers of Markov blankets. We're saying like, look, between me and Rob, I can carve out lots of layers of air or layers of floor or whatever. And if anything wants to propagate from me to him, it's going to have to go through all those layers. And then the key theorem here is the telephone theorem. So, we're going to imagine that there's a message passing from me to Rob through each of these layers of air. There is in fact, right now, I know it's so difficult to imagine, but we're going to imagine that the layer of air is perhaps not quite so good at passing messages as we usually imagine. So, instead we're going to replace the layers of air with people. Hey, you two, come here. This would work better with more people, but the actual way it's going to work is if I'm going to give my message to Adam, then Adam's going to turn around and give it to Flint. And then ideally we'd have a very long line of people here. And then the last person in line would pass the message to Rob, right? This is the game of telephone. Maybe you played this in elementary school. As you pass the message along, it gets more and more garbled, right? I actually want to try this now with four people and see how garbled it gets, come here. Dinner's in an hour and a half. How much can I drink? Hmm? How much can I drink? Go ahead, pass it on. All right, what message did you hear? I got dinner is in the hours. Huh, I said dinner is in an hour and a half. Yeah, the funny thing is the air itself is too good. Too good. Maybe some trolls in the middle here. Yeah, yeah, all right, go sit down. So the important point here is that there's sort of a duality when you're playing the game of telephone, which is any information is either going to be perfectly conserved or it's going to be completely lost, right? So like, there are ways we could make the message perfectly conserved even though our hearing is imperfect. Like we could just repeat ourselves enough times that the damn message gets through at each single step, right? But absent that, if there's any loss as we pass the message through enough people, it's just going to be hopelessly garbled. And that's exactly what the telephone theorem says. Basically, we think about how much information each of the messages gives us about the original in an information theoretic sense. So we're looking at mutual information and this has to go down. Eventually it will approach a limit because mutual information is non-negative. So it has to approach a limit at some point. And once it's arbitrarily close to that limit, that means the information is arbitrarily perfectly conserved from step to step. So the English language version of this is the only information conserved over a long distance in our graph is exactly the information which in the limit is perfectly conserved. So we may lose some of it at first, but eventually we have to stop losing and whatever's left has to be conserved. Make sense? It can be zero. It can absolutely be zero. Oh yeah, that is correct. It's longer than zero, I feel like. Yes. It is often zero. Yeah, it's bounded below. So you know that it's going to be, it's decreasing. So you know it's going to converge. No such thing as negative. Yeah, you know it's going to converge, but you don't know where. All right, so that's the telephone theorem. A few problems with the telephone theorem. First of all, these messages are just very high dimensional. These Markov blankets, so just generally hard to work with, you know, algorithmically or whatever. Second, exactly which information is conserved depends on which sequence of Markov blankets we take. So if I'm thinking about information propagating from the stove to the fridge, that's going to be potentially very different from information propagating from me to Rob and different conserved quantities will apply. Oh, I should explicitly write the formula for perfect conservation. Okay. So these are the perfectly conserved quantities which carry our information in the limit. So they are just straightforward functions of each Markov blanket. Okay. So the way we can get around this issue with like having different Markov blankets and also just get a much more elegant model in general is to think about natural abstractions instead in terms of redundancy. So part of the thing here is if the information is in each of the messages in the limits, like it's in mn, it's in mn plus one, it's in mn plus two, that means it's extremely redundant, right? There's lots of separate sets of variables in the model and we can back out the information exactly from any of those variables. So if we just like forget the values of some variables and then regenerate, then we must get the same value back because like we can pull it out of that constraint. Like if I forget mn plus one, that's fine. I can get the relevant information back by looking at mn. And this is exactly the same thing we were talking about with this coconut cup, where like if I hide it behind my back, you still know what it looks like because object permanence and all that, right? The information that's conserved here is exactly like whatever you can figure out about the mug behind my back from having looked at it before. So to formalize that, we'll take same causal graph as before, which is not actually gonna be the same because I'm not taking the time to copy it that carefully, but you get the idea. And we're just gonna imagine a process where we forget the value of one variable and then we resample it conditioned on all of its neighbors. So this would be equivalent to like taking the coconut cup and copying it, but we're not gonna do that. We're just gonna copy it. So this would be equivalent to like taking the coconut mug and deleting coconut mug at right now from my model and then resampling what I think the coconut mug was like based on what I know of it from before and what I know of it after. Make sense? And then we're gonna repeat this. We're gonna do it over and over again, resampling all of the different variables. Sometimes we'll double resample over and over again until this whole process converges. If you've done Markov chain Monte Carlo before, this is exactly the standard method. Fun fact, as the model gets large, it no longer converges to just like a trivial stationary distribution. If the model is large or if we have like conserved quantities like this, these two go together, then these sorts of quantities will be conserved by the resampling process. In fact, that's all that will be conserved by the resampling process. So once we take this process to convergence, we resample for long enough, everything will sort of smooth out to our prior distribution except that we'll have these guys left over. Okay? What you're saying is there's like crazy causal structure in the world. And information probably in this crazy way comes information at last. It's very difficult to take a track of what's there, but there's something that's not lost. There's something that's not lost and it's not, so it's sort of like not ever lost. There's like a qualitative difference compared to the things that like fritter away. Yep. And it's not lost. There's two different processes in which it's not lost. One is just like kind of, you could think of it as moving through space. Yep. Or through time. Of course, there's space-time in general. Yep. Movement. I guess it's always through time, right? It could be time or space or both. Either way. And then there's this other process which is like a resample. Yep. Resampling. These are two processes through which the thing that's not lost is in fact not lost. Yep, exactly. So what you end up with at the end of this, you still have the same graph structure. So you haven't, you've simplified what's in the data structure, but you haven't simplified the data structure, right? Yes. So let me write out a bit of what's in the data structure. So we'll say this original graph, the distribution was p of x, and then that factors according to the graph structure. Okay. Everybody good with that? I didn't actually talk about factorization earlier. Okay. When you have a probability distribution on a causal DAG, the key fact about the probability distribution itself is that the distribution factors we say it factors over the graph, meaning that we can write it in terms of the probability of each thing given its parents in the graph. So like this thing given its parent, and then we take a giant product of all those, and that's the overall joint distribution. All right. So down here, we've now introduced a couple of different x's. We have some, I'll call it, call it x is just like the same original thing. It was like whatever actual stuff is out there. And then we have what I'll call x infinity, which is a totally different set of values generated by running this resampling process with x as our initial condition. Okay. And the key thing here, if I look at the distribution of x given x infinity, this guy is still going to have all of the information about conserved quantities, but it's going to have nothing else, which means this distribution, within this distribution, mutual information is always going to go to zero. This distribution is always going to go to zero over a long distance. This distribution right here. Hang on. Let me, let me spell that out a bit more. So, hold on, hold on. This distribution, first of all, it turns out to factor on the same graph. So p of x given x infinity will be equal to product on i p of x i given x parents of i and x infinity. So, all the graph structure is still conserved. And now, when we take our layers of Markov blankets through that graph, what we're going to use, rather than using this distribution, the unconditional distribution, we're using the distribution conditional on x infinity. So we're effectively conditioning on these conserved quantities. And given those conserved quantities, our mutual information graph will always look like this. It's always going to drop down arbitrarily close to zero in the limit. Exactly. Yep. Exactly. So, conceptually, a useful way to think about this is you can sort of think of this x infinity as capturing the abstract information from x. So there's sort of this high level information in x infinity, like all these values, and condition on that high level information, all interactions within this model are short range. That high level information summarizes all of the information that propagates over long distances anywhere in the model. Make sense? Now... Yes, that's a great way to think of it. Also a good way to think of it. It is like high frequency in the sense of distance in the graph, which is often physically spatial, in fact. This question is like, if we really look, will we find any signal? Like, is anything really there? What do you mean? I mean, if you really look carefully, we're going to find some signal with some noise, right? Yep. We're going to maybe find out is the signal, is there more than zero signal? Really? Like really, when we get the real, for real, is there anything like that? It's surprisingly good at predicting things far away. I'm still a little confused about what x infinity looks like. Like, I have a sense of like what, that like this, this causal graph represents the world, as it is. And x infinity is like very, it's the exact same shape of thing. And so it feels like it should be a world in a way. It's like, what is that world? So that was, gets at something I was about to bring up next, which is, this isn't a very efficient representation. This x infinity, it's like a value for every single variable in the world. That's like the part of the point of this is that the only information we really need is these conserved quantities. So we should be able to collapse it down. Yes. So in particular, what we'd expect is p of x given x infinity is equal to 1 over z e to the lambda transpose sum on i, fi, xi, x parents of i. Lambda is a function of x infinity. Times probability of x given a fixed value of x infinity. I call that x star. All right, let's talk about what that means real quick. So where this is coming from is the Koopman-Pittman-Darmois theorem, the KPD. The general idea of the KPD theorem is anytime you have a summary statistic that's much smaller than the thing it's summarizing. So you can take all of the information in x that's relevant to x infinity and summarize it by these much lower dimensional constraints. Anytime you have that, it gives you this factorization, basically. So it's saying that we have this exponential form and essentially all of the abstract information is in these sums right here. We're not actually going to use this for anything in particular. I'm putting it up here mainly so that you know that this whole thing can sort of be translated into a particular way of factorizing the distribution. Adam. It's a very superficial math question. Yeah. The form of that looks like a Boltzmann model, like a model with latent variable. Yep. Is it not a coincidence? I mean, it's an exponential form. Like tons and tons of things are exponential forms for broadly similar reasons. Is there any summary statistic that can be represented as an exponential form? Yep. Roughly speaking. There's some terms and conditions there, but they're pretty loose. Does the summary statistic look like all the points lie on a circumference of a circle or something like that? Mm-hmm. I guess that wouldn't satisfy the condition. Well, either it won't have a small summary statistic, which is often what happens, or if it does have a small summary statistic, we'll be able to factor it like this. For something on a circle, the like make it a circle part would mostly be done by the second term. Like the second term. What the second term is saying is there's some sort of like base distribution that is just like the distribution for an arbitrary value of x infinity, and then everything else is like an adjustment of that. Oh, okay. Yeah. That's what would restrict it to a circle probably. What's the importance of this? I know you said you're not going to do it. Yeah. In general, what's that? What's the importance of this? So mostly this is important if you want to like actually start doing math with this thing. So like I said, this x infinity, very high dimensional, not convenient to work with. If you want to be writing algorithms to compute these quantities, then this representation gives you something much nicer. It's not particularly high dimensional to find these functions. The sum itself is just a sum. So it's generally much more convenient to work with. Yeah. Yeah. Yep. Yep. Exactly. Another way to think of it is, it's not quite this exact expression, but you can use it to get a local global factorization. So you can just like directly factor apart the local interactions and the large scale abstract stuff. All right. So that was two of our three points of view. Are we ready for number three? Any more questions on these guys? Okay. Number three starts out very different. Number three says I have two random variables, A and B, and I have their distribution, P of AB. And we'll assume that the distribution is not, they're not independent. And we want to figure out some latent variable C, which explains the dependence between them. So we want to have P of ABC equal to P of C times P of A given C times P of B given C. That's the factorization corresponding to this graph. And we also want someone see P of ABC equal to our original P of AB. So we're trying to discover a latent variable. This is latent variable discovery. We're backing out a latent variable that explains what's going on in the world. Make sense? And the question is, is there some most canonical latent variable? Some latent variable that is like the minimum latent variable or the latent variable, which like has to be there. Something along those lines. Something that we'd expect anything that explains the relationship between these two variables, anything that explains that relationship has to have something like C sort of baked into it. So part of the intuition here is if you're thinking about, say, the step from classical physics to quantum mechanics, we knew that all the stuff that worked in classical physics still had to work when we got to quantum mechanics. Because of that, we expected that a lot of the structure still had to be there. There still had to be some way to take quantum mechanics and back out from it all that classical structure that we had before. We have the classical limit. Exactly. And that's what we're imagining here is we want some C that it's the most fundamental thing of the things we've observed so far. Any new thing that we might discover had better still reduce to that in the appropriate situations. So the way we're going to formulate that is we want C star such that C star does the thing we want. And for any C, which does the thing we want, C, A, B, we have C, A, B, C star. This is kind of a weird one at first glance. Intuitively, what it's saying is that any other C we could pick that explains the relationship between these two has to also contain whatever information is in C star that's relevant to those two. It looks like a universal property. Yes. It is very much a category theory flavored thing. Is this on the left hand side, A and C on the bottom left? Does that be A and B? Sorry, where? Right here. Oh, that should be B. Thank you. There we go. Good call. All right. So the result here is in general, this doesn't exist. In general, there is no such C star. The one situation in which it does exist is when? There is stuff. There is stuff, Alex. U exists. B stuff. The one situation where it does exist is when we have some FA of A equal to FB of B with probability 1. So exactly the same sort of condition before. We have some function we can compute over some of the variables, which is equal to some function over the other variables with probability 1. That is exactly the condition in which we can have this sort of universal thing. And then we just have C star equal to those guys. In that case, don't you need something stronger, like everything that could be in any such F? If you had such a F. Yes. You do have to take the most general F. That is a constraint here. You could have a function like F, which is always your time 0. Yes. And it's like trivially. Yeah. And again, this isn't always possible for all A's and B's. Basically, all of the information that's in common between A and B has to be carried by these F's in order for it to work. That's important. If it was a kind of universal, like an actual universal property or something like that, the last graph would be C, go through C star to influence A and B. Yes. Why? Which is fine. I think we can actually just draw it that way. It's still a graph though. I think that ends up being equivalent. Yeah, that does end up being equivalent. Why? With causal graphs in general, the small ones you can reorder in various ways. For instance, you have x, y, z is equivalent to z, y, x. It's also equivalent to, let's see if I can get the direction right. I think it's y, z, and on the other side, x. In generality, you can just mirror a closer graph? This is definitely not a thing you can do with causal graphs in general. It's just the small ones just have some accidental properties, basically. These guys sort of accidentally end up equal to each other. In the real world, these have reverse causality just like that? Yes. You definitely, in general, as the graph gets bigger, a lot more of the structure is locked in. So I thought there was something interesting there about the theorem. No, no. So this is actually a slightly older version of this theorem. I'm going to now state a stronger one. So going back to our thing about redundancy, a lot of this redundancy stuff didn't really necessarily need to use the causal structure. The part about far-awayness did. In order to say that variables far away are independent, conditional on x infinity, we need to have this causal structure to talk about far-awayness. But this process of dropping and resampling variables in order to keep around the redundant information, that we can do even without a causal structure. That we can do in any distribution. So an interesting question. Is there some sort of property like this, which we can get out of that resampling process in general? And I'm pretty sure there is. The claim is, so we have some probability distribution on a bunch of x's. And then we're going to do our resampling process. Resampling gives us an x infinity, like before. And now I claim that if we have some g of x, which is finite, that's important. If it has infinite entropy, then we're not going to be able to use this decreasing mutual information argument. So we need it to have finite entropy. And I'm going to talk about the mutual information between g of x and x's greater than some k, given x infinity. So we'll take the first k of the x's, forget about those, and then look at mutual information between g and the rest of them. And g takes all of the x's? Yep. The important thing is just it's not, so it can't just spit all of the x's back, because it has to be small. So we're going to take limit as n goes to infinity. So the sort of argument that we're going to make is we're going to look at the mutual information between these two guys, and it's going to be going down. Right? We need this mutual information to be finite, because otherwise it could just keep going down, keep going down indefinitely without reaching a limit. And the easy way to force your mutual information to be finite initially is to say that this guy just has some finite entropy. And the claim is that given x infinity, for any g of x we choose, this is going to go to 0. All the x's, mutual information between g of x, and all of the x's are after xk. Yep. Conditioned on x infinity, x infinity being the distribution that we get, or the value that we get after the sampling. Yep. Strange thing to think about. Let's try an example. There you go. You got it. So the idea here is basically any g of x is basically any function of the x's that we can dream up, as long as it has limited entropy, is going to only have any mutual information with a finite number of the x's. Like it can't have lots of mutual information with all of the x's, because if it did, then that information would be very redundant, and it would be captured in x infinity. So once we condition on x infinity, things can only have mutual information with a limited number of the variables. Make sense? So once you've passed the limited number of variables for which it has redundant information, then you'd have a 0 mutual information or something like that? Arbitrarily small, but yeah. So it goes to 0 in the limit. You should measure between g of x. For any function g of x, for any function whatsoever, you're saying basically for any function whatsoever, it's going to be captured by x infinity as some finite number of x's. Bingo. In fact, not only can we do this for any function whatsoever, we could also reorder these x's, so you can take any sequence you want. So if g of x was, OK, so a simple case would be if it returns x1, then obviously this is true. You could have it return what? Return x100. Could have it return the average of all the x's. Could return the average, right? And the claim would be that the average is going to, well, certainly you're going to get information about the average quickly, except that you're not even going to need x infinity. So some combination of either the average will be conserved by the resampling process, in which case you can recover it from x infinity, or you can get the average from the first however many samples. You can get an arbitrarily good estimate of it. No, I think it has to, I think for this to be true, it's not, it has to be that the average is captured by x infinity, which I guess it would be. Yes, I expect it would be. Because you're saying that the mutual information goes to 0 for after you chop off the first case. Bingo. It's not to get the information from the first case. It's carved in the first case. Oh, right. Yeah, sorry, that was backwards. You're right. Yeah, I had the same thing. So it's sort of a statistic, which must include average and a bunch of other things that you could do with it. But it's a weird thing. Like, you compute the average of all the prime numbers, the x's, and so on. That would be weird. I'm pretty sure it would still work. You just took the average of all the odd x's, and then mark up the joint distribution such that the odds are a little different from the even ones. So it's going to be odd 1, 0, 0, and then even 0, 1. Yep. Then the claim is the x infinity is going to contain that information. Yep. I'm thinking about what you took the parity of all the x's, but that's a super addition to the infinite number. Parity is actually a really fun one. Because if you have, like, parity of n coin flips, as soon as you forget the first one, you've forgotten the parity, and we're done. Your mutual information has already gone to 0 as soon as we drop x1. Yep. That would not be an x infinity at all. That information is extremely not redundant. You need literally every single variable to reconstruct it. So the main conceptual takeaway from this is you can imagine that the property we want is that we have some x infinity contains the abstract information. And given this x infinity, lots of things in the model are just independent of each other so that we don't have to account for lots of interactions. And this gives us a version of that. It's, like, would not be anybody's first pick. Would love it if it were simpler. But this seems to be, like, the strongest thing we can reasonably get that lets us do something like that. All right. Wow, you're really jumping ahead here. Good job. That was actually the next item on my list was more evidence. Science works. Yep, that's exactly right. So the broader question here is, OK, we have these, like, three different sort of intuitions leading to different math, which all converge on the same place. In general, that's a pretty damn good sign that you're headed in the right direction. But we'd still like more empirical evidence. So two key pieces of empirical evidence here. First one is science works. In general, when we go into the lab and do experiments, we usually find that you need to control for, like, three things, 10 things, a few dozen things in order to get reproducible results. As a general rule, you don't need to control for billions of things to get reproducible results. You don't need to control for the position of every atom in the universe in order to get reproducible results in your experiment. And that's one of the key things that all of these are saying. This is saying, look, if you're looking at the influence of things far away, only, like, a small amount of that information is going to be relevant. Most of it's going to be wiped out by noise. It's not going to impact the reproducibility of your experiment. Yes, exactly. And, like, similar with this guy. And then this guy is the more abstract, more general version. We don't even need the causal structure. Say, look, in general, if G is your experiment, and you're controlling for the abstract stuff, then, like, you just don't need to control for that many things. Make sense? So, yeah. Science works. That's a pretty good indicator that something like all this has to be true. Then the other interesting piece of evidence is the phrase, words point to clusters in think space. Was it? Surely not. It is a blue cascade, yeah. Let's go back to the cluster picture. So let's think about doing resampling with this cluster stuff, right? So, like, I drop this point. That point's gone now. And then I'm going to resample it. So when I resample it, I don't know exactly where it was. But I know the overall cluster statistics. So I know I'm going to be sampling from something roughly shaped like that cluster, right? Then I do that again with another point. So this one's gone. Get rid of this point. And I resample it. I don't know exactly where it is, but it's going to be somewhere in this cluster, right? And I do this over and over again, losing points, resampling them. And you can see I'm losing information about the individual points. But the overall shape of the cluster is conserved. Another way to put this would be if we look at this causal model. For purposes of the telephone theorem, it turns out we don't even need infinitely many Markov blankets in this case. All of these guys are independent given that information right there. So the summary information about the cluster is exactly what's conserved under resampling. It's exactly the thing that induces independence between all of these points, right? So the cluster doesn't spread out at all in this resampling process. There may be a small amount of drift. But in general, if the number of points is large, then the drift is going to be arbitrarily small. So the natural abstraction here is like the cluster itself. It's not the individual things. It's not like the individual trees. It's the concept of tree, right? So if we say words point to clusters in think space, well, gee, that sounds an awful lot like words pointing to natural abstractions in exactly the sense we're talking about here. Like clusters in think space are just directly examples of natural abstractions. Exactly. These are the latent variables. Those are what the words are pointing to. Your concept of tree, to the extent that the clustering model works, that's what it is. More generally, I don't think that clustering is a perfect analogy for what we do. I think this is what we do. All right. Any questions about those? We're almost done now. The last thing I'm going to talk about is just what all that gives us. All right. So we have these cool theorems about natural abstraction. What does it give us? First things first. When we have, there we go. When we have these sorts of world models, these world models represented as lazy data structures, essentially, they directly give us ways to make predictions about stuff far apart in the world model without having to calculate everything. So that's piece one. Piece two. They tell us how to look for things in general. What do we mean by things in general? So right now, we don't really have a good way of measuring chairs without a human in the loop. If you just have a camera, you point it at the chair, you need some ML system to figure out what the chair is, make some measurements of it, and that's going to involve a lot of ad hoc crap. We don't have a good way to boil down exactly what is the concept of chair, such that you pull out a new sensor, and the thing can immediately connect it to the concept of chair. So we can start to talk about detecting things in the world in an extremely robust way that hopefully directly corresponds to how humans think of things in the world. And in particular, the things in the world include agents. So we can start to talk about directly measuring agents in the world in the same way that humans would think of this, or humans in the world in the same way that humans would think of humans in the world, or world models in a human's brain, or values, any of those. And then the last big reason this is useful, the original reason we got into it. Yeah, go ahead. Yes. Yep. Yep. And that is indeed the last reason on my list that we care about this, is that we expect that the things humans care about, the inputs to our values, are exactly these sorts of things. They're natural abstractions. So tie that all back to yesterday. OK. We had a bunch of stuff about type signatures. We kind of rushed through a bunch of these. But we have this idea that, in particular, that human values are functions of humans' latent variables. Those are hopefully natural abstractions. And more generally, we have this idea that natural abstractions fit really well with the types of world models we were talking about that can represent very large worlds with a lazy data structure. Now, what we'd really like would be to go find selection theorems that tell us that, indeed, evolved things will tend to make use of these natural abstractions in the ways that we've predicted here. We would ideally like selection theorems that would tell us that the values or the goals of whatever is popping out of evolution should be functions of latent variables in the models. And we'd like to show that the models themselves will have latent variables that are natural abstractions. Make sense? That would be the ideal outcome of all this stuff about selection theorems. And then, of course, we can also go measure it, so we have good abstraction tools. All right, that's it. Questions? How would you find these natural abstractions given a simulated world? Yeah, so first, there's kind of a funny answer to it, given that I don't have good algorithms. I actually have OK algorithms. I don't have algorithms for this that I'm really happy with yet. But it's intuitively hilariously easy, because it's exactly the stuff that's super redundant. Like, the whole point is that there's a gazillion ways to find it, right? It's represented all over the place. You just go look for the information that's super redundant. Like, conceptually, that's all you need. Actually, coming up with algorithms is where things like this formula here come in. Like, that's exactly one of the main use cases for this factorization. It's like figuring out good algorithms for, like, taking a simulation of a world and backing out the natural abstractions from it. So just one way you could do it would be that a simulation that goes through time would do this. For example, you could do this. Right. Yeah, in principle. That's, like, exactly what I do analytically. But algorithmically, that would be wildly inefficient. Right. Yeah. So this is not something I've gone super deep into yet. But ironically, the cases, well, maybe not even ironically, the cases that are hard for building causal nets directly from observations of the world are exactly the cases where abstraction is relevant. Like, when you have a lot of variables that are, like, moving independently or, like, moving relatively independently, there's not a lot of strong constraints between them. That's when it's pretty easy to build out a causal graph. The hard cases are where you have these sorts of things going on. And then because they're, like, moving in lockstep, it's hard to untangle the causality. Make sense? Yeah. I mean, yeah. Finite vector sets is extremely similar to, like, the methods we use for learning causal structure in general. It's, yeah. Correct. Maybe in physics. Yep. No, this is still not, like, the thing that's not born and never dies. Yeah. Yeah. Yep. Yes. As the saying says, the wheel of science turns, but it doesn't turn backwards. Like, when we come up with new theories, the old theories are still just as correct as they were before. So, like, to the extent that they worked well, they do have to carry over into the new theories. GR had to be consistent with Newtonian gravity. Quantum had to be consistent with classical, all that jazz. Yeah. Yeah, that's a good way to think of it. Yes. Mm-hmm. Yeah, so an important point to remember here is there's a difference between the abstraction and your model of the abstraction. Think of a tree genome. Like, that's a natural abstraction. It's, like, redundant information across lots of trees, can do the resampling thing, but you don't know it. So, like, yeah, very straightforward example. Like, you know the abstraction is there, you have a pointer to it in your model, but you don't actually know its value. Yep. Yeah. Yep. Yep. Mm-hmm. Yeah. Yep, that is exactly the sort of theorem. Like, that would be a great selection theorem on its own, or a great class of selection theorems, even. So, just, like, right off the bat, one very simple thing you can say is, in general, if you're inside the model, then the natural abstractions are the only information you have about anything far away, because they're the only information which prop gets to you, right? So that's, like, the absolute simplest version. It's information which reaches your sensors. That doesn't mean you're necessarily parsing it very well. Not quite, because, remember, our theorems don't say that it's available everywhere, just that it's available in lots of places. In the limit, it's available in infinitely many places. It could still be in lots of places, and none of them are near you. Not necessarily. Nope. Like, you could even be very far. So, like, an example here would be, on the planet Earth, we have trees. It may be that there's just nowhere else in the universe that has trees like here, and that's still extremely redundant information. Like, you'd have to delete and resample the whole damn Earth in order to get rid of it. It's not like the thing is not x infinity, that it's, like, the trees, to the extent that they're specific to the Earth. Yep. They're, like, not in g of x. It's the thing about them that's not specific. Yeah. So, in general, like, all of these theorems have been talking about, like, what happens in the limit. And, like, in practice, that's actually going to mean that you have information which is not quite perfectly conserved is going to drop off over some, like, time, some period of time in resampling, or it's going to drop off over some distance in spacetime, things like that. There's going to be some rate at which it drops off. And, like, sometimes we're interested in information which propagates to Andromeda. Sometimes we're interested in information which propagates to my life next week. And these are going to be different. Sure, sure, sure, sure. And then go to the next page. In this g of x thing, right, if you can delete a finite number of nodes, and that causes you to lose a bit of information, if it's no longer accessible, that means that thing is not in x infinity. Roughly speaking, yes. There's an exception if, like, some of the nodes you deleted have perfect constraints between them, but roughly speaking. So whatever x infinity is for our universe, like, just the full x infinity for the full universe, you can't include the specific notion of trees. The way I would put it is not, like, the x infinity of our whole universe so much as, like, x infinity taken at a spacetime length scale of our whole universe. So, like, the idea that there will be somewhat different x infinities associated with different spacetime scales, right? Like, how far does the information propagate? So with the other one, with the layers of the causal graph, you would say that there's, you'd say there's, like, something that eventually persists forever. You wouldn't say that, you could look at it, you know, there's a few, and you'd say, there's more that persists from here to here, but there's something that persists in the middle. Yeah. Yeah. In principle, yes. And, yeah, for something like a tree that, like, if you really take the limit, it's going to go to zero. In practice, I don't think that's how most human abstractions work. Like, we're, clearly we're interested on scales that are, like, what's relevant to my life next week much more than we're interested in, like, what's relevant in Andromeda or far beyond that. Yep. Yes and no. I mean, from an evolutionary standpoint, what you'd expect is that you're going to be interested in the things about your life that will be relevant to your life later on. Eh, debatable. That's, yeah, that's a philosophically tricky point there. I mean, no, I think whether in fact that is true is itself debatable. Yeah, because if you say that evolution is the thing that gave you the ability to think about, oh, I want to do these things that are better than the right thing that evolution is doing, so. Exactly. There's the abstraction of evolution that you say, I don't want to do that now. There's the actual process that created you anyway. Yeah. Anyway, I propose we have this discussion over dinner. Good place to stop the video and whatnot. 