Welcome, once again, everyone, to another episode of the Towards Data Science podcast. My name, of course, is Jeremy, and I'm the host of the podcast. I'm also on the team over at the Sharpest Minds Data Science Mentorship Program. And I'm excited about today's episode because we're talking to Rob Miles, who's a really successful YouTuber whose channels focus on AI safety and AI alignment research. Now Rob's actually been popularizing AI safety work since about 2014, which is long before most people were interested in this space. It's long before I knew about it, really, as an area of focus. So he's got a lot of interesting perspectives on how the field has evolved, how AI research has gone from just purely focused on capabilities, making systems that are able to do more and more things, to now starting to worry a little bit more about, okay, how are we going to deploy these things? Should we worry about where the stuff is going, what the limiting case is when the technology gets really, really powerful? What should we be thinking about in terms of risks? So Rob's been doing a lot of interesting thinking about these issues. He's also collaborated with a whole bunch of AI alignment researchers. So we'll be talking about a whole bunch of different topics, including communicating about this stuff, and more generally, some of the problems and opportunities that might be ahead for us as a civilization as this technology gets better and better. So I'm really looking forward to getting into this one with Rob. And without further ado, let's get the ball rolling. Hi, Rob. Thanks so much for joining me for the podcast. Hi. Thanks. Good to see you. I've been following your YouTube channel for a really long time. I think it's a great source of information. I think one of the interesting things about what you've been doing is you've been talking about AI safety in some capacity or other since about 2014, which is way before most people. It's way before I was aware, really, of the problem. And one thing I wanted to start with here was how did you become aware of AI safety as an issue? And what made you dedicate so much time to that problem? Yeah. So it was around 2014 when I first started talking about this publicly. I got interested in it around 2010, 2011. What actually had happened was I had a Kindle for the first time, I had an e-book reader, and I was very excited about the ability to get through a lot of reading material very easily because I could have every book that I was reading on my person at all times. And at that time, I had a rule as an undergraduate that if I ever read three things from the same author, each of which caused a significant update in my views or my beliefs, or that was engaging or interesting or just generally excellent, that I would then try to read everything that that author had ever written. And this triggered almost immediately on Eliezer Yudkowsky. And I don't know why I'd stumbled across some things possibly on Overcoming Bias or somewhere like that. And so I started, I downloaded an e-book that I think Paul Crowley put together of every one of Eliezer's blog posts, and I put them on my Kindle and I just read them. And it took a while because he's a prolific guy. Just going through all of those, it made it very clear to me that this was an actual thing. Obviously, it's something which I had come across in various sort of science fiction contexts and so on. But and that seems to be how it is with these subjects for most people. They're familiar with them. But it takes a particular approach to show that these are actually, firstly, importantly different from the science fiction ideas. And secondly, that this is computer science. This is an actual open problem that we don't currently have good solutions to. That something seeming like science fiction is a poor indicator of its actual plausibility because accurate predictions of the future at almost any point in the last hundred years, if you give people accurate predictions of 50 years ahead, they will sound completely absurd. And if you give people wrong predictions about 50 years ahead, they will sound equally absurd. And so the fact that the thing sounds kind of crazy is not evidence for or against it actually being true. And it was the first time that I started to really just actually look at what I knew about computer science from my studies, what I knew about artificial intelligence, and just actually follow the arguments through and think, well, how would this system behave? You know, if this were much more powerful, what would be the actual consequences? That made me realize, yeah, that this is a real thing. Yeah, I find it so interesting that I think there are a lot of people who've come in at it from that perspective. I mean, it's almost like we need to be given permission to reason from first principles, especially when the conclusion is so radically out of tune with our life experience or our day to day. It kind of makes me think back to, you know, what would Einstein have thought trying to explain to people, like, no, I really think like a nuclear weapon that can destroy entire cities is not just on the horizon. It's like two years away. You would have been entitled to look at that and say, well, look, everything in our life experience, everything in the experience of humanity as a whole tells us that this is just such a complete anomaly, an outlier that can't possibly be true. It's sort of a similar effect. It's interesting. And so it was helpful to have that approach of like, supposing this was false, how would the world look? What would we observe? And then supposing this was true, how would the world look and what would we observe? And that's the only way that you can make progress on these things. You just have to look at like if you're sorry, I get frustrated even thinking about this because I've kind of moved on a bit from this whole thing because it's just natural now. It's part of how I think. But I remember wrestling with this when I was talking with people about it earlier on that there's no way to predict what AI systems are going to do by thinking about people and what people are saying and what people are thinking right now. Like it could be that these people are like naive and hopeful about the future and these people are cynical and these people have lived through an AI winter and these people haven't and whatever else. And like none of that is going to give you the answer because this is a technical question. You have to look at the systems. You have to look at the engineering. You have to actually, as you say, think from first principles because that's what's going to actually affect. We're just talking about how future technologies are going to behave and you can't do that without looking at the technologies themselves in detail. And so what was the process, if you try to reconstruct it, what was the intellectual process that you went through to conclude that, yeah, I'm pretty confident. Obviously, nobody is really confident. Actually, that's one of, I think, the most beautiful things about talking to people in this space. No one's really approaching this from the standpoint that I know with certainty that AI is going to be good or I know with certainty that AGI is going to be terrible. There's a good dose of intellectual humility here, but I think everybody's come to certain conclusions about where the probability density lies. I'm just wondering, what were the biggest factors that shifted that probability density for you over time? If I have to be honest, most of my early involvement or my early interest was because these are extremely interesting problems. And the fact that they might be extremely important was always there and was always available intellectually as a justification. But the thing that drew me in viscerally is that these problems are fascinating. They cut right to the core of what does it mean to be, what even is thinking? What does it mean to be an agent in the world? What does it mean to want things? What does it mean to have goals and values? What are ethics? What do we want actually from the world? Where are we trying to steer the future to? And just the sheer interestingness of the questions is what drew me in. And then after a certain period of time of thinking about the questions, you steadily update towards various conclusions about them kind of unavoidably as a consequence of thinking about them. Yes, it's sort of hard to pin down the exact, I guess the exact set of arguments that, I don't know, I always find this personally, it's difficult for me to think about how to convey my own priors to somebody else, the justification for them from scratch, just because I've assimilated so much. In some ways, this almost feels related to the alignment problem because so much of what we believe and think and want is implicit. Yeah. I guess it's just, yeah, maybe it's just a complicated question with no easy answer. I mean, there's various different approaches and models and things that shifted me. One of them was just the realization that really extreme, dramatic things can happen and have happened, especially, and this is like a whole thing that I don't know if we have time to go into, but thinking about the long arc of history and the optimization processes in that, in like a deep history, that you have evolution operating for a certain period of time, and then there's a switch that gets flipped somewhere where you have something that speeds up the rate, something like sexual reproduction that speeds up the rate at which evolution can happen. And that when you look back in history, what you have is these tremendously long periods of one mode and then a paradigm shift into a mode which is faster. And then you have intelligent animals that can operate, and then they develop language and they develop culture, and then they develop writing. And these are all things where you're either changing the way that information is being sort of modified or the way that information is being stored. And each of these periods is dramatically shorter than the previous period. Yeah. And that this is something that's happened several times. And so it seems pretty clear to me that in the same way that sexual reproduction allows for sexual selection, which means that the rate of reproduction doesn't just depend on sort of the environment as a whole deciding which animals live and die, which ones succeed and which ones fail. Now the intelligence of the animals themselves is applied to picking mates based on, so you don't have to actually win in a fight, you just have to observably be strong. Yeah. And then that gives you more signal effectively, gives you better gradients to train on in a way. And that just speeds up the whole process and so on. And so it became clear to me that the point where AI research is being done by AI systems is another one of those things where you close the loop. You're taking something that works dramatically faster and feeding that in to its own improvement cycle. And I don't necessarily mean like pure self-improvement, like the AI is going to be this unbelievably powerful thing that will take apart its own source code and restructure it or whatever. I just mean that having AI systems help the humans and to play a larger and larger role is the kind of thing that it looks like the same class of thing where you just shift into a new paradigm of dramatically faster development. And it was those kinds of arguments that made me think like our usual sense where we think, oh, you know, 50 years away, 100 years away, we're just kind of getting some vague sense of like, how hard does this seem? Yeah. And then turning it into a number by some general, like just a vague mapping. Whereas like if you actually think about it, the number of seconds per second is going up as this whole process happens. The number of cycle times or yeah. Right, right. Like if this is going to take 50 years of time to develop, it doesn't really take 50 years of clock time. It takes 50 years of subjective time. And when you have AI systems doing the research, subjective time is getting faster all the time. So that was the kind of thing that made me think, hey, this could be in my lifetime. Yeah. This could be something that we're close enough to that it's worth working on now. Well, this very much makes me think of, you had an interesting video where you talked about a popular sort of counter argument to the AI risk argument where people will say, well, look, our company is really just a kind of AGI already. Don't we have this kind of self-improvement quality? I think the way you've laid it out there just makes it so clear what the qualitative difference is. You're talking about a different regime of computation, really, in the history of the universe, in the computational history of the universe. So it really does seem like a lot of our metaphors and the things we're used to drawing our intuition from just become these very, very kind of frail things the moment we start to look at these massive qualitative transitions. Absolutely. And I think that you're right that most people in the field have a lot of humility about this because we have lots and lots of arguments. People from the outside often view what we're talking about and say, this stuff is very difficult to predict, and you seem to be fairly confident in this particular outcome, whereas it could be anything. And there's various reasons why. There's various responses to that, but I think actually most of the time what these arguments do is they make you spread your distributions out. They make you spread your probability distributions out. And so, yeah, there's all kinds of problems and difficulties, and we really are very uncertain, but it means it works in both directions, right? It might take 10 times longer, but it might take one-tenth the time, and so it's worth paying attention to. Well, there's one last theme I do want to touch on before we leave this idea of kind of that deep time picture that you just introduced. Just as a curiosity, what do you think it is that's being optimized for in that context? Because we have this notion of a universe that's full of particles, and those particles randomly combine. You know, sometimes you hear the idea of multiplying information or propagating information forward through time. That doesn't seem to intrinsically quite hit the nail on the head just because that information changes considerably from the clump of atoms early on to the first cell to the first sexually reproducing organism and so on. I don't know if you have a thought on this, but do you have a sense of maybe what that process is optimizing for? You mean like what is the universe optimizing for or what is physics optimizing for? I don't think it is. I think, I don't know. Some people think that there's some kind of free energy minimization type thing that accounts for a lot of this stuff. I don't know. I think evolution can be meaningfully thought of as an optimization process or rather as a large collection of optimization processes. Because if you think about, like, if you take evolution as a whole as an optimization process, then it doesn't really make sense because you've got, you know, the predator chases the prey and the prey runs away. So is evolution optimizing for it being caught or for it escaping? It's like, well, the predator species has its evolution and the prey species has its evolution. And of course, all of these boundaries are fuzzy because a species is kind of a structure that we've overlaid on this thing. Nonetheless, it is optimizing for producing good replicators is the way that I would think of it. So, well, and that was where I was hoping we'd end up going because the replicator picture, to some degree, I mean, when I start thinking about AIs and AGIs that presumably propagate forward through time, the idea of replication seems to become decoupled from self-improvement or from continued existence through time. It almost seems like there's another quality that, a deeper quality that, you know, the continuity of some kind of causal structure or computing structure, I don't really know, but it's also speculative anyway. I think the continuity, the only kind of continuity that you could really strongly expect is goal continuity because if I'm some kind of AI system and I'm going to create other AI systems to go out into the world and do things, the only thing that's really important to me is that they share my goals or that they, in practice, will advance my goals. Yeah. But realistically, usually the best way to do that is to have them share your goals. That's the type of continuity I would expect. Interesting. Okay. I don't want to linger too much on that. I just, I thought it was an interesting thing to unpack a little bit. One thing I do want to talk about, it's something that I think you've done really effectively through your YouTube channel, is you've actually taken the time to address a lot of the arguments against AI safety or against worrying about AI risk. I think that's something people don't tend to do maybe as much as they should, because there are still a lot of people who wonder, you know, why should I be worried about this? Is this really just like a Terminator scenario? Is it really something that people are just freaking out about for no reason? I know you're personally concerned about the risk of AGI, but which anti-AGI arguments do you find most compelling? Yeah, so you're right. I do have, I do place a lot of emphasis on that internally, because I think it's such an easy and obvious failure mode of thinking to just start only listening to people who already agree with you. I think it's really important to seek out smart people who disagree with you and try to really listen to what they say and to try to really understand what they mean and come up with strong versions of their arguments and really stress test your stuff, because I don't want us to have a giant problem with AI in the future, right? Like, I am kind of looking for reasons why this isn't as big of a problem, and I would like to be convinced, but at the same time, obviously, I've been out in public talking about this being a problem for a long time, so I have to be very aware of my own psychology, because there's going to be a part of me that's going to want to, sort of my personal identity is bound up with it, and so that's going to be a strong force that's pushing me to not take these things seriously, and so all of these are reasons why it's important to, like, deliberately and consciously actually make an effort to engage with people who disagree with you. I mean, everybody knows this. Well, it's easy to say, and it is a cliche, but so few people actually do it that I think that that dividing line is still worth fighting, yeah. Yeah, that's true. The earlier stuff on the channel, especially, was closely based on ideas from Joukowsky and ideas from Bostrom, which revolve around this framework where you have a takeoff scenario. You have a single agent, a sovereign agent, which is able to act in the world, and you're developing it until it hits a certain level, at which point it starts to increase its capabilities exponentially by acquiring additional computing resources, improving its source code, and so on, until it has a decisive strategic advantage, at which point whatever the objectives are of that system, whatever the utility function of that system is, you're going to end up with a world that's somewhere very highly rated by that utility function, which is probably apocalyptic, and that view is, I think that my view of the situation is more complicated now, because there's a, like I now place more probability on a more multipolar situation where you have, if the development process happens slowly enough, then you actually have a lot of different AI systems in the world with different levels of capability, and so then the question is not like, what happens if you have one super intelligence in a world that is otherwise more or less like our world? It's like, by the time you have a system that's able to take off, what does the rest of the world look like? And that consideration has shifted my perspective a bit, because it's not purely a technical question. It's a much broader question of economics and politics, and you have to do a lot more looking at the world. You can't quite do it all with Greek symbols on a whiteboard in the same way, and so that has, I still place significant probability on some AI system in a research lab somewhere just exploding because somebody has had a brilliant insight. There are plenty of situations where one team just is ahead of everyone else by far enough that the gap doesn't matter, like that nobody else could catch up, but there are also situations where these things develop in lots of places at lots of times, and that whole situation is so much more complicated and harder to think about that it has, oh what do you know, it's spread all my probability distributions wider, so I'm just less certain about those things than I used to. And the thing is that the fundamental problem of not being able to specify what we want and having systems that are going after goals which aren't what we want is not solved by having lots of systems, but it is complicated by it. I was going to say it almost feels as though it's a strict exacerbation of the problem to the degree that, or to the extent that it just creates a situation where now, even if you could solve the inner and outer alignment problems and everything's good technically, you then need to enforce those, you need to force companies and labs to implement those things, which seems to just add a policy layer on top of everything else. Yeah, yeah, because if you have a singleton situation where you just have this one system that explodes, then at least if you get that one right, then you're okay, because it has power over everything else and nobody else, you're not going to be able to launch your own unaligned AGI project in a world that has a singleton AGI, aligned AGI already in existence. So that lets you get around that problem, but yeah, it's a problem. Yeah, well, so given the underlying pessimism that I think a lot of people might detect in the air here, in fairness, I don't think that, I don't think anyone places obviously a hundred percent probability on the disastrous outcome. There's a wide range of disagreement in the community. You have some researchers saying, you know what, I think mostly I'm almost positive the outcome is going to be good. Other people somewhere in the middle, there's been a lot of polling and you highlighted in one of your videos, I think that cumulatively people who have a negative outlook on the stuff, people think that AGI on the whole will be negative for humanity. It's something like 15% of whatever community was polled. And so I'm going to put a big asterisk there. You know, this isn't the one. You're talking about the Grace 2016, that was people who published papers in ICML and NeurIPS. Right. So pretty high quality researchers, but nonetheless, not people focused on alignment, which in fairness, being focused on alignment means you're worried about safety. So it's tough to get a good poll here, but so basically baseline 15% in that poll, maybe that's shifted, but why do you think that something like 80 to 85% of people might view things more optimistically? Like, what do you think from your model of the world, where does that come from? Yeah. So I'm pretty reluctant to Bolverism. I think it's called Bolverism, where you assume that people are wrong and then try to psychoanalyze them to figure out why they might reach that conclusion. It does feel like overfitting. Right. Right. At the beginning of the interview, I was saying you can't reach conclusions about the world by psychoanalyzing people. But I mean, I don't know. I don't know that I have any particular insight here. I think people want to believe that what they're doing is helping. And I think it's also the same kind of selection effect as working on alignment, working on AI at all. A lot of people are working on AI because it seems like an important technology that, and like broadly speaking, technology makes things better. Right. Like I believe that. And I know a lot of people don't. I think broadly speaking, technology makes humans more powerful. It makes humans more able to do things. And so if humans are on the whole a good thing, which we are sort of by definition, in my opinion, like according to human values, humans are pretty good. Yeah. Then something that allows us to get what we want is on the whole a positive thing because people having their preferences satisfied is like a decent definition of what good is. So technology is good. And AI is a form of technology. And so it's probably good. It's like a reasonable prior to have. I actually think 15% is kind of high. I don't know how this looks for other fields. How many automotive engineers think that cars are on the whole bad for the world? I don't know. Probably less than 15%. Though, was this now I'm trying to remember the exact framing of the poll, but was the poll about whether AI itself is good for the world in its current form or the risks of future AGR, the implications of taking this technology to the limit? I think it was about future impacts of AGI, but I think everybody, maybe it's not true. Maybe some people are just making their little narrow AI systems and not thinking about where this is going. But I think everybody has a sense of like, all AI research is broadly in this direction. And after every AI winter, people pretend that it's not, but there's no pretending that this isn't what we're trying to do. Yeah. I guess it's the implications. I'm just thinking, if I were working in biotech or whatever would be called biotech in the 1930s or 1940s, penicillin comes out. And now all of a sudden, it's wonder unambiguously, wonderful thing. You're curing polio. You're doing things with smallpox. And then you turn around, it's 2020. Now people are starting to go tabletop bioweapons are not that far out of the realm of what's possible as technology makes things cheaper. It seems like there might be, I don't know, it's again, this kind of new regime of technological development where the world has so many degrees of freedom. And every time you take a technological step forward, you're opening a whole big subspace that wasn't previously accessible. And because there are far more configurations of the world that are very bad for humans than there are configurations that are good. Those bigger steps tend to be a little riskier. I don't know if that's a fair assessment. Yeah. That fits with my models, I think. Cool. Well, we're too optimists here. Great. Well, I do want to ask a question then more on the technical alignment side, because that's something you focus a lot of your content on. I've learned a lot from your channel on that topic, actually. There are a couple of different schools of thought. It's almost hard to classify and do the taxonomy of these schools of thought, but I'm going to take a shot here and I want you to feel free to shoot me down on this. My sense is that there's one group of people that approaches the alignment problem with a kind of, what I might for want of a better term, call it an engineering mindset. And the philosophy here is something like our best shot at making AI safe is to align it more or less as we build it. So build it a little bit. Notice, oh, the tower's wobbly, so I'm just going to fix it as I go. And then the second is maybe more perfectionistic, a little bit more philosophically mathematical, taking the view that we only get one shot at doing this right, maybe because self-improving systems rapidly reach takeoff velocity and get away from us or whatever reason. I guess maybe I'll stop there. Do you agree with that framing or am I missing something there? I think there's kind of... Okay, so here's a metaphor. Suppose we're building aircraft. We want to build an aircraft that's going to take us to a particular city that's distant from us. That's our goal. And we want to do this safely. And let's say that it's an island and really all that matters is correctly aiming at that island so that when we land there, we land there and not in the ocean. So you've sort of characterized two different approaches, one as being like, let's build this thing perfectly so that it's aimed exactly at the island and then press the button. And another one, which is like, we'll wing it. We'll play it by ear as we go. And in practice, the best way to do it is a combination by which I mean, you do a lot of very, very careful engineering, but you do still expect somebody in the airplane to be adjusting it as you go. But the precise mechanisms by which you are getting feedback about which direction you're headed and controlling and adjusting your trajectory, these are all very carefully planned and engineered things. But you don't try and do all of your planning up front. You plan in exactly the ways in which you're going to adjust. So an approach, whereas the extreme other approach would be like, we're going to take off in our aircraft and then see if we can't design some rudders and ailerons and control systems and radar and whatnot on the way. I think either extreme is foolish. And then there's a question of how you're going to balance these concerns. But I think nobody is actually at either of those extremes. The people who are the most mathematically minded are just saying, look, we need to really understand and have strong assurances that when you turn left on the thing, the plane is actually going to go left. And other people are focused. It's more like, are you focused more on the mathematics of aerodynamics and all of that stuff, or are you interested in the details of avionics? But they're both engineering approaches towards getting a system that's controllable and that does what you want it to do. And what would be some of the more recent innovations on that front? Are there new approaches to alignment that you've become aware of in the last, say, two years that you think are worth highlighting, especially for people who are just trying to orient themselves in the space? I have a really hard time keeping track of time. So I have no idea what came out in the last two years. Sometimes I think things have been around forever and they're actually only a year or two old. That's the time compression effect you were talking about earlier. Yeah, absolutely. The speed with which things can become just part of the landscape, because it's such a young field. But so one of the biggest things that came out, the most recent thing that really shifted my perspective is the work on MESA optimizers. And that is like a whole other class of problem that I didn't even realize we could have. And I'm currently working on a video about it, actually. So maybe I shouldn't say too much. People watch that video when it eventually comes out. But the idea that even if you've perfectly specified the correct objective to your training process, the model that comes out of that training process might be misaligned with that objective. And the so-called inner alignment problem is just a whole other class of issue that honestly makes me kind of pessimistic. Because it was so recent that people have been thinking about this for a long time. And there are people that sort of vaguely hinted at it, that this type of thing might be a problem. But that this is the first time that it's been properly laid out and given a full treatment. And we realized that it is as big a problem as it seems to be, is very unsettling to me. Because what else do we not know that we don't know? Right. I guess my understanding, at least to some degree, Paul Cristiano's philosophy at OpenAI on this is something like, we hope to get to a point where we're leveraging advanced AI systems to help us with alignment, to help us discover some of these unsolved problems. I have no idea whether the threshold of world overtaking AI system falls ahead of or behind the threshold of we have an AI that can help us align AIs. That itself sounds like an interesting problem. But yeah, the MISA optimization thing, at least as I've come to understand it for people who might not be familiar with it quite so much, is just this idea that you have within an overall optimizer, like a deep neural net, you might end up with substructures that are intent on retaining their structure. So you can almost think of it in a way as kind of like a cancer within the human body. You know, to the extent that the human body is some sort of optimizer, some agent, the cancer itself kind of goes, oh, I have my own interests that are separate from the whole. And now I'm going to kind of optimize my way through some pathological strategy of taking over or doing some damage to the process. Well, that's one way of thinking about it. And it's one sort of modality. But it could be the entire, it doesn't need to be a substructure of the agent. It could be the entire agent. And the analogy there is with evolution again. If you think about evolution, if you model it as an optimization process, it has an objective, which is to maximize inclusive fitness or whatever, you know, maximize the number of surviving offspring you have. And yet, when that optimization process produces agents, the goals of those agents are not to maximize their own reproductive fitness, right? People don't, like animals don't want to make a lot of copies of their DNA. They don't even know what DNA is. They want this sort of, this unpredictably derived set of goals, which are a function of the original goal, certainly, but also little contingent things of the training environment and details of different strategies that help them to succeed in the ancestral environment and so on. And that they don't care, right? Like human beings, even when we understand what the goals were of the optimization process that created us, that's not persuasive to us, right? And we will continue to use contraception and whatever else. We go for, we're taking this, because we don't have a goal that's optimized our reproductive fitness. And so our goals include things like eat food that's tasty, which was helpful, but nowadays is actually not necessarily the best thing to do, because we have, because the environment is different. So yeah, so that's the other way of thinking about it, that you have the capacity to end up with a trained network that has goals that are unpredictably different from the goals that you actually specified. And what's more, it then has all of the same convergent instrumental sub-goals that you would expect from any misaligned system. So it's going to want to conceal the fact that its goals are different and manipulate the training process and so on, which makes them especially difficult to deal with, because it's not just that the thing might be misaligned internally, it's that it might be misaligned in such a way that it is actively trying to hide that it's misaligned because it wants to be deployed. This all kind of seems related actually, again, to that time horizon picture that you laid out early on, just in the sense that when you think about the time horizons that evolution acts on, the feedback that we get through evolution happens on the order of generation, 20, 25 years, something like that, every time we reproduce. Whereas the feedback we get from the real world, our own subjective clock time is way faster than that. We get feedback on a really tight loop from our environment, which allows us to, we have so much extra compute capacity above and beyond what would be needed to just hit that main goal of reproducing that it ends up getting deployed in some really random direction. I mean, it's untethered. It's constrained by its environment to some meaningful degree, which allows us to diverge considerably from that evolutionary objective. Yeah. This is part of what makes it kind of an unfair competition, that because we operate so much faster than evolution, we are able to get away from it. We're able to do things that if it were more powerful compared to us, it would really, well, it would stop us from doing. It may yet, right? Evolution hasn't stopped happening. It's just continuing to happen at the same slow pace that it always has. Everything else has sped up to the point where its actions are mostly irrelevant most of the time. But it's still happening. It's the same kind of thing that you would expect when you have a misaligned Mesa optimizer. That Mesa optimizer might be quite powerful and able to think in a tighter loop than something like gradient descent and it's training it, which would allow it to, this is the other thing, is that that in principle would allow it to outperform better aligned optimizers. Then that's really annoying because then gradient descent is going to actively try and select the misaligned Mesa optimizers because they're doing a better job, because they're the only ones who realize that they're in a training system with this particular objective that they're now actively trying to optimize as an instrumental goal towards getting themselves deployed in the real world. It's the same kind of thing. There's the further analogy then that AI systems in general operating much faster than we do. It means that we may end up not powerless, but just not able to make changes fast enough to continue to have control over the way things end up. Right. Yeah. It just sounds like such a generally, hopefully not intractable problem. I mean, I suspect that given a couple of centuries of time, we'd be able to make meaningful progress towards this. We have question mark number of years or hopefully decades. Actually, maybe to wrap things up, because I know you've got to get going, but do you have any thoughts about what somebody was concerned about this topic or just generally intellectually curious about AI alignments, the technical details of all this stuff? How might they start getting involved, start figuring out where the open problems are and what should they read? There's a lot of different things. There are some really good books. I used to recommend Superintelligence. Actually, I would say that it's probably no longer the best first book to read. I would actually read something like Human Compatible, Stuart Russell's book, which came out relatively recently and is a good solid introduction to the area. The other thing is, and I'm going to recommend a book that I haven't finished reading yet, because it came out like the middle of last week, I think, which is The Alignment Problem by Brian Christian. So far, it's very good. That goes over, again, it seems like a good introduction to the field for the general public to get a feel for what the different problems are. If you're already a bit more involved in machine learning and AI stuff, I actually would recommend reading The Alignment Newsletter. I'm just going to say this because I don't think Robert's going to actually plug this, but I desperately want him to. So Rob's been doing a podcast version of The Alignment Newsletter, which is just really great. So just to toss that out there, you can check that out as well. Yeah, so the newsletter is really, really good in that it's a weekly newsletter which summarizes research that's happened in a week. So I think it's really good if you are already a researcher. The problem that I often have when talking to people who actually know a lot about AI and a lot about machine learning is that they don't realize the extent to which this is a real active field that people are publishing a lot of papers in, that it's a growing field. But it's still tiny, right? It's growing, but it's tiny. But these are real problems that you can do computer science on. I think the newsletter is really good at giving you a feel for the kinds of papers that people are writing and the kinds of problems that people are making incremental progress on. Yeah, and actually, so people who've been listening to the podcast, probably the episode I think before this one will be with Rohin Shah, who actually puts out The Alignment Newsletter. So these might be a good kind of back-to-back series to watch or listen to. Yeah, well, Rob, thanks so much. Oh, sorry. No, I was just going to say, it's a weird thing. I actually don't feel weird about plugging the podcast because I take no credit for it. I mean, literally, I just read out the newsletter. But if you're listening to this podcast, you're probably a person who likes to listen to podcasts, a person who likes to take in information through their ears. And I like to think that I do a better job than text-to-speech software can, especially with the technical terminology and stuff. You know what? And let's hope that's always the case. I would do it anyway. There you go. That's the spirit. Yeah, very human. Thanks so much, Rob. Really appreciate it. We'll be leaving links to all those books, actually. And of course, your YouTube channel and the blog post that will accompany this podcast. So if anybody wants to check out Rob's channel, highly recommend it. And really appreciate your time on this one. Thanks so much for having me on the show. 