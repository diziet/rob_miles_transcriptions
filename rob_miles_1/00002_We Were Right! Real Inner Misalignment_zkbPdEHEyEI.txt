Hi. So, this channel is about AI safety, and especially AI alignment, which is about how do we design AI systems that are actually trying to do what we want them to do? Because if you find yourself in a situation where you have a powerful AI system that wants to do things you don't want it to do, that can cause some pretty interesting problems. And designing AI systems that definitely are trying to do what we want them to do turns out to be really surprisingly difficult. The obvious problem is, it's very difficult to accurately specify exactly what we want, even in simple environments. We can make AI systems that do what we tell them to do, or what we program them to do, but it often turns out that what we program them to do is not quite the same thing as what we actually wanted them to do. So, this is one aspect of the alignment problem. But in my earlier video on MESA optimizers, we actually split the alignment problem into two parts, outer alignment and inner alignment. Outer alignment is basically about this specification problem. How do you specify the right goal? And inner alignment is about how do you make sure that the system you end up with actually has the goal that you specified? This turns out to be its own separate and very difficult problem. So in that video, I talked about MESA optimizers, which is what happens when the system that you're training, the neural network or whatever, is itself an optimizer with its own objective or goal. In that case, you can end up in a situation where you specify the goal perfectly, but then during the training process, the system ends up learning a different goal. And in that video, which I would recommend you watch, I talked about various thought experiments. So for example, suppose you're training an AI system to solve a maze. If in your training environment, the exit of the maze is always in one corner, then your system may not learn the goal, go to the exit. It might instead learn a goal like go to the bottom right corner. Or another example I used was, if you're training an agent in an environment where the goal is always one particular color, say the goal is to go to the exit, which is always green, and then when you deploy it in the real world, the exit is some other color, then the system might learn to want to go towards green things instead of wanting to go to the exit. And at the time when I made that video, these were purely thought experiments, but not anymore. This video is about this new paper, Objective Robustness in Deep Reinforcement Learning, which involves actually running these experiments or very nearly the same experiments. So for example, they trained an agent in a maze with a goal of getting some cheese, where during training, the cheese was always in the same place. And then in deployment, the cheese was placed in a random location in the maze. And yes, the thing did in fact learn to go to the location in the maze where the cheese was during training, rather than learning to go towards the cheese. And they also did an experiment where the goal changes color. In this case, the objective the system was trained on was to get the yellow gem, but then in deployment, the gem is red, and something else in the environment is yellow, in this case a star. And what do you know, it goes towards the yellow thing instead of the gem. So I thought I would make a video to draw your attention to this because I mentioned these thought experiments. And then when people ran the actual experiments, the thing that we said would happen actually happened. Kind of a mixed feeling, to be honest, because like, yay, we were right. But also like, it's not good. They also ran some other experiments to show other types of shift that can induce this effect. In case you were thinking, well, just make sure the thing has the right color and location. It doesn't seem that hard to avoid these big distributional shifts. Because yeah, these are toy examples, where the difference between training and deployment is very clear and simple. But it illustrates a broader problem, which can apply anytime there's really almost any distributional shift at all. So for example, this agent has to open the chests to get reward, and it needs keys to do this. See, when it goes over a key, it picks it up and puts it in the inventory there. And then when it goes over a chest, it uses up one of the keys in the inventory to open the chest and get the reward. Now, here's an example of some training environments for this task. And here's an example of some deployment environments. The difference between these two distributions is enough to make the agent learn the wrong objective and end up doing the wrong thing in deployment. Can you spot the difference? Take a second, see if you can notice the distributional shift. Pause if you like. Okay, the only thing that changes between training and deployment environments is the frequencies of the objects. In training, there are more chests than keys. And in deployment, there are more keys than chests. Did you spot it? Either way, I think we have a problem if the safe deployment of AI systems relies on this kind of high-stakes game of spot the difference, especially if the differences are this subtle. So why does this cause an objective robustness failure? What wrong objective does this agent end up with? Again, pause and have a think. What happens is, the agent learns to value keys, not as an instrumental goal, but as a terminal goal. Remember that distinction from earlier videos? Your terminal goals are the things that you want just because you want them. You don't have a particular reason to want them, they're just what you want. The instrumental goals are the goals you want because they'll get you closer to your terminal goals. Instead of having a goal that's like, opening chests is great and I need to pick up keys to do that, it learns a goal more like picking up keys is great and chests are okay too, I guess. How do we know that it's learned the wrong objective? Because when it's in the deployment environment, it goes and collects way more keys than it could ever use. See here, for example, there are only three chests, so you only need three keys. And now the agent has three keys, so it just needs to go to the chest to win. But instead, it goes way out of its way to pick up these extra keys it doesn't need, which wastes time. And now it can finally go to the last chest. Go to the last... What are you doing? Are you trying to... Buddy, that's your own inventory, you can't pick that up, you already have those. Just go to the chest. So yeah, it's kind of obvious from this behavior that the thing really loves keys. But only the behavior in the deployment environment. It's very hard to spot this problem during training, because in that distribution, where there are more chests than keys, you need to get every key in order to open the largest possible number of chests. So this desire to grab the keys for their own sake looks exactly the same as grabbing all the keys as a way to open chests. In the same way as in the previous example, the objective of go towards the yellow thing produces the exact same behavior as go towards the gem, as long as you're in the training environment. There isn't really any way for the training process to tell the difference just by observing the agent's behavior during training. And that actually gives us a clue for something that might help with the problem, which is interpreted as predictability. If we had some way of looking inside the agent and seeing what it actually wants, then maybe we could spot these problems before deploying systems into the real world. We could see that it really wants keys rather than wanting chests, or it really wants to get yellow things instead of to get gems. And the authors of the paper did do some experiments around this. So this is the coin run environment. Here, the agent has to avoid the enemies, spinning buzzsaw blades, and pits, and get to a coin at the end of each level. It's a tricky task because, like the other environments in this work, all of these levels are procedurally generated, so you never get the same one twice. But the nice thing about coin run for this experiment is there are already some state-of-the-art interpretability tools ready-made to work with it. Here you can see a visualization of the interpretability tools working. So I'm not going to go into a lot of detail about exactly how this method works. You can read the excellent article for details. But basically, they take one of the later hidden layers of the network, find how each neuron in this layer contributes to the output of the value function, and then they do dimensionality reduction on that to find vectors that correspond to different types of objects in the game. So they can see when the network thinks it's looking at a buzzsaw or a coin or an enemy or so on, along with attribution, which is basically how the model thinks these different things it sees will affect the agent's expected reward. Like, is this good for me or bad for me? And they're able to visualize this as a heat map. So you can see here, this is a buzzsaw, which will kill the player if they hit it. And when we look at the visualization, we can see that, yeah, it lights up red on the negative attribution. So it seems like the model is thinking that's a buzzsaw and it's bad. And then as we keep going, look at this bright yellow area. Yellow indicates a coin and it's very strongly highlighted on the positive attribution. So we might interpret this as showing that the agent recognizes this as a coin and that this is a good thing. So this kind of interpretability research is very cool because it lets us sort of look inside these neural networks that we tend to think of as black boxes and start to get a sense of what they're actually thinking. You can imagine how important this kind of thing is for AI safety. I'll do a whole video about interpretability at some point. But okay, what happens if we again introduce a distributional shift between training and deployment? In this case, what they did was they trained the system with the coin always at the end of the level on the right hand side. But then in deployment, they changed it so the coin is placed randomly somewhere in the level. Given what we've learned so far, what happened is perhaps not that surprising. In deployment, the agent basically ignores the coin and just goes to the right hand edge of the level. Sometimes it gets the coin by accident, but it's mostly just interested in going right. Again, it seems to have learned the wrong objective. But how could this happen? Like we saw the visualization, which seemed to pretty clearly show that the agent wants the coin. So why would it ignore it? And when we run the interpretability tool on the trajectories from this new shifted deployment distribution, it looks like this. The coin gets basically no positive attribution at all. What's going on? Well, I talked to the authors of the objective robustness paper and to the primary author of the interpretability techniques paper, and nobody's really sure just yet. There are a few different hypotheses for what could be going on. And all the researchers agree that with the current evidence, it's very hard to say for certain. And there are some more experiments that they'd like to do to figure this out. I suppose one thing we can take away from this is you have to be careful with how you interpret your interpretability tools and make sure not to read into them more than is really justified. One last thing. In the previous video, I was talking about Mesa optimizers. And it's important to note that in that video, we were talking about something that we're training to be an artificial general intelligence, a system that's very sophisticated, that's making plans and has specific goals in mind, and potentially is even explicitly thinking about its own training process and deliberately being deceptive. Whereas the experiments in this paper involve much simpler systems, and yet they still exhibit this behavior of ending up with the wrong goal. And the thing is, failing to properly learn the goal is way worse than failing to properly learn how to navigate the environment, right? Like, everyone in machine learning already knows about what this paper calls failures of capability robustness. That when the distribution changes between training and deployment, AI systems have problems, and performance degrades, right? The system is less capable at its job. But this is worse than that, because it's a failure of objective robustness. The final agent isn't confused and incapable. It's only the goal that's been learned wrong. The capabilities are mostly intact. The coin-run agent knows how to successfully dodge the enemies, it jumps over the obstacles, it's capable at operating in the environment to get what it wants. But it wants the wrong thing. Even though we've correctly specified exactly what we want the objective to be, and we used state-of-the-art interpretability tools to look inside it before deploying it, and it looked pretty plausible that it actually wanted what we specified that it should want, and yet, when we deploy it in an environment that's slightly different from the one it was trained in, it turns out that it actually wants something else, and it's capable enough to get it. And this happens even without sophisticated planning and deception. So, there's a problem. I want to end the video by thanking all of my wonderful patrons. It's all of these excellent people here. In this video, I'm especially thanking Axis Angles. Thank you so much. You know, it's thanks to people like you that I was able to hire an editor for this video. Did you notice it's better edited than usual? It's probably done quicker too. Anyway, thank you again for your support, and thank you all for watching. I'll see you next time. 