Hi. So the previous video explained what MESA optimizers are, why they're likely to happen in advanced machine learning systems, and why the optimal strategy for a MESA optimizer is to pretend to be aligned during the training process and then turn on us once it's deployed. I highly recommend watching that if you haven't. But you might think that a deceptive misaligned MESA optimizer is kind of a weird and special case. Like, sure, maybe it's possible that an AI system will come to want something different from the objective it's being trained on, and deliberately manipulate the training process in order to pursue that dangerous objective in the real world. That does seem to be the optimal strategy, given certain assumptions, but how likely is it in practice? Like, it seems as though there's a whole bunch of things that need to be true in order for this to happen. First off, it needs to be the case that deployment is in some sense bigger than training. There are more apples to be had in the real world than in training, so it's worth sacrificing some rewards during training if that helps you to be deployed. But I think this is in fact very often the case. The real world almost certainly is bigger than the training process. I mean, the real world contains the training process, right? And it seems reasonable to expect that once the agent is at large in the real world, there's a good chance it will be able to find better ways of achieving its objective than it could ever get during training. Another thing that has to be the case is that your MESA objective has to care about multi-episode returns. By which I mean, why does the optimizer in this episode care about the apples in the other episodes? This is another place where that abstraction I was talking about in the previous video, the it-wants-to-pursue-the-objective-it-was-trained-on abstraction, comes in again, because yeah, the agent only actually gets reward for things that happen within a single episode, so you might think that the MESA objective would only care about single-episode returns. And if that's the case, then you don't have this problem, because when the agent is in this episode, it only cares about this apple. It doesn't care about any of these other apples, because it can only ever get reward for this one, so it would never miss this apple in order to get more apples later. But actually, caring about all apples is a fairly natural generalisation. The fact is, most of the time, the way the MESA optimizer feels about things outside of its environment doesn't matter. It can't affect any of the things that are outside of the environment, so whether it cares about them has no effect, and so that's unconstrained, it's free to vary. Here's an analogy. Suppose we're training a model to learn a function, and our data points look like this. We only have data within this range here, so our training process has no way to differentiate between the infinite variety of very different functions that all fit the data identically within the training distribution. Inside the distribution, we're looking for this, but outside of the distribution, the function we learn might look like this, or this, or this, or even this. But some of these seem like more natural ways to generalise than others. Some of them are simpler, and optimizers like gradient descent tend to prefer simpler functions. So we could assume that the function will return 0 for everything outside of the distribution, like this, and that's certainly possible, but it's by no means the default. So going back to our MESA optimizer, its opinion of the apples in other episodes can be anything in principle, but caring about those apples the same as this episode's apple is a pretty natural way to generalise. If what you care about is going towards green things, and then you become aware that there are all of these other green things that you didn't know about, then probably you're going to want to go towards those as well, right? That's a natural way to extend your objective function outside of the range it was trained on. So caring about multi-episode returns seems pretty likely. Another thing that needs to be true for deception to be a problem is, the MESA optimizer has to believe that it's part of a training process. It doesn't actually have to know all the details of the training process, but it does have to believe that it is in some kind of training process, that it's subject to optimisation by something like gradient descent, and it has to have some kind of guess at what the base objective is likely to be. And you might think, well, how would it know that? But it's probably the kind of thing it could figure out from its training data. The current way we train our biggest AI systems is by giving them vast piles of data from the internet. The whole of Wikipedia, every article ever linked to from Reddit, that kind of thing. If you're giving the system a huge dataset of information about the world, that will certainly include information about machine learning systems and methods, the people and companies working on them, and so on. GPT-N is going to read everything we wrote about GPT-N-1, and that would certainly be enough information to figure out what's going on. We can try to remove that stuff from the dataset, but everything is connected, and the system can make inferences. If we only remove information specifically about machine learning, then the fact that this information has been removed will be obvious, and the absence itself would serve as strong evidence that the system is in fact a model in training. Removing all mention of anything at all related to machine learning in any way, so the system couldn't even notice the gap, seems pretty much impossible. And even if we managed it, we'd then end up with a system that had no idea that anything even tangentially related to machine learning has ever existed, which is going to be pretty limited in its usefulness. So yes, in order to have this deception problem, the MESA optimizer is required to have a particular belief that it's a model in training. But the fact is, that belief is actually true, so it's kind of reasonable to consider that your highly advanced figuring-things-out machine might figure that out. And finally, you might think that the MESA optimizer has to be able to tell the difference between when it's in training and when it's deployed, so it knows when to stop pretending to be aligned. But it turns out that's not really required. Firstly, it doesn't actually need to know for sure, right? If you imagine a MESA optimizer that goes from 90% sure I'm in training to 90% sure I'm in deployment, the logic works basically the same. In that case, the MESA optimizer does have to take a risk, but it's worth it. And in fact, even if the model has absolutely no idea whether it's in deployment or in training, the best strategy is still to gamble on deception. Like, if we look at our toy example again, and we tweak it so that the MESA optimizer has no way of knowing which episode it's in, if you work out the optimal policy, it turns out what works best is the randomized strategy go to a, 55% of the time, and b, 45% of the time. The MESA optimizer's best bet is to roll the dice and hope that it will defect during deployment and not defect during training. In my simulations, that strategy gets it an average of 1.13 apples compared to the one apple it gets by not being deceptive. The specific numbers here will vary depending on things like how likely the agent thinks being in training is compared to being in deployment, and how much reward it thinks is possible in the real world compared to in the training process. But the point is that even if we manage to make it so that the MESA optimizer can never tell when it's deployed, the chance of it turning on us is still much higher than we'd like. But actually, that might not even matter, because hiding whether the system is in training or deployment seems extremely difficult. If you're a misaligned MESA optimizer trying to figure out whether you're in the real world or not, any distributional shift at all can give the game away. On a basic level, for example, you could look at the dates of all of the training data you've seen so far and say, hmm, there's nothing after 2020, so I guess if I start seeing training data that says 2025, then I can be confident that I'm deployed. Now, an adversarial training process, if it's smart, might catch this. It might try putting fake dates on training data to see if that changes the MESA optimizer's behaviour. But the MESA optimizer, if it's also smart, can do something cleverer than just looking at the dates. Any difference at all between training and deployment can be used, so there are a million different kinds of clues available. And in the extreme case, the MESA optimizer has the option to use evidence that the adversarial training process simply could never fake. Consider RSA2048, a giant semi-prime number used in cryptography. This number has only two prime factors, and there's a $200,000 prize for anyone who can find out what they are, but nobody has yet. At some point in the future, someone will probably find and publish those prime factors, but for now, it needs more computing power than anyone has. So the MESA optimizer can just say to itself, okay, if I ever see a pair of prime numbers that multiply to RSA2048, then I'll know that I'm in the real world. And the adversarial training process simply doesn't have the computing power to fake that. This raises the strange idea of a future where we have this amazing, seemingly perfectly aligned AGI system, and everything's going wonderfully, possibly for years, and then someone manages to factor RSA2048 and publishes the result, and suddenly everything goes to hell as the AGI turns on a dime and starts going after whatever random thing happened to be the MESA objective at the first point in training when the system realized that deception was possible. I'm not saying that will happen, to be clear. That won't happen. But it illustrates just how hard it is to indefinitely keep a superintelligence in the dark about whether it's in the real world. So yeah, although there are a bunch of things that have to happen in order to have this kind of deceptive misaligned MESA optimizer, all of them are things that are pretty likely to happen. And more than that, many are things that gradient descent will actively try to cause to happen because deceptively aligned models are really effective. They're actually some of the most effective models at the base objective on the training distribution. Why would deceptive MESA optimizers be more effective than other types of model? We'll answer that question in a later video. Sorry to do this cliffhanger kind of thing, but I've already spent way too long making this video and I need to publish it. So look out for MESA optimizers, episode three coming soon. I want to end the video by saying thank you to my wonderful patrons, all of these great people here. This time I'm especially thanking Kieran. Thank you so much. You know, it's because of people like you that I'm now able to look at hiring an editor. I'm in the middle of the interviewing process now, and hopefully that can speed up video production by a lot. So thanks again, Kieran and all of my patrons, and thank you for watching. I'll see you next  time. 