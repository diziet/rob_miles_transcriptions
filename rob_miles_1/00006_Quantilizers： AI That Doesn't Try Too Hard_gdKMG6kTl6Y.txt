Hi. So way back in the before time, I made a video about maximizers and satisficers. The plan was that was going to be the first half of a two-parter. Now I did script out that second video and shoot it and even start to edit it, and then certain events transpired and I never finished that video. So that's what this is. Part two of a video that I started ages ago, which I think most people have forgotten about. So I do recommend going back and watching that video if you haven't already, or even re-watching it to remind yourself. So I'll put a link to that in the description. And with that, here's part two. Take it away past me. Hi. In the previous video, we looked at utility maximizers, expected utility maximizers, and satisficers using unbounded and bounded utility functions. A powerful utility maximizer with an unbounded utility function is a guaranteed apocalypse. With a bounded utility function, it's better in that it's completely indifferent between doing what we want and disaster. But we can't build that because it needs perfect prediction of the future. So it's more realistic to consider an expected utility maximizer, which is a guaranteed apocalypse, even with a bounded utility function. Now, an expected utility satisficer gets us back up to indifference between good outcomes and apocalypses, but it may want to modify itself into a maximizer, and there's nothing to stop it from doing that. The situation doesn't look great. So let's try looking at something completely different. Let's try to get away from this utility function stuff that seems so dangerous. What if we just tried to directly imitate humans? If we can get enough data about human behavior, maybe we can train a model that, for any given situation, predicts what a human being would do in that scenario. If the model's good enough, you've basically got a human-level AGI, right? It's able to do a wide range of cognitive tasks just like a human can, because it's just exactly copying humans. That kind of system won't do a lot of the dangerous counterproductive things that a maximizer would do, simply because a human wouldn't do them. But I wouldn't exactly call it safe, because a perfect imitation of a human isn't safer than the human it's perfectly imitating, and humans aren't really safe. In principle, a truly safe AGI could be given just about any level of power and responsibility, and it would tend to produce good outcomes, but the same can't really be said for humans. And an imperfect human imitation would almost certainly be even worse. I mean, what are the chances that introducing random errors and inaccuracies to the imitation would just happen to make it more safe rather than less? Still, it does seem like it would be safer than a utility maximizer. At least we're out of guaranteed apocalypse territory. But the other thing that makes this kind of approach unsatisfactory is, a human imitation can't exceed human capabilities by much, because it's just copying them. A big part of why we want AGI in the first place is to get it to solve problems that we can't. You might be able to run the thing faster to allow it more thinking time or something like that, but that's a pretty limited form of superintelligence, and you have to be very careful with anything along those lines, because it means putting the system in a situation that's very different from anything any human being has ever experienced. Your model might not generalize well to a situation so different from anything in its training data, which could lead to unpredictable and potentially dangerous behavior. Relatively recently, a new approach was proposed called quantalizing. The idea is that this lets you combine human imitation and expected utility maximization to hopefully get some of the advantages of both without all of the downsides. It works like this. You have your human imitation model. Given a situation, it can give you a probability distribution over actions that's like, for each of the possible actions you could take in this situation, how likely is it that a human would take that action? So in our stamp collecting example, that would be, if a human were trying to collect a lot of stamps, how likely would they be to do this action? Then you have whatever system you'd use for a utility maximizer that's able to figure out the expected utility of different actions according to some utility function. For any given action, it can tell you how much utility you'd expect to get if you did that. So in our example, that's how many stamps would you expect this action to result in? So for every action, you have these two numbers, the human probability and the expected utility. Quantalizing sort of mixes these together, and you get to choose how they're mixed with a variable that we'll call q. If q is zero, the system acts like an expected utility maximizer. If it's one, the system acts like a human imitation. By setting it somewhere in between, we can hopefully get a quantalizer that's more effective than the human imitation, but not as dangerous as the utility maximizer. So what exactly is a quantalizer? Let's look at the definition in the paper. A q-quantalizer is an agent that, when faced with a decision problem, returns a random action in the top q proportion of some base distribution over actions, sorted by the expected utility achieved if that action is executed. So let's break this down and go through how it works step by step. First, we pick a value for q, the variable that determines how we're going to mix imitation and utility maximization. Let's set it to 0.1 for this example, 10%. Now we take all of the available actions and sort them by expected utility. So on one end, you've got the actions that kick off all of the crazy extreme utility maximizing strategies, you know, killing everyone and turning the whole world into stamps, all the way down through the moderate strategies, like buying some stamps, and down to all of the strategies that do nothing and collect no stamps at all. Then we look at our base distribution over actions. What is that? In the version I'm talking about, we're using the human imitation system's probability distribution over actions for this. So our base distribution is how likely a human is to do each action. That might look something like this. No human is ever going to try the wacky extreme maximizing strategies, so our human imitator gives them a probability of basically zero. Then there are some really good strategies that humans probably won't think of, but they might if they're really smart or lucky. Then a big bump of normal strategies that humans are quite likely to use that tend to do okay. Then tailing off into less and less good strategies and eventually stupider and stupider mistakes that humans are less and less likely to make. Then what we do is we find the point in our action list such that 10% of the probability mass is on the high expected utility side. So that's what Q is really changing. It's where we make this cutoff. Note that it's not 10% of the actions. That would be over here. It's 10% of the probability mass. Then we throw away everything on the right, all the stupid and useless choices. We set them to zero and we keep the top 10%. Now this is no longer a valid probability distribution because it only sums up to 0.1, so we multiply all of these by 10 so that the whole thing sums to 1 again. And that's our final probability distribution which we sample from to get our chosen action. So let's look at some different actions here and see how they do. Consider something like misremember your credit card details and keep trying to order stamps with the wrong number and you can't figure out why it's not working. A human is reasonably likely to do that. Not very likely, but we've all met people who... Point is, a pure human imitation might do that, but the expected utility maximizer can see that this results in very few stamps, so it ends up low on the list and doesn't make the 10% cutoff. So there are lots of mistakes that a human imitation might make that a quantalizer won't. And note that for our stamp collecting utility function, the worst case is zero stamps. But you could imagine with other utility functions, a human imitator could make arbitrarily bad mistakes that a quantalizer would be able to avoid. Now the most common boring human strategies that the human imitator is very likely to use also don't make the cutoff. A 50% quantalizer would have a decent chance of going with one of them, but a 10% quantalizer aims higher than that. The bulk of the probability mass for the 10% quantalizer is in strategies that a human might try that work significantly better than average. So the quantalizer is kind of like a human on a really good day. It uses the power of the expected utility calculation to be more effective than a pure imitation of a human. Is it safe, though? After all, many of the insane maximizing strategies are still in our distribution, with hopefully small but still non-zero probabilities. And in fact, we multiplied them all by 10 when we renormalized. If there's some chance that a human would go for an extreme utility maximizing strategy, the 10% quantalizer is 10 times more likely than that. But the probability will still be small. Unless you've chosen a very small value for q, your quantalizer is much more likely to go for one of the reasonably high-performing human plausible strategies. And what about stability? Satisficers tend to want to turn themselves into maximizers. Does a quantalizer have that problem? Well, the human model should give that kind of strategy a very low probability. A human is extremely unlikely to try to modify themselves into an expected utility maximizer to better pursue their goals. Humans can't really self-modify like that anyway. But a human might try to build an expected utility maximizer rather than trying to become one. That's kind of worrying, since it's a plan that a human definitely might try that would result in extremely high expected utility. So although a quantalizer might seem like a relatively safe system, it still might end up building an unsafe one. So how's our safety meter looking? Well, it's progress. Let's keep working on it. Some of you may have noticed your questions in the YouTube comments being answered by a mysterious bot named Stampy. The way that works is Stampy cross-posts YouTube questions to the RubMiles AI Discord, where me and a bunch of patrons discuss them and write replies. Oh yeah, there's a Discord now for patrons. Thank you to everyone on the Discord who helps reply to comments. And thank you to all of my patrons, all of these amazing people. In this video, I'm especially thanking Timothy Lillicramp. Thank you so much for your support, and thank you all for watching. I'll see you next time. 