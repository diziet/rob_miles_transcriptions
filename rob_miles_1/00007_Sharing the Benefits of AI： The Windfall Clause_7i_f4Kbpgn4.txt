Since the beginning, one of the main goals of the field of artificial intelligence has been to create very capable AI systems, to create systems which match or exceed human capabilities across a wide range of tasks. Given this, it's somewhat surprising just how recently people have started to take the possibility seriously, and to ask, what would happen if we actually succeeded at this challenge? What if we managed it? The answer is, it looks like we have a problem. See, just because the system is very capable, just because it's able to do very well at very difficult things, does not mean that it's trying to do good things. We could easily end up with AI systems that are trying to do things that we really don't want them to do. An AI system that works very effectively in the service of values that are different from human values might be hugely destructive from the perspective of human values. AI safety is an attempt to deal with this problem. How do we create AI systems that are aligned with our goals, that are robustly beneficial, that are trying to do what we want them to do? But then you might ask, well, okay, what if we succeed at that? What if we managed to create AI systems that are very capable, and also are trying to do what the creators wanted them to do? Rather than destroying everything that humans value, as an unaligned system might, such systems would presumably create enormous amounts of value. Might we still have a problem in that circumstance? I think a lot of people would say, yeah, we probably do still have a problem, because what happens when you create enormous amounts of wealth, and that wealth all belongs to a small number of people? One aspect of this is the possibility that automation will result in large-scale unemployment. This has never really happened much in the past, but advanced AI might be an exception. If there are AI systems that can do every task that a human can, it becomes difficult to employ humans to do most tasks. So then you have a situation where you can think of the world as having two types of people. People who make money by selling their labour, and people who make money by owning AI systems. And in this scenario, you've dramatically increased money-making ability for one of those types of people, while dramatically decreasing it for the other type. What happens to people who work for a living, when companies can produce goods and services without employing anyone? What happens to labour, when capital has no need for it? Hands up who's excited to find out? Yeah, me neither. This possible outcome of artificial intelligence transforming the world economy, but in the process creating massive wealth and income inequality, with its associated political and social problems, certainly seems suboptimal. Better than extinction, sure, but still not the outcome that we really want. How do we get the winners in this scenario to share their newly created wealth? Well, some researchers at the Centre for the Governance of AI at the University of Oxford have an idea for something that they think might help, which we're going to talk about in this video. It's called the Windfall Clause. They define it as an ex-ante commitment to share extreme benefits of AI. So basically it's a contract a company can sign that says, if at some point in the future we make huge windfall profits, of the kind that you can really only get by transforming the world economy with AI, we will share some significant percentage of it. So obvious questions first, what do we mean by share it? Well, that could be a variety of things, ranging from having a charitable foundation that uses the money to alleviate inequality according to some set of principles, to just writing everyone a cheque. The other question is, when does this actually happen? What counts as extreme profits? Well, setting an absolute number here doesn't really work, because who knows how far into the future this might happen. And anyway, what we really care about is relative profits, right? So it's defined as profits above a certain percentage of the world's gross domestic product, say 1%. In other words, if your profits are more than 1% of the world's total economic output, you agree to share it. This is a level of profitability that's higher than any company in history, but is actually pretty plausible if your company creates AGI. And in practice, there would probably be several levels of this, where as profits go up as a percentage of world GDP, so does the percentage of the profits that's shared. Kind of like a progressive marginal taxation scheme. Speaking of which, why not just do this with taxes? What advantage does this have over taxation? Well, the first thing to note is that this isn't instead of taxes. This is something they'd agree to over and above whatever taxes governments may impose. But it has some important advantages over taxation. Firstly, governments are not actually great at spending money effectively. Giving money to, for example, the United States federal government is not necessarily the most effective way to spend money to improve the world. This isn't really controversial. A 2011 poll found that Republicans think 52% of their federal tax money is wasted, while Democrats think it's 47%. So if you had a bunch of money and you were looking for the best way to spend it to improve the lives of Americans, give it to the federal government would be pretty low on the list. But actually it's worse than that, because this is a global issue, not a national one, and tax money tends to stay in the country it's collected in. Countries like the US and the UK spend less than 1% of their taxes on foreign aid, and much of that's military aid. Those deep mind creates AGI and starts accounting for more than 1% of the world's gross domestic product, making just absurdly giant amounts of money. Even if the UK taxes those profits very heavily, someone in India or China or the USA is going to see basically none of that money. So you still have this problem of enormous inequality. And the thing is, if an AGI superintelligence turns out to be misaligned and it decides to kill everyone, it's not going to stop at the borders of the country it was created in. Everyone in the world shares pretty much equally in the risks from AGI. But if you just use national taxes, only a small minority of people actually get any share of the benefits. Does that seem right to you? That said, one advantage of taxes is that they're not voluntary, you can actually make companies pay them. But this isn't as big of an advantage as it seems. In practice, it seems that getting companies to pay their taxes is not that easy. It's possible that they'd be more likely to pay something that they actually chose to sign up to. But then, why would they want to sign up in the first place? Why volunteer to give away a load of money? One thing is, the decision makers might be human beings. The executives of these companies certainly talk a big game about wanting to improve the world, and we can't rule out the possibility that they might mean it. What about the shareholders, though? Won't shareholders have something to say about the companies they've invested in agreeing to give away huge amounts of money? Corporate executives do have a legal obligation to act in the interests of their shareholders. But legally, at least, it would probably be fine. The Windfall Clause is a form of corporate philanthropy, and when shareholders have sued executives on the grounds that their philanthropy was a violation of their duty to shareholders, they've won those cases zero out of seven times. But actually, they probably wouldn't even want to, because, in fact, even if the executives and the shareholders are all, hypothetically, complete sociopaths, they still have a good reason to sign something like a Windfall Clause, namely, appearing to not be sociopaths. This is sometimes also called public relations. Signing a Windfall Clause is a clear and legally binding show of goodwill. It improves your company's relationship with the public and with public opinion, which tech companies certainly value. It improves your relationship with governments, which is very important for any large company. And it improves your relationship with your employees, who in this case actually have a lot of bargaining power. Don't forget that, if you're a highly skilled tech company employee, as some of my viewers you have a surprisingly large amount of power over the direction your company takes. Look at things like Project Maven, for example. So from the perspective of a tech company executive, signing a Windfall Clause is a lot of great PR, the kind of thing you'd usually have to pay a lot of money for. But it's all free, for now at least. It only costs anything at all, if you end up with giant windfall profits, which might never happen, and if it does, it's probably a long time in the future, when you've probably already retired. Now this is why it's important that it's an ex-ante agreement, that it's made before we know how things are going to turn out. It's much easier to persuade people to agree to give away something they don't have than something they do. Like, two people might agree to both buy lottery tickets, and to share the winnings if either of them win. This halves how much they'd win, but doubles their chance of winning. Which might be something you'd want to do, depending on your risk tolerance and marginal utility of money. But note that the commitment has to be binding, and it has to be made before the lottery numbers are drawn. Right? You won't have much luck trying to set that up afterwards. But as long as we don't know who, if anyone, is going to be making these giant profits from AI, it could be in everyone's interest to sign this thing, and encourage others to sign it too. For an AI company, in a world where all of the other major AI companies have agreed to something like this, choosing not to join them is effectively saying, screw you guys, there's going to be one winner here, and it's going to be me. I intend to make absurd amounts of money, and not share any of it. And you could do that. But if you do, you might find that others don't want to cooperate with you as much. You might face boycotts and activism. Maybe governments wouldn't feel like giving you the contracts you'd like, or the regulatory environment you'd prefer. You might find it hard to get good collaborators, and to hire and keep the best researchers. You might find that it's actually kind of hard to get things done in the world when you've effectively stuck a big sticky label on your own forehead that reads, I am a dickhead. Can I say that on YouTube? So we want to set this kind of thing up sooner rather than later, because the more uncertainty there is about who, if anyone, is going to get windfall profits, the less likely it is for any individual company to think, well, I don't care about anyone's opinion, I can do this all by myself, without the cooperation of the rest of the world. Hopefully, we can get all of the major players to agree to something like a windfall clause, and that should help mitigate the inequality problems that high-level AI might bring. So how do we help make that happen? Well, if I see something online about 20th century military history, and I'm not sure what to make of it, I ask my uncle, because he's really into that stuff, and I respect his opinion on the subject. I think we've all got people like that for various things. And when it comes to AI, if you're the kind of person who watches this channel, you might be that person for some of the people you know. So if at some point, some AI company signs something like a windfall clause, people might ask you about it, and you can tell them that, yeah, it's pretty legit, that it's not just a publicity stunt. I mean, it probably is a publicity stunt, but it's not just a publicity stunt, right? It probably would be legally binding and would actually help. It's good for people to understand that, because the better the reaction that first company gets, the more likely other companies will be to follow suit, and that's what we want. Generally, this channel focuses more on the technical research that goes into trying to make sure that advances in AI result in good outcomes. But there's also a lot of research on the more human side of things, AI strategy, AI policy, and AI governance research. It's something I don't know as much about, but if there's interest, I can make more videos like this one, exploring the research going on into legal, political, and economic aspects of the future of AI. Is that the kind of thing you'd be interested in seeing? Let me know in the comments. Thank you so much to all my excellent patrons. It's all these wonderful people here. In this video, I'm especially thanking Michael Andrig, who I actually happened to bump into at a conference recently. We had a great talk about his company, which is developing special optical computing hardware for AI. Fascinating stuff. Anyway, thank you, Michael. And also thank you to Colin O'Keefe, the primary author of the Windfall Clause paper, who was kind enough to have a call with me and explain it. I've uploaded that whole conversation for patrons, so do consider becoming one if you want more in-depth information like that. Thank you so much to those who do, and thank you all for watching. I'll see you next time. 