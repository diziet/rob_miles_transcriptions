Hi. What is technology? Don't skip ahead, I promise I'm going somewhere with this. So you could have some kind of definition from a dictionary that's like, technology is machinery and equipment made using scientific knowledge, something like that. But where are the boundaries of the category? What counts? For example, pair of scissors. Technology? I think most people would say no, although it does meet the definition. Perhaps scissors used to be technology, but now I think they're too simple, they're too well understood. I think once we've really nailed something down and figured out all of the details, people stop thinking of it as technology. I think in order to be technology, something has to be complex and unpredictable, maybe even unreliable. YouTube, for example, is definitely technology, as is the device you're watching this on. Okay, why does this matter? I guess part of my point is, exact definitions are really difficult. And this generally isn't much of a problem, because language doesn't really work by exact definitions. Maybe it's hard to specify exactly what we mean when we use a word like technology. But to paraphrase something from the US Supreme Court, you know it when you see it. And that's good enough for most uses. The reason I bring this up is sometimes people ask me about my definition of artificial intelligence. And I actually think that's pretty similar. You could say that AI is about trying to get machines to carry out human cognitive tasks. But then arithmetic is a cognitive task. Does that make a calculator artificial intelligence? You know, sorting a list is a cognitive task. I don't think most people would call that AI. Playing a perfect game of noughts and crosses used to be considered AI, but I don't think we'd call it that these days. So to me, AI is about making machines do cognitive tasks that we didn't think they could do. Maybe it's because it's about making machines do human cognitive tasks. And once machines can do something, we no longer think of it as a human cognitive task. This means that the goalposts are always moving for artificial intelligence. Some people have complained about that, but I think it's pretty reasonable to have that as part of the definition. So that means that the goal of AI research is to continue to expand the range of tasks that computers can handle so they can keep surprising us. It used to be that AI research was all about figuring out and formalizing things so that we could write programs to do them. Things like arithmetic, sorting lists and playing noughts and crosses. These are all in the class of problems that you might call things we can specify well enough to write programs that do them. And for a long time, that was all that we could do. That was the only type of problem we could tackle. But for a lot of problems, that approach is really, really hard. Like consider, how would you write a program that takes an image of a handwritten digit and determines what digit it is? You can formalize the process and try to write a program. It's actually kind of a fun exercise if you want to get to grips with the old school, like computer vision and image processing techniques. And once you've written that program, you can test it using the MNIST dataset, which is a giant collection of correctly labeled small images of digits. What you'll find is, if you do well, then this thing will kind of work. But even the best programs written this way don't work that well. They're not really reliable enough to actually use. Someone is always going to come along with a really blunt pencil and ruin your program's accuracy. And this is still a pretty easy problem. I mean, what if you wanted to do something like letters as well as numbers? Now you have to differentiate between O and 0 and 1 and I and lowercase L. Forget about it. It's never going to work. And even that is a relatively simple problem. What if you're trying to do something like differentiating pictures of cats from pictures of dogs? This whole approach is just not going to work for that. But there is a fact that we can exploit, which is that it's a lot easier to evaluate a solution than to generate a solution for a lot of these problems. I've talked about this before. I couldn't generate a good rocket design myself, but I can tell you that this one needs work. It's easier to write a program to evaluate an output than to write one to produce that output. So maybe it's too hard to write a program that performs the task of identifying handwritten numbers, but it's pretty easy to write a program that evaluates how well a given program does at that task, as long as you have a load of correctly labeled examples. You just keep giving it labeled examples from the dataset and you see how many it gets right. In the same way, maybe you can't write a program that plays an Atari game well, but you can easily write a program that tells you how well you're doing. You just read off the score. And this is where machine learning comes in. It gives you ways to take a program for evaluating solutions and use it to create good solutions. All you need is a dataset with a load of labeled examples or a game with a score or some other way of programmatically evaluating the outputs, and you can train a system that carries out the task. There's a sense in which this is a new programming paradigm. Instead of writing the program itself, you write the reward function or the loss function or whatever, and the training process finds you a set of parameters for your network that perform well according to that function. If you squint, the training process is sort of like a compiler. It's taking code you've written and turning it into an executable that actually performs the task. So in this way, machine learning expands the class of tasks that machines can start to perform. It's no longer just tasks that you can write programs to do, but tasks that you can write programs to evaluate. But if this is a form of programming, it's a very difficult one. Anyone who has programmed in C or C++ will tell you that the two scariest words you can see in a specification are undefined behavior. So how many folks here are a little bit afraid of undefined behavior in their source code? Everybody. And machine learning as a programming paradigm is pretty much entirely undefined behavior. And as a consequence, programs created in this way tend to have a lot of quite serious bugs. And these are things that I've talked about before on the channel. For example, reward gaming, where there's some subtle difference between the reward function you wrote and the actual reward function that you kind of meant to write. And an agent will find ways to exploit that difference to get high reward, to find things it can do, which the reward function you wrote gives a high reward to, but the reward function you meant to write wouldn't have. Or the problem of side effects, where you aren't able to specify in the reward function everything that you care about. And the agent will assume that anything not mentioned in the reward function is of zero value, which can lead to it having large negative side effects. There are a bunch more of these specification problems. And in general, this way of creating programs is a safety nightmare. But also, it still doesn't allow machines to do all of the tasks that we might want them to do. A lot of tasks are just too complex and too poorly defined to write good evaluation functions for. For example, if you have a robot and you want it to scramble you an egg, how do you write a function which takes input from the robot's senses and returns how well the robot is doing at scrambling an egg? That's a very difficult problem. Even something simple like getting a simulated robot to do a backflip is actually pretty hard to specify. What can we do about this? Well, normal reinforcement learning looks like this. You have an agent and an environment. The agent takes actions in the environment. And the environment produces observations and rewards. The rewards are calculated by the reward function. That's where you program in what you want the agent to do. So some researchers tried this with the backflip task. They spent a couple of hours writing a reward function. It looks like this. And the result of training the agent with this reward function looks like this. I guess that's basically a backflip. I've seen better. Something like evaluating a backflip is very hard to specify. But it's not actually hard to do. Like, it's easy to tell if something is doing a backflip just by looking at it. It's just hard to write a program that does that. So what if you just directly put yourself in there? If you just play the part of the reward function? Every time step, you look at the state, and you give the agent a number for how well you think it's doing at backflipping. People have tried that kind of approach, but it has a bunch of problems. The main one is, these systems generally need to spend huge amounts of time interacting with the environment in order to learn even simple things. So you're going to be sitting there saying, no, that's not a backflip. No, that's not a backflip either. That was closer. Nope, that's worse again. And you're going to do this for hundreds of hours? Nobody has time for that. So what can we do? Well, you may notice that this problem is a little bit like identifying handwritten digits, isn't it? We can't figure out how to write a program to do it, and it's too time-consuming to do it ourselves. So why not take the approach that people take with handwritten numbers? Why not learn a reward function? But it's not quite as simple as it sounds. Backflips are harder than handwritten digits, in part because where are you going to get your data from? For digits, we have this data set, MNIST. We have this giant collection of correctly labeled images. We built that by having humans write lots of numbers, scanning them, and then labeling the images. We need humans to do the thing, to provide examples to learn from. We need demonstrations. Now, if you have good demonstrations of an agent performing a task, you can do things like imitation learning and inverse reinforcement learning, which are pretty cool, but they're a subject for a later video. But with backflips, we don't have that. I'm not even sure if I can do a backflip. And that wouldn't help anyway. Wait, really? No, I don't have to do it. No, we don't need a recording of a human backflipping. We need one of this robot backflipping, right? Their physiology is different. But I don't think I could puppeteer the simulated robot to backflip either. That would be like playing co-op on nightmare mode. So we can't demonstrate the task. So what do we do? Well, we go back to the Supreme Court. Technically defining a backflip is hard. Doing a backflip is hard. But I know a backflip when I see one. So we need a setup that learns a good reward function without demonstrations, just by using human feedback, without requiring too much of the human's time. And that's what this paper does. It's called Deep Reinforcement Learning from Human Preferences. And it's actually a collaboration between OpenAI and DeepMind. The paper documents a system that works by reward modeling. If you give it an hour of feedback, it does this. That looks a lot better than two hours of reward function writing. So how does reward modeling work? Well, let's go back to the diagram. In reward modeling, instead of the human writing the reward function, or just being the reward function, we instead replace the reward function with a reward model, implemented as a neural network. So the agent interacts with the environment in the normal way, except the rewards it's getting are coming from the reward model. The reward model behaves just like a regular reward function, in that it gets observations from the environment and gives rewards. But the way it decides those rewards is with a neural network, which is trying to predict what reward a human would give. Okay, how does the reward model learn what reward a human would give? Well, the human provides it with feedback. So the way that works is, the agent is interacting with the environment, trying to learn. And then the system will extract two short clips of the agent flailing about, just a second or two. And it presents those two clips to the human, and the human decides which they like better, which one is more backflippy. And the reward model then uses that feedback in basically the standard supervised learning way. It tries to find a reward function such that, in situations where the human prefers the left clip to the right clip, the reward function gives more reward to the agent in the left clip than the right clip, and vice versa. So which clip gets more reward from the reward model ends up being a good predictor of which clip the human will prefer. Which should mean that the reward model ends up being very similar to the reward function the human really wants. But the thing I like about this is, the whole thing is happening asynchronously. It's all going on at the same time. The agent isn't waiting for the human, it's constantly interacting with the environment, getting rewards from the reward model, and trying to learn, at many times faster than real time. And the reward model isn't waiting either. It's continually training on all of the feedback that it's got so far. When it gets new feedback, it just adds that to the dataset and keeps on training. This means the system is actually training for tens or hundreds of seconds for each second of human time used. So the human is presented with a pair of clips, and gives feedback, which takes just a few seconds to do. And while that's happening, the reward model is updating to better reflect the previous feedback it got. And the agent is spending several minutes of subjective time learning and improving using that slightly improved reward model. So by the time the human is done giving feedback on those clips and it's time for the next pair, the agent has had time to improve. So the next pair of clips will have new, hopefully better behavior for the human to evaluate. This means that it's able to use the human's time quite efficiently. Now to further improve that efficiency, the system doesn't just choose the clips randomly. It tries to select clips where the reward model is uncertain about what the reward should be. Like, there's no point asking for feedback if you're already pretty sure you know what the answer is, right? So this means that the user is most likely to see clips from unusual moments, when the agent has worked out something new and the reward model doesn't know what to make of it. That maximizes the value of the information provided by the human, which improves the speed the system can learn. So what about the usual reinforcement learning safety problems, like negative side effects and reward gaming? You might think that if you use a neural network for your reward signal, it would be very vulnerable to things like reward gaming, since the reward model is just an approximation and we know that neural networks are very vulnerable to adversarial examples and so on. And it's true that if you stop updating the reward model, the agent will quickly learn to exploit it, to find strategies that the reward model scores highly, but the true reward doesn't. But the constant updating of the reward model actually provides pretty good protection against this, and the way that the clips are chosen is part of that. If the agent discovers some crazy new illegitimate strategy to cheat and get high reward, that's going to involve unusual novel behavior, which will make the reward model uncertain. So the human will immediately be shown clips of the new behavior, and if it's reward gaming rather than real progress, the human will give feedback saying, no, that's not what I want. The reward model will update on that feedback and become more accurate, and the agent will no longer be able to use that reward gaming strategy. So the idea is pretty neat, and it seems to have some safety advantages. How well does it actually work? Is it as effective as just programming a reward function? Well, for the backflip, it seems like it definitely is. And it's especially impressive when you note that this is two hours of time to write this reward function, which needs a lot of expertise, compared to under one hour of rating clips, which needs basically no expertise. So this is two hours of expert time versus one hour of novice time. Now they also tried it on the standard Mojoko simulated robotics tasks that have standard reward functions defined for them. Here, it tends to do not quite as well as regular reinforcement learning that's just directly given the reward function, but it tends to do almost as well. And sometimes it even does better, which is kind of surprising. They also tried it on Atari games. Now for those, it needed more feedback because the task is more complex, but again, it tended to do almost as well as just providing the correct reward function for several of the games. Also, there's kind of a fun implementation detail here. They had to modify the games to not show the score, otherwise the agent might learn to just read the score off the screen and use that. They wanted to rely on the feedback. So it seems like reward modeling is not much less effective than just providing a reward function. But the headline, to me, is that they were able to train these agents to do things for which they had no reward function at all. Like the backflip, of course. They also got the cheetah robot to stand on one leg, which is a task I don't think they ever tried to write a reward function for. And in Enduro, which is an Atari game, a racing game, they managed to train the agent using reward modeling to stay level with other cars, even though the game's score rewards you for going fast and overtaking them. And what all this means is that this type of method is, again, expanding the range of tasks machines can tackle. It's not just tasks we can write programs to do, or tasks we can write programs to evaluate, or even tasks we're able to do ourselves. All that's required is that it's easy to evaluate outputs, that you know good results when you see them. And that's a lot of tasks. But it's not everything. Consider, for example, a task like writing a novel. Like, sure, you can read two novels and say which one you liked more. But this system needed 900 comparisons to learn what a backflip is. Even if we assume that writing a novel is no more complicated than that, does that mean comparing 900 pairs of AI-generated novels? And a lot of tasks are like this. What if we want our machine to run a company, or design something complex like a city's transportation system, or a computer chip? We can't write a program that does it. We can't write a program that evaluates it. We can't reliably do it ourselves, enough to make a good data set. We can't even evaluate it ourselves, without taking way too much time and resources. So we're screwed, right? Not necessarily. There are some approaches that might work for these kinds of problems, and we'll talk about them in a later video. I recently realized that my best explanations and ideas tend to come from actual conversations with people. So I've been trying a thing where, for each video, I first have a couple of video calls with Patreon supporters, where I try sort of running through the idea, and seeing what questions people have, and what's not clear, and so on. So I want to say a big thank you to the patrons who helped with this video. You know who you are. I'm especially thanking Jake Ehrlich. And of course, thank you to all of my patrons, who make this whole thing possible with their support. Which reminds me. This video is sponsored by...nobody. No, I actually turned down a sponsorship offer for this video. And I'll admit, I was tempted, because it's a company whose product I've used for like 10 years, and the offer was thousands of pounds. But they wanted me to do this whole 60 second long spiel, and I just thought, no, I don't want to waste people's time with that. And I don't have to, because I've got Patreon. So thank you again to all of you. If you like learning about AI safety more than you like learning about mattresses and VPNs, you might want to consider joining. There's a link in the description. Thanks again for your support. And thank you all for watching. Hi. You hit my knees. 