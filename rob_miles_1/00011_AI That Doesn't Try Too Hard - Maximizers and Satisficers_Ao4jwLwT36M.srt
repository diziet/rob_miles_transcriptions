1
00:00:00,000 --> 00:00:05,280
Hi. So way back when I started this online AI safety videos thing on Computerphile,

2
00:00:05,280 --> 00:00:10,080
I was talking about how you have a problem when you maximize just about any simple utility function.

3
00:00:10,640 --> 00:00:15,600
The example I used was an AI system meant to collect a lot of stamps, which works like this.

4
00:00:15,600 --> 00:00:19,600
The system is connected to the internet, and for all sequences of packets it could send,

5
00:00:19,600 --> 00:00:24,080
it simulates exactly how many stamps would end up being collected after one year if it sent

6
00:00:24,080 --> 00:00:29,120
those packets. It then selects the sequence with the most stamps and sends that. This is what's

7
00:00:29,120 --> 00:00:34,720
called a utility maximizer. And it seems like any utility function you give this kind of system

8
00:00:34,720 --> 00:00:40,880
as a goal, it does it to the max. Utility maximizers tend to take extreme actions. They're

9
00:00:40,880 --> 00:00:45,440
happy to destroy the whole world just to get a tiny increase in the output of their utility function.

10
00:00:45,440 --> 00:00:50,960
So unless their utility function lines up exactly with human values, their actions are pretty much

11
00:00:50,960 --> 00:00:56,240
guaranteed to be disastrous. Intuitively, the issue is that utility maximizers have precisely

12
00:00:56,240 --> 00:01:01,440
zero chill. To anthropomorphize horribly, they seem to have a frantic, obsessive, maniacal

13
00:01:01,440 --> 00:01:05,520
attitude. We find ourselves wanting to say, look, could you just dial it back a little? Could you

14
00:01:05,520 --> 00:01:12,400
just relax? Just a bit? So suppose we want a lot of stamps, but like, not that many. It must be

15
00:01:12,400 --> 00:01:17,840
possible to design a system that just collects a bunch of stamps and then stops, right? How can we

16
00:01:17,840 --> 00:01:22,640
do that? Well, the first obvious issue with the existing design is that the utility function is

17
00:01:22,640 --> 00:01:28,160
unbounded. The more stamps, the better, with no limit. However many stamps it has, it can get more

18
00:01:28,160 --> 00:01:33,360
utility by getting one more stamp. Any world where humans are alive and happy is a world that could

19
00:01:33,360 --> 00:01:38,320
have more stamps in it. So the maximum of this utility function is the end of the world. Let's

20
00:01:38,320 --> 00:01:44,320
say we only really want a hundred stamps. So what if we make a bounded utility function that returns

21
00:01:44,320 --> 00:01:49,520
whichever is smaller, the number of stamps or 100. Getting a hundred stamps from eBay gives a hundred

22
00:01:49,520 --> 00:01:55,040
utility. Converting the whole world into stamps also gives a hundred utility. This function is

23
00:01:55,040 --> 00:01:59,440
totally indifferent between all outcomes that contain more than a hundred stamps. So what does

24
00:01:59,440 --> 00:02:04,080
a maximizer of this utility function actually do? Now the system's behavior is no longer really

25
00:02:04,080 --> 00:02:08,960
specified. It will do one of the things that results in a hundred utility, which includes

26
00:02:08,960 --> 00:02:12,480
a bunch of perfectly reasonable behaviors that the programmer would be happy with,

27
00:02:12,480 --> 00:02:16,640
and a bunch of apocalypses, and a bunch of outcomes somewhere in between. If you select

28
00:02:16,640 --> 00:02:21,440
at random from all courses of action that result in at least a hundred stamps, what proportion of

29
00:02:21,440 --> 00:02:27,600
those are actually acceptable outcomes for humans? I don't know. Probably not enough. This is still a

30
00:02:27,600 --> 00:02:31,840
step up though, because the previous utility function was guaranteed to kill everyone,

31
00:02:31,840 --> 00:02:36,640
and this new one has at least some probability of doing the right thing. But actually, of course,

32
00:02:36,640 --> 00:02:42,240
this utility maximizer concept is too unrealistic, even in the realm of talking about hypothetical

33
00:02:42,320 --> 00:02:46,800
agents in the abstract. In the thought experiment, our stamp collector system is able to know,

34
00:02:46,800 --> 00:02:51,360
with certainty, exactly how many stamps any particular course of action will result in.

35
00:02:51,360 --> 00:02:54,880
But you just can't simulate the world that accurately. It's more than just

36
00:02:54,880 --> 00:03:00,640
computationally intractable, it's probably not even allowed by physics. Pure utility maximization

37
00:03:00,640 --> 00:03:06,000
is only available for very simple problems, where everything is deterministic and fully known.

38
00:03:06,000 --> 00:03:10,480
If there's any uncertainty, you have to do expected utility maximizing.

39
00:03:10,480 --> 00:03:14,560
This is pretty straightforwardly how you'd expect to apply uncertainty to this situation.

40
00:03:14,560 --> 00:03:19,200
The expected utility of a choice is the utility you'd expect to get from it on average.

41
00:03:19,200 --> 00:03:23,840
So like, suppose there's a button that flips a coin, and if it's tails you get 50 stamps,

42
00:03:23,840 --> 00:03:29,120
and if it's heads you get 150 stamps. In expectation, this results in 100 stamps,

43
00:03:29,120 --> 00:03:32,560
right? It never actually returns 100, but on average that's what you get.

44
00:03:32,560 --> 00:03:37,760
That's the expected number of stamps. To get the expected utility, you just apply your utility

45
00:03:37,760 --> 00:03:42,480
function to each of the outcomes before you do the rest of the calculation. So if your utility

46
00:03:42,480 --> 00:03:47,600
function is just how many stamps do I get, then the expected utility of the button is 100. But

47
00:03:47,600 --> 00:03:52,960
if your utility function is capped at 100, for example, then the outcome of winning 150 stamps

48
00:03:52,960 --> 00:03:58,000
is now only worth 100 utility. So the expected utility of the button is only 75.

49
00:03:58,560 --> 00:04:02,720
Now suppose there were a second button that gives either 80 or 90 stamps,

50
00:04:02,720 --> 00:04:07,360
again with 50-50 probability. This gives 85 stamps in expectation,

51
00:04:07,360 --> 00:04:12,080
and since none of the outcomes are more than 100, both of the functions value this button at 85

52
00:04:12,080 --> 00:04:17,280
utility. So this means the agent with the unbounded utility function would prefer the first button

53
00:04:17,280 --> 00:04:21,680
with its expected 100 stamps, but the agent with the bounded utility function would prefer the

54
00:04:21,680 --> 00:04:27,280
second button, since its expected utility of 85 is higher than the first button's expected utility

55
00:04:27,280 --> 00:04:32,240
of 75. This makes the bounded utility function feel a little safer. In this case,

56
00:04:32,240 --> 00:04:35,760
it actually makes the agent prefer the option that results in fewer stamps,

57
00:04:35,760 --> 00:04:38,720
because it just doesn't care about any stamps past 100.

58
00:04:38,720 --> 00:04:43,360
In the same way, let's consider some risky extreme stamp collecting plan.

59
00:04:43,360 --> 00:04:47,600
This plan is pretty likely to fail, and in that case the agent might be destroyed and get no

60
00:04:47,600 --> 00:04:52,320
stamps. But if the plan succeeds, the agent could take over the world and get a trillion stamps.

61
00:04:52,320 --> 00:04:56,240
An agent with an unbounded utility function would rate this plan pretty highly.

62
00:04:56,240 --> 00:04:59,440
The huge utility of taking over the world makes the risk worth it.

63
00:05:00,000 --> 00:05:04,960
But the agent with the bounded utility function doesn't prefer a trillion stamps to 100 stamps.

64
00:05:04,960 --> 00:05:09,840
It only gets 100 utility either way, so it would much prefer a conservative strategy

65
00:05:09,840 --> 00:05:13,840
that just gets 100 stamps with high confidence. But how does this kind of system behave in the

66
00:05:13,840 --> 00:05:18,960
real world, where you never really know anything with absolute certainty? The pure utility maximizer

67
00:05:18,960 --> 00:05:24,240
that effectively knows the future can order 100 stamps and know that it will get 100 stamps.

68
00:05:24,240 --> 00:05:28,560
But the expected utility maximizer doesn't know for sure. The seller might be lying,

69
00:05:28,560 --> 00:05:33,600
the package might get lost, and so on. So the expected utility of ordering 100 stamps is a

70
00:05:33,600 --> 00:05:38,880
bit less than 100. If there's a 1% chance that something goes wrong and we get 0 stamps,

71
00:05:38,880 --> 00:05:44,320
then our expected utility is only 99. That's below the limit of 100, so we can improve that

72
00:05:44,320 --> 00:05:49,440
by ordering some extras to be on the safe side. Maybe we order another 100. Now our expected

73
00:05:49,440 --> 00:05:58,400
utility is 99.99. Still not 100, so we should order some more, just in case. Now we're at 99.9999.

74
00:05:58,400 --> 00:06:04,160
The expected value of a utility function that's bounded at 100 can never actually hit 100. You

75
00:06:04,160 --> 00:06:08,320
can always become slightly more certain that you've got at least 100 stamps. Better turn the

76
00:06:08,320 --> 00:06:12,960
whole world into stamps, because hey, you never know. So an expected utility maximizer with a

77
00:06:12,960 --> 00:06:17,520
bounded utility function ends up pretty much as dangerous as one with an unbounded utility

78
00:06:17,520 --> 00:06:23,840
function. Okay, what if we try to limit it from both sides? Like, you get 100 utility if you have

79
00:06:23,840 --> 00:06:29,200
100 stamps, and 0 otherwise. Now it's not going to collect a trillion stamps just to be sure,

80
00:06:29,200 --> 00:06:34,640
it will collect exactly 100 stamps. But it's still incentivized to take extreme actions to be sure

81
00:06:34,640 --> 00:06:39,600
that it really does have 100. Like turning the whole world into elaborate, highly redundant,

82
00:06:39,600 --> 00:06:44,240
stamp counting and recounting machinery, getting slightly more utility every time it checks again.

83
00:06:44,240 --> 00:06:50,000
It seems like whatever we try to maximize, it causes problems. So maybe we could try not

84
00:06:50,000 --> 00:06:54,640
maximizing. Maybe we could try what's called satisficing. Rather than trying to get our

85
00:06:54,640 --> 00:06:59,840
utility function to return as high a value as possible in expectation, what if we set a threshold

86
00:06:59,840 --> 00:07:04,160
and accept any strategy that passes that threshold? In the case of the stamp collector, that would

87
00:07:04,160 --> 00:07:09,280
look like, look through possible ways you could send out packets, calculate how many stamps you'd

88
00:07:09,280 --> 00:07:13,680
expect to collect on average with each strategy, and as soon as you hit one that you expect to get

89
00:07:13,680 --> 00:07:19,040
at least 100 stamps, just go with that one. This satisficer seems to get us to about where we were

90
00:07:19,040 --> 00:07:23,920
with the pure utility maximizer with a bounded utility function. It's not clear exactly what it

91
00:07:23,920 --> 00:07:29,280
will do, except that it will do one of the things that results in more than 100 stamps in expectation,

92
00:07:29,280 --> 00:07:33,360
which again includes a lot of sensible behaviors, and a lot of apocalypses, and a lot of things

93
00:07:33,360 --> 00:07:38,320
somewhere in between. Since the system implements the first satisfactory strategy it finds,

94
00:07:38,320 --> 00:07:42,640
the specific behavior depends on the order in which it considers the options. What order might

95
00:07:42,640 --> 00:07:48,160
it use? Well, one obvious approach is to go with the simplest or shortest plans first. After all,

96
00:07:48,160 --> 00:07:53,040
any plan that takes over the world probably requires much more complexity than just ordering

97
00:07:53,040 --> 00:07:58,720
some stamps on eBay. But consider the following plan. Get into your own source code and change

98
00:07:58,720 --> 00:08:04,800
yourself from a satisficer into a maximizer. All you're doing there is changing a few lines of code

99
00:08:04,800 --> 00:08:09,600
on your own system, so this is a pretty simple plan that's likely to be considered fairly early on.

100
00:08:09,600 --> 00:08:14,400
It might not be simpler than just ordering some stamps, but that's not much reassurance. The more

101
00:08:14,400 --> 00:08:18,640
challenging the task we give our AGI, the more likely it is that it will hit on this kind of

102
00:08:18,640 --> 00:08:23,360
self-modification strategy before any legitimate ones. And the plan certainly satisfies the search

103
00:08:23,360 --> 00:08:29,200
criteria. If you change yourself into a maximizer, that maximizer will predictably find and implement

104
00:08:29,200 --> 00:08:33,520
some plan that results in a lot of stamps. So you can tell that the expected stamp output of the

105
00:08:33,520 --> 00:08:38,720
become-a-maximizer plan is satisfactorily high even without knowing what plan the maximizer

106
00:08:38,720 --> 00:08:43,840
will actually implement. So satisficers kind of want to become maximizers, which means that being

107
00:08:43,840 --> 00:08:52,080
a satisficer is unstable. As a safety feature, it tends to uninstall itself. So to recap, a powerful

108
00:08:52,080 --> 00:08:57,040
utility maximizer with an unbounded utility function is a guaranteed apocalypse. With a

109
00:08:57,040 --> 00:09:01,840
bounded utility function, it's better in that it's completely indifferent between doing what we want

110
00:09:01,840 --> 00:09:06,480
and disaster. But we can't build that because it needs perfect prediction of the future, so it's

111
00:09:06,480 --> 00:09:11,440
more realistic to consider an expected utility maximizer, which is a guaranteed apocalypse even

112
00:09:11,440 --> 00:09:17,760
with a bounded utility function. Now, an expected utility satisficer gets us back up to indifference

113
00:09:17,760 --> 00:09:22,800
between good outcomes and apocalypses, but it may want to modify itself into a maximizer,

114
00:09:22,800 --> 00:09:26,480
and there's nothing to stop it from doing that. So currently, things aren't looking great. But

115
00:09:26,480 --> 00:09:30,000
we're not done. People have thought of more approaches, and we'll talk about some of those

116
00:09:30,000 --> 00:09:43,360
in the next video. I want to end the video with a big thank you to all of my wonderful patrons.

117
00:09:43,360 --> 00:09:50,400
That's all of these great people right here. In this video, I'm especially thanking Simon Strandgaard.

118
00:09:50,400 --> 00:09:54,240
Thank you so much. You know, thanks to your support, I was able to buy this boat.

119
00:09:54,240 --> 00:10:00,000
Well, this... I bought a green screen, actually. But I like it because it lets me make videos like

120
00:10:00,000 --> 00:10:04,320
this one that I put up on my second channel, where I use GPT-2 to generate a bunch of fake

121
00:10:04,320 --> 00:10:08,800
YouTube comments and read them. That video ties in with three other videos I made with Computerphile,

122
00:10:08,800 --> 00:10:13,120
talking about the ethics of releasing AI systems that might have malicious uses.

123
00:10:13,120 --> 00:10:16,960
So you can check all of those out. There's links in the description. Thank you again to my patrons,

124
00:10:16,960 --> 00:10:19,920
and thank you all for watching. I'll see you next time.

