Hi. So way back when I started this online AI safety videos thing on Computerphile, I was talking about how you have a problem when you maximize just about any simple utility function. The example I used was an AI system meant to collect a lot of stamps, which works like this. The system is connected to the internet, and for all sequences of packets it could send, it simulates exactly how many stamps would end up being collected after one year if it sent those packets. It then selects the sequence with the most stamps and sends that. This is what's called a utility maximizer. And it seems like any utility function you give this kind of system as a goal, it does it to the max. Utility maximizers tend to take extreme actions. They're happy to destroy the whole world just to get a tiny increase in the output of their utility function. So unless their utility function lines up exactly with human values, their actions are pretty much guaranteed to be disastrous. Intuitively, the issue is that utility maximizers have precisely zero chill. To anthropomorphize horribly, they seem to have a frantic, obsessive, maniacal attitude. We find ourselves wanting to say, look, could you just dial it back a little? Could you just relax? Just a bit? So suppose we want a lot of stamps, but like, not that many. It must be possible to design a system that just collects a bunch of stamps and then stops, right? How can we do that? Well, the first obvious issue with the existing design is that the utility function is unbounded. The more stamps, the better, with no limit. However many stamps it has, it can get more utility by getting one more stamp. Any world where humans are alive and happy is a world that could have more stamps in it. So the maximum of this utility function is the end of the world. Let's say we only really want a hundred stamps. So what if we make a bounded utility function that returns whichever is smaller, the number of stamps or 100. Getting a hundred stamps from eBay gives a hundred utility. Converting the whole world into stamps also gives a hundred utility. This function is totally indifferent between all outcomes that contain more than a hundred stamps. So what does a maximizer of this utility function actually do? Now the system's behavior is no longer really specified. It will do one of the things that results in a hundred utility, which includes a bunch of perfectly reasonable behaviors that the programmer would be happy with, and a bunch of apocalypses, and a bunch of outcomes somewhere in between. If you select at random from all courses of action that result in at least a hundred stamps, what proportion of those are actually acceptable outcomes for humans? I don't know. Probably not enough. This is still a step up though, because the previous utility function was guaranteed to kill everyone, and this new one has at least some probability of doing the right thing. But actually, of course, this utility maximizer concept is too unrealistic, even in the realm of talking about hypothetical agents in the abstract. In the thought experiment, our stamp collector system is able to know, with certainty, exactly how many stamps any particular course of action will result in. But you just can't simulate the world that accurately. It's more than just computationally intractable, it's probably not even allowed by physics. Pure utility maximization is only available for very simple problems, where everything is deterministic and fully known. If there's any uncertainty, you have to do expected utility maximizing. This is pretty straightforwardly how you'd expect to apply uncertainty to this situation. The expected utility of a choice is the utility you'd expect to get from it on average. So like, suppose there's a button that flips a coin, and if it's tails you get 50 stamps, and if it's heads you get 150 stamps. In expectation, this results in 100 stamps, right? It never actually returns 100, but on average that's what you get. That's the expected number of stamps. To get the expected utility, you just apply your utility function to each of the outcomes before you do the rest of the calculation. So if your utility function is just how many stamps do I get, then the expected utility of the button is 100. But if your utility function is capped at 100, for example, then the outcome of winning 150 stamps is now only worth 100 utility. So the expected utility of the button is only 75. Now suppose there were a second button that gives either 80 or 90 stamps, again with 50-50 probability. This gives 85 stamps in expectation, and since none of the outcomes are more than 100, both of the functions value this button at 85 utility. So this means the agent with the unbounded utility function would prefer the first button with its expected 100 stamps, but the agent with the bounded utility function would prefer the second button, since its expected utility of 85 is higher than the first button's expected utility of 75. This makes the bounded utility function feel a little safer. In this case, it actually makes the agent prefer the option that results in fewer stamps, because it just doesn't care about any stamps past 100. In the same way, let's consider some risky extreme stamp collecting plan. This plan is pretty likely to fail, and in that case the agent might be destroyed and get no stamps. But if the plan succeeds, the agent could take over the world and get a trillion stamps. An agent with an unbounded utility function would rate this plan pretty highly. The huge utility of taking over the world makes the risk worth it. But the agent with the bounded utility function doesn't prefer a trillion stamps to 100 stamps. It only gets 100 utility either way, so it would much prefer a conservative strategy that just gets 100 stamps with high confidence. But how does this kind of system behave in the real world, where you never really know anything with absolute certainty? The pure utility maximizer that effectively knows the future can order 100 stamps and know that it will get 100 stamps. But the expected utility maximizer doesn't know for sure. The seller might be lying, the package might get lost, and so on. So the expected utility of ordering 100 stamps is a bit less than 100. If there's a 1% chance that something goes wrong and we get 0 stamps, then our expected utility is only 99. That's below the limit of 100, so we can improve that by ordering some extras to be on the safe side. Maybe we order another 100. Now our expected utility is 99.99. Still not 100, so we should order some more, just in case. Now we're at 99.9999. The expected value of a utility function that's bounded at 100 can never actually hit 100. You can always become slightly more certain that you've got at least 100 stamps. Better turn the whole world into stamps, because hey, you never know. So an expected utility maximizer with a bounded utility function ends up pretty much as dangerous as one with an unbounded utility function. Okay, what if we try to limit it from both sides? Like, you get 100 utility if you have 100 stamps, and 0 otherwise. Now it's not going to collect a trillion stamps just to be sure, it will collect exactly 100 stamps. But it's still incentivized to take extreme actions to be sure that it really does have 100. Like turning the whole world into elaborate, highly redundant, stamp counting and recounting machinery, getting slightly more utility every time it checks again. It seems like whatever we try to maximize, it causes problems. So maybe we could try not maximizing. Maybe we could try what's called satisficing. Rather than trying to get our utility function to return as high a value as possible in expectation, what if we set a threshold and accept any strategy that passes that threshold? In the case of the stamp collector, that would look like, look through possible ways you could send out packets, calculate how many stamps you'd expect to collect on average with each strategy, and as soon as you hit one that you expect to get at least 100 stamps, just go with that one. This satisficer seems to get us to about where we were with the pure utility maximizer with a bounded utility function. It's not clear exactly what it will do, except that it will do one of the things that results in more than 100 stamps in expectation, which again includes a lot of sensible behaviors, and a lot of apocalypses, and a lot of things somewhere in between. Since the system implements the first satisfactory strategy it finds, the specific behavior depends on the order in which it considers the options. What order might it use? Well, one obvious approach is to go with the simplest or shortest plans first. After all, any plan that takes over the world probably requires much more complexity than just ordering some stamps on eBay. But consider the following plan. Get into your own source code and change yourself from a satisficer into a maximizer. All you're doing there is changing a few lines of code on your own system, so this is a pretty simple plan that's likely to be considered fairly early on. It might not be simpler than just ordering some stamps, but that's not much reassurance. The more challenging the task we give our AGI, the more likely it is that it will hit on this kind of self-modification strategy before any legitimate ones. And the plan certainly satisfies the search criteria. If you change yourself into a maximizer, that maximizer will predictably find and implement some plan that results in a lot of stamps. So you can tell that the expected stamp output of the become-a-maximizer plan is satisfactorily high even without knowing what plan the maximizer will actually implement. So satisficers kind of want to become maximizers, which means that being a satisficer is unstable. As a safety feature, it tends to uninstall itself. So to recap, a powerful utility maximizer with an unbounded utility function is a guaranteed apocalypse. With a bounded utility function, it's better in that it's completely indifferent between doing what we want and disaster. But we can't build that because it needs perfect prediction of the future, so it's more realistic to consider an expected utility maximizer, which is a guaranteed apocalypse even with a bounded utility function. Now, an expected utility satisficer gets us back up to indifference between good outcomes and apocalypses, but it may want to modify itself into a maximizer, and there's nothing to stop it from doing that. So currently, things aren't looking great. But we're not done. People have thought of more approaches, and we'll talk about some of those in the next video. I want to end the video with a big thank you to all of my wonderful patrons. That's all of these great people right here. In this video, I'm especially thanking Simon Strandgaard. Thank you so much. You know, thanks to your support, I was able to buy this boat. Well, this... I bought a green screen, actually. But I like it because it lets me make videos like this one that I put up on my second channel, where I use GPT-2 to generate a bunch of fake YouTube comments and read them. That video ties in with three other videos I made with Computerphile, talking about the ethics of releasing AI systems that might have malicious uses. So you can check all of those out. There's links in the description. Thank you again to my patrons, and thank you all for watching. I'll see you next time. 