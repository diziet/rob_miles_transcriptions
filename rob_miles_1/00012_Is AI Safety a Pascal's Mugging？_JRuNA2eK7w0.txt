Hi. Today we're going to talk about Pascal's wager and Pascal's mugging. This is necessarily going to touch on religious topics, pretty heavily actually, and I'm just going to say at the beginning of the video that, personally, I don't believe that any god or gods have ever existed, and I'm not going to pretend otherwise in this video, so be forewarned if that's likely to bother you. So, as you may know, Pascal was a 17th century philosopher who was interested in, amongst other things, the question of the existence of the Christian god. Various philosophers at the time were arguing that god didn't exist, and there was a lot of discussion going on about the various kinds of evidence for and against god in the world. But there's this thing that's quite common when people think about religious questions, where it feels sort of unsatisfying to talk about worldly evidence, as if you were considering some everyday question. There's a feeling that these supernatural concepts are very grand and mysterious, they're special, and so just straightforwardly considering the evidence for and against god is not the right way to do things. This is, of course, encouraged by religious thinking. The idea that some hypotheses aren't subject to the usual rules of evidence and logic is pretty appealing if you want to advocate for an idea that doesn't fare very well by those standards. I suspect Pascal may have felt something like that, because his position was that reason has nothing to say about the question of whether or not god exists. It's sort of an unknowable thing. And instead, he proposed that we should make a wager. We should think about it like this. There are two possibilities. Either the Christian god exists or he doesn't, and reason gives us no way to choose between those. We have two options available to us. Either we can live according to god's laws and act as though we believe, or we can not do that. So we have a sort of payoff matrix here, with four sections. If god exists and we believe in him, then we get infinite reward in heaven. If god exists and we don't believe in him, we get infinite punishment in hell. If god doesn't exist and we believe in him, then we pay some costs, you know, there are some rules we have to follow and so on. And if he doesn't exist and we don't believe in him, then maybe we get a few perks from not believing, like having a lion on Sundays, and being right. Pascal's point is that this payoff matrix is completely dominated by the case in which god exists, because we're talking about infinite rewards and infinite punishments, as opposed to the other case with these very finite costs and benefits. So regardless of the evidence, Pascal argues we should believe in god, or at least act like it, because it's just the sensible bet to make. This is really kind of a nice argument from Pascal's perspective, because it doesn't need evidence at all. No finite earthly evidence can outweigh infinite supernatural payoffs. It feels like the kind of clean, abstract reasoning that you're supposed to do when thinking about the supernatural. All of this hard work looking at history and psychology and science and trying to figure out where the ideas of religion come from and whether our world seems like the kind of world with a god in it, it's long-winded, confusing, it's just messy. But here we just have a clean argument that says we should believe in god, or at least act like it, and that seems very neat. No evidence required. So consider now, Pascal is walking down the street and he's stopped by a shady-looking man who says, give me your wallet. I would prefer not to. Do you even have a weapon? No, UK laws are very strict about that, but I don't need one, because I'm god. You're god? Yep, I'm god. And christianity got a lot of things wrong about me, my forgiving nature, my infinite mercy, and so on. But the infinite torture thing is legit, and if you don't give me your wallet right now, I will torture you for eternity in the afterlife. Now if you're Pascal, you're in kind of a difficult situation, because the fact that it seems very unlikely that this mugger actually is god is not meant to be part of your calculation. Your argument is one of pure logic, it works independently of any evidence. You didn't need to look for evidence of the christian god, and you don't need to look for evidence that this mugger is god either. So you kind of have to give him your wallet. And now you're really in trouble, because of course when this gets out, there's going to be a line around the block of Erzat's deities asking for handouts. How are you going to deal with this endless stream of fizzy gods? Well one thing you can do is you can play the muggers off against one another. You can bring in two of them and say, listen, you say that you're going to torture me forever if I don't give you my wallet, and you say the same thing. I only have one wallet, so it looks like whatever I do I'm going to be tortured forever by somebody. And if I'm going to be infinitely tortured anyway, well two times infinity is still just infinity, so I may as well hang on to the wallet. Now get the hell out of my house. All right, next two. No doubt these self-proclaimed deities may try to argue that they have some reason why they are in fact a real deity and this other mugger is just a random guy who's pretending, but that's all worldly evidence which you've decided isn't required for your argument. And the muggers don't really want you to become interested in evidence because, well, the evidence points very strongly towards none of them being real gods. So this is a better position to be in. You're still spending a lot of your time arguing with charlatans, but at least you still have your wallet. And you don't actually have to pair them up against each other, right? You can just make up a deity. When someone comes in pretending to be a god, you can say, oh well, there's this other god who demands exactly the opposite thing from you. A goddess, actually, and she's very powerful. Uh, but she goes to a different school, you would know her. Really? Yeah, she lives in Canada. She's omnipresent, obviously, but she lives in Canada. Anyway, she says that I'm not to give you the wallet, and if I do, then she'll torture me forever in the afterlife. Dang. Yeah. So you can solve a lot of these problems by inventing gods arbitrarily. And of course, this applies just as well to the original version of Pascal's Wager. Because although it's implied that this payoff matrix has enumerated all of the possibilities, and in a sense it has, the Christian god either exists or he doesn't, nonetheless, those may not be the only things that affect the payoffs. For any given god, you can take the god down, flip it, and reverse it, and say, what about anti-god, who wants me to do the exact opposite and promises the exact opposite consequences? Now you can see that they cancel out. Somebody who's arguing for the existence of the first god might say, okay, but this anti-god is just made up, which I mean, yeah, it is. It's true that the situation isn't really symmetrical. Someone might think god is more likely than anti-god because of evidence from the Bible, and there's no such thing as the anti-Bible, and so on. The point is, though, we're back to talking about the evidence. That's really the problem I have with Pascal's Wager, the way it uses infinite costs and infinite benefits to completely override our necessarily finite evidence. But what if the costs and benefits aren't infinite, just very, very large? That ends up being a much more interesting question. On one end of the scale, we can easily name numbers so large that no amount of evidence anyone could ever actually gather in their lifetime could have an impact on the conclusion. We can specify costs and benefits that are technically finite, but that still feel very much like a Pascal's Wager. On the other end of the scale, if you come across a bet that pays out 10,000 to 1 on an event with a probability of 1 in 100, that's a very good bet to take. Someone could complain that it's a Pascal's Wager to bet on an unlikely outcome just because the payoff is so high, but if you take enough bets like that, you're sure to become very rich. In the same way, if there's a button which has a 1 in a million chance of starting a global thermonuclear war, it's still worth expending significant resources to stop that button being pressed. 1 in a million isn't much, but the cost of a nuclear war is really high. I don't think that's a Pascal's Wager either. The difference seems to come in somewhere in the gap between very small probabilities of very large costs and benefits and really extremely small probabilities of near infinite costs and benefits. So why are we talking about this? What does this have to do with AI safety? Well, suppose somebody stops you in the street and says, hey, if we ever create powerful artificial general intelligence, then that will have a tremendous impact. In fact, the future of the whole of humanity hinges on it. If we get it right, we could have human flourishing for the rest of time. If we get it wrong, we could have human extinction or worse. Regardless of how likely superhuman AGI is, the potential impact is so high that it makes AI safety research tremendously important. So, uh, give me your wallet. It's been claimed by some that this is more or less what AI safety as a field is doing. This is kind of an interesting point. As AI safety advocates, are we victims of Pascal's mugging? Or are we in fact Pascal's muggers ourselves? Well, if people were saying these AI risks may be extremely unlikely, but the consequences of getting AI wrong are so huge that it's worth spending a lot of resources on regardless of the probabilities, so we don't even need to consider the evidence. Well, I would consider that to be a Pascal's Wager style bad argument. But what I actually hear is not that. What I hear is more like, look, we're not completely sure about this. It's quite possible that we're wrong, but considering the enormity of what's at stake, it's definitely worth allocating more resources to AI safety than we currently are. That sounds pretty similar, but that's mostly because natural language is extremely vague when talking about uncertainty. There's an enormous difference in the probabilities being talked about in the same way. If when you talk to AI safety researchers, they said things like, well, I think the chance of any of this ever being relevant are really extremely tiny. It seems more or less impossible to me, but I've decided to work on it anyway, because the potential costs and benefits are so unimaginably vast. Then yeah, I'd be a little concerned that they might be victims of Pascal's mugging. But when you ask AI safety researchers, they don't think that the probability of their work ever becoming relevant is very tiny. They don't necessarily think it's huge either, maybe not even more than 50%, but it's not so small that you have to rely on the unimaginable vastness of the consequences in order to make the argument. To borrow a metaphor from Stuart Russell, suppose you're part of a team working on building a bridge, and you believe you've found a flaw in the design that could cause the structure to fail catastrophically. Maybe the disaster would only happen if there's a very rare combination of weather conditions, and there's only a one in a hundred chance that those conditions will ever happen during the course of the bridge's expected lifespan. And further suppose that you're not completely sure of your calculations because this kind of thing is complicated. Maybe you only give yourself a 40% chance of being right about this. So you go to the civil engineer in charge of the project and you say, I think there's a serious risk with this bridge design. Do you think the bridge is going to collapse? Probably not, no. But I'm about 40% sure that there's a design flaw which would give this bridge a one in one hundred chance of catastrophic failure. So you're telling me that in the event of a scenario which is very unlikely to happen, the bridge might collapse, and you yourself admit that you're more likely to be wrong than right about this? Stop wasting my time. But if the bridge collapses, it could kill a lot of people. I think this is a Pascal's mugging. Don't try to get me to ignore the low probabilities just by threatening very large consequences. Obviously that isn't what would happen. No civil engineer is going to accept a 1 in 250 chance of catastrophic failure for a major piece of infrastructure, because civil engineers have a healthy organizational culture around safety. What it comes down to, again, is the difference between different levels of improbability. The chance of an AGI catastrophe may not be very big, but it's much, much larger than the chance that a mugger is actually a god. And what about our anti-god tactic? Finding the opposite risk? Does that still work? Like, what if we consider the possibility that there's another opposite design flaw in the bridge, which might cause it to collapse unless we don't spend extra time evaluating the safety of the design? What? Just look at the schematic, would you? And what if working on AI safety actually ends up making the risks worse somehow? I think this actually is worth considering. Unintended consequences are a real problem, after all. Speaking generally, there's a clear argument that the future is very important, and that we're probably able to have a very big impact on it, but it's hard to know for sure whether that impact will be positive or negative for any given course of action. Prediction is very difficult, as they say, especially about the future. And the further into the future we look, the more difficult it gets. Like, imagine if you lived in the year 1900, and you had some insight that made you realize that nuclear weapons were possible, and nuclear war was a risk. You'd hope that you could use that understanding to reduce the risk, but it would certainly be possible to make things worse by accident. In the case of AI safety, though, I don't see that being anywhere near as much of a concern. We're heading towards AI regardless, and it seems very unlikely that thinking about safety would be more dangerous than not thinking about safety. It's definitely possible to make things worse while trying to make them better, but you can't avoid that by never trying to make things better. I guess my point is, there's just no getting around the messy, confusing, complicated work of looking at and thinking about the evidence. Any argument that doesn't rely on the evidence will work equally well, whatever the truth is. So at the end of the day, that kind of thing isn't going to give you an answer. You have to just stare at the bridge design and really think. You have to actually do the engineering. And that's something I'm trying to get across with this channel. You won't find me saying, never mind the evidence, AI safety is important because it could have huge consequences. What I do on this channel is, I try to show you some of the evidence and some of the arguments, and let you think about the situation and draw your own conclusions. It can be tricky and involved. It requires some thought, but it has the advantage of being the only thing that has any chance of actually getting the right answer. So thanks for watching. As my wonderful patrons will know, the Alignment Newsletter is a weekly publication from Rohin Shah, which I read every week to stay up to date with what's going on in AI safety. And now I'm recording myself reading it out and publishing that as the Alignment Newsletter podcast. It's aimed at researchers, so it's a fair bit more technical than this channel. But if you're interested in getting 15 minutes of AI safety news in your ear holes each week, check the link in the description. I'm never going to put ads or sponsors on that podcast, and that's largely thanks to my patrons. In this video, I'm especially thanking Chris Canal. Thank you so much for your support, Chris. Thank you to all of my patrons, and thank you for watching. I'll see you next time. Ugh, costume changes. 