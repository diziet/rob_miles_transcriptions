Hi. A quick foreword before we get started. I wrote the script for this video in maybe February or March of last year in response to an article Steven Pinker wrote for Popular Science magazine, which was basically an extract from his new book Enlightenment Now. I ended up not publishing that video because I thought it was kind of mean-spirited and sarcastic in places, and I didn't want to be that kind of YouTuber. I figured Pinker is on the side of the angels, as it were, and there'd be a retraction or correction issued before long, like we heard from Neil deGrasse Tyson. But it's now been almost a year, and Pinker has stood by this work in response to criticism, so here we go. Steven Pinker is a cognitive psychologist and an award-winning popular science author. His books tend to be carefully researched and well thought out, books which have really influenced the public conversation about various scientific topics. How the mind works, the relative roles of nature and nurture in human psychology and behavior, and society and social progress. His latest book is titled Enlightenment Now, which advances, amongst other things, one of Pinker's main points, a point which I think is true and important and not widely enough believed, which is that things are on the whole good and getting better. Everybody's always talking about how everything is terrible and society is on the wrong path and everything is going down the drain, but if you look at almost anything you can measure, it's getting better. Crime is down, violence is down, war is down, infant mortality is way down, disease is down. Generally speaking, people are happier, more prosperous, and living in better conditions than they ever have been. The statistics on this are pretty clear. Things do seem to be getting better. And in spite of this, a lot of people have a tendency to be unreasonably pessimistic about the future. It can be pretty aggravating. So if you can't tell, I quite like Steven Pinker. He's the kind of person who, if I didn't know much about a subject and I heard that he'd written an article about that subject in Popular Science Magazine, I would think, oh good, I can read that article and it will give me a good understanding of that subject. But I think you can tell from the setup here that that was not the case with this piece. So the article is titled We're Told to Fear Robots, But Why Do We Think They'll Turn on Us? With the subtitle The Robot Uprising is a Myth. Pinker starts the article by talking about this fact that things are generally getting better and a lot of people seem to want to think that things are getting worse. There's a popular narrative that says, even though technology seems to be making our lives better, what if we're actually on our way to some disaster and technology is actually going to be terrible for us and we just don't realize it yet? And it seems that Pinker sees concern about AI risk as just more of the same, more of this just general purpose technology pessimism. Is it though? I'm sure it's part of the reason for the popularity of the idea. I've certainly talked to people who seem to think that way, but when it comes to deciding whether AI risk is a real concern, I think this is a weak argument. I'm generally very wary of arguments that talk about people's psychology and their motivations for making a claim, rather than talking about the claim itself. To demonstrate, you can make a very similar argument and say, well, Steven Pinker has spent many years of his life railing against this sort of general pessimism. And because this is kind of his thing, he's inclined to see it everywhere he looks. And so when he sees people who actually have an understanding of the technical subject matter raising serious safety concerns, he's inclined to just dismiss it as more of the same Luddite pessimism because that's what he knows. That's what he expects to see. Is that what's actually happened? I don't know, maybe, but I think it's a weak argument to make. The fact that people might have reasons to want to say that AI is a risk doesn't mean that it isn't. And the fact that Steven Pinker might be inclined to dismiss such concerns as this kind of reflexive pessimism doesn't mean that it isn't. So these are just bulverism and we should get down to the actual technical issues. So what are the actual arguments put forward by the article? Before we start in on that, I just want to say summarizing arguments is difficult. Summarizing arguments fairly is especially difficult. And I'm not trying to construct straw man here, but I might by accident. I would encourage everyone to read the original article. There's a link in the description. But I know most of you won't, and that's okay. Okay, so one of the first points that the article makes when it gets to actual arguments is it says, among the smart people who aren't losing sleep are most experts in artificial intelligence and most experts in human intelligence. This is kind of an appeal to authority, but I think that's totally legitimate here. The opinions of AI experts are important evidence about AI risk, but is it actually true? Do most AI researchers not think that advanced AI is a significant risk? Well, just recently a survey was published and in fact, I made a whole video about it. So I'll link to that video and the paper in the description. Grace et al surveyed all of the researchers who published work at the 2015 NIPS and ICML conferences. This is a reasonable cross section of high level AI researchers. These are people who have published in some of the best conferences on AI. Like I say, I made a whole video about this, but the single question that I want to draw your attention to is this one. Does Stuart Russell's argument for why a highly advanced AI might pose a risk point at an important problem? So the argument that's being referred to by this question is more or less the argument that I've been making on this channel, the general AI safety concern argument, all of the reasons why alignment and AI safety research is necessary. So do the AI experts agree with that? Well, 11% of them think, no, it's not a real problem. 19% think, no, it's not an important problem, but the remainder, 70% of the AI experts agree that this is at least a moderately important problem. Now, I don't know how many of them are losing sleep, but I would say that the implication of the article's claim that AI researchers don't think AI risk is a serious concern is just factually not true. AI experts are concerned about this. And the thing that's strange is the article quotes Ramez Naam as a computer scientist, but the only time anyone is referred to as an AI expert is this quote at the end from Stuart Russell. That name should be familiar. He's the guy posing the argument in the survey. The AI expert who Pinker quotes kind of giving the impression that they're on the same side of this debate is not at all. The thing that astonishes me about this is just how little research would have been needed to spot this mistake. If you're wondering about Stuart Russell's opinion, you might read something he's written on the subject. If you're kind of lazy, you might just read the abstracts. If you're very lazy and you just want to get a quick feel for the basic question, is Stuart Russell worried about existential risks from artificial intelligence, you might just go to his website and look at the titles of his recent publications. Oh, these academics with their jargon, how are we supposed to decipher what they really think? Anyway, the next point the article makes is that the arguments for AI risk rely on an oversimplified conception of intelligence, that they think of intelligence as this sort of magical one-dimensional thing that gives you power. It's just like animals have some of it, humans have more, and AI would have even more, and so it's dangerous. The article makes the point that intelligence is multifaceted and multidimensional. Human intelligence is not monolithic. It isn't this single thing which automatically gives you effectiveness in all domains. And that's true. It's an important point. And I think sometimes when communicating about this stuff to the public, people oversimplify this fact. But the article seems to use this to suggest that general intelligence is not a thing, or that generality is not a thing, that all intelligence is an accumulation of a number of small, sort of narrow, specific abilities. He lists some things that human intelligence can do. Find food, win friends and influence people, charm prospective mates, bring up children, move around the world, and pursue other human obsessions and pastimes. And yeah, these are pretty much all things that animals also do to some extent, but they aren't the limits of human intelligence. Human beings do definitely seem to have some general intelligence. There's some ability we have which allows us to act intelligently in an unusually broad range of domains, including domains which are quite different from what our ancestors experienced, things which our brains didn't evolve specifically to do. Like, we can learn how to play games like chess and Go, to do very intelligent manipulation of these pretty arbitrary systems which we invented. We can learn how to drive cars, although there were no cars in the ancestral environment. And in fact, we can learn how to build cars, which is pretty unlike anything our ancestors or other animals can do. We can operate intelligently in new environments, even in completely alien environments. We can operate on the moon. We can build rockets. We can attach cars to rockets, fly them to the moon, and then drive the cars on the moon. And there's nothing else on earth that can do that yet. So yeah, intelligence is clearly not a single thing, and it's clearly possible to be smart at some things and stupid at others. Intelligence in one domain doesn't automatically transfer to other domains and so on. Nonetheless, there is such a thing as general intelligence that allows an agent to act intelligently across a wide range of tasks. And humans do have it, at least to some extent. The next argument that he makes is, I think, the best argument in the article. It actually comes out of the orthogonality thesis. He's saying that concern about AI risk comes from a confusion between intelligence and motivation. Even if we invent superhumanly intelligent robots, why would they want to enslave their masters or take over the world? He says, being smart is not the same as wanting something. This is basically a restatement of the orthogonality thesis, which is that almost any level of intelligence is compatible with almost any terminal goal. So this is a pretty sensible argument, saying you're proposing that AI would do these fairly specific world domination type actions, which suggests that it has world domination type goals, but actually it could have any goals. There's no reason to believe it will want to do bad things to us. It could just as easily want to do good things. And this is true so far as it goes, but it ignores the argument from instrumental convergence, which is that while intelligence is compatible with almost any terminal goal, that doesn't mean that we have no information about instrumental goals. Certain behaviors are likely to occur in arbitrary agents because they're a good way of achieving a wide variety of different goals. So we can make predictions about how AI systems are likely to behave, even if we don't know what their terminal goals are. And if that doesn't seem obvious to you, I made a whole video about instrumental convergence, which you should watch, link in the description. The next thing the article says is, the scenarios that supposedly illustrate the existential threat to the human species of advanced artificial intelligence are fortunately self-refuting. They depend on the premises that one, humans are so gifted that they can design an omniscient and omnipotent AI, yet so moronic that they would give it control of the universe without testing how it works. And two, the AI would be so brilliant that it could figure out how to transmute elements and rewire brains, yet so imbecilic that it would wreak havoc based on elementary blunders of misunderstanding. Okay, so before we get into the meat of this, I have some nitpicks. Firstly, an AGI doesn't have to be omniscient and omnipotent to be far superior to humans along all axes. And it doesn't need to be far superior to humans along all axes in order to be very dangerous. But even accepting that this thing is extremely powerful, because that assumption is sometimes used, I object to this idea that humans would give it control of the universe without testing it. You've said it's omnipotent. It doesn't need to be given control of anything, right? If you actually do have an omniscient, omnipotent AGI, then giving it control of the universe is pretty much just turning it on. The idea that a system like that could be reliably contained or controlled is pretty absurd, though that's a subject for a different video. Now this second part, about the AI being smart enough to be powerful, yet dumb enough to do what we said instead of what we meant, is just based on an inaccurate model of how these systems work. The idea is not that the system is switched on and then given a goal in English which it then interprets to the best of its ability and tries to achieve. The idea is that the goal is part of the programming of the system. You can't create an agent with no goals. Something with no goals is not an agent. So he's describing it as though the goal of the agent is to interpret the commands that it's given by a human and then try to figure out what the human meant, rather than what they said, and do that. If we could build such a system, well, that would be relatively safe. But we can't do that. We don't know how, because we don't know how to write a program which corresponds to what we mean when we say, listen to the commands that the humans give you and interpret them according to the best of your abilities, and then try to do what they mean rather than what they say. This is kind of the core of the problem. Writing the code which corresponds to that is really difficult. We don't know how to do it, even with infinite computing power. And the thing is, writing something which just wants to collect stamps or make paperclips or whatever is way easier than writing something which actually does what we want. That's a problem, because generally speaking we tend to do the easier thing first, and it's much easier to make an unsafe AGI than it is to make a safe one. So by default we'd expect the first AGI systems to be unsafe. But apart from those problems, the big issue with this part of the article is that both of these points rely on exactly the same simplified model of intelligence that's criticized earlier in the article. He starts off saying that intelligence isn't just a single monolithic thing, and being smart at one thing doesn't automatically make you smart at other things, and then goes on to say if humans are smart enough to design an extremely powerful AI, then they must also be smart enough to do so safely. And that's just not true. It's very possible to be smart in one way and stupid in another. So in this specific case, the question is, are humans ever smart enough to develop extremely sophisticated and powerful technology, and yet not smart enough to think through all of the possible consequences and ramifications of deploying that technology before they deploy it? And the answer here seems to be very clearly, yes. Yeah, humans do that a lot. Like, all the time humans do that. And then similarly, the idea that an AI would be so brilliant that it could transmute elements and rewire brains, and yet so imbecilic that it would wreak havoc based on elementary blunders of misunderstanding, you can be smart at one thing and stupid at another. Especially if you're a software system. And anyone who's interacted with software systems knows this, to the point that it's a cliche. I mean, you want to talk about self-defeating arguments? There are a few more things I would want to say about this article, but this video is long enough as it is. So I'll just close by saying, I really like Steven Pinker, but this article is just uncharacteristically bad. I agree that we have a problem with unthinking, cynical pessimism about technology, but concern about AI risks is not just more of the same. In its most productive form, it's not even pessimism. The thing is, AI can be a huge force for good, but it won't be by default. We need to focus on safety, and we won't get that without suitable concern about the risks. We need to push back against pessimism and make the point that things are getting better, and science and technology is a big part of the reason for that. But let's not make AI safety a casualty in that fight. I've had some of my patrons suggest that I try making a larger volume of lower effort content, so I'm trying an experiment. I've made a video that's just me reading through this whole article and adding my comments as I go. It's like 45 minutes long, it's kind of unstructured, rambly sort of video. I'm not sure it belongs on this channel, but I posted it to Patreon so people can let me know what they think of it. So if you're interested in seeing that kind of experimental stuff from me, consider joining all of these amazing people and becoming a patron. And thank you so much to everyone who does that. In this video, I'm especially thanking JJ Hepboy, who's been a patron for more than a year now, and has finally come up in the video thanks rotation. Thank you so much for your support, JJ. And thank you all for watching. I'll see you next time. 