1
00:00:00,000 --> 00:00:04,480
Hi. Today we're going to talk about iterated distillation and amplification.

2
00:00:04,480 --> 00:00:08,320
So let's say we want to play Go, and we want to be really good at it.

3
00:00:08,320 --> 00:00:12,880
So we're trying to create a function which, if we're given a Go board in a particular state,

4
00:00:12,880 --> 00:00:16,880
will take that board state as input and return a very high quality move

5
00:00:16,880 --> 00:00:18,560
that you can make from that position.

6
00:00:18,560 --> 00:00:24,320
What we're trying to create is a policy, a function which maps states onto actions.

7
00:00:24,320 --> 00:00:27,600
Suppose that what we have, though, is something just slightly different.

8
00:00:27,600 --> 00:00:31,920
Suppose what we have is our intuition about moves, which takes a board state,

9
00:00:31,920 --> 00:00:35,120
and it gives us, for each of the possible moves we could make,

10
00:00:35,120 --> 00:00:37,520
some sense of how good that move would be.

11
00:00:37,520 --> 00:00:39,920
We can think of this as an action value function,

12
00:00:39,920 --> 00:00:44,160
which assigns a real number to each move, which represents how good we think that move is.

13
00:00:44,880 --> 00:00:49,760
Alternatively, we can think of it as outputting a distribution over all possible moves.

14
00:00:49,760 --> 00:00:52,240
So for a human player, this represents your intuition.

15
00:00:52,800 --> 00:00:55,680
The Go player looks at the board state and says,

16
00:00:56,240 --> 00:01:00,960
eh, it looks like maybe this move might be good, this move probably is a bad idea,

17
00:01:01,600 --> 00:01:02,800
this one looks okay.

18
00:01:02,800 --> 00:01:07,520
You could also have a neural network which takes the board state and a possible move as input,

19
00:01:07,520 --> 00:01:09,520
and outputs how good it thinks that move is.

20
00:01:10,240 --> 00:01:14,320
Okay, so how do you get the understanding of the game that allows you to evaluate moves?

21
00:01:14,320 --> 00:01:18,720
Well, as a human, you can study the rules and watch some games played by people who are better

22
00:01:18,720 --> 00:01:22,080
at Go than you are. If you have a neural network, then it's also fairly straightforward.

23
00:01:22,080 --> 00:01:26,080
You can train the network with a large number of high-quality human-played games,

24
00:01:26,080 --> 00:01:29,680
until its output gives a good prediction of what a skilled human would do.

25
00:01:29,680 --> 00:01:34,080
So strictly speaking, in that case, the network isn't evaluating how good a move is,

26
00:01:34,080 --> 00:01:37,440
it's evaluating how likely a good player would be to make that move.

27
00:01:37,440 --> 00:01:40,000
But that can be used as a proxy for how good the move is.

28
00:01:40,560 --> 00:01:42,640
Once we have this action value function,

29
00:01:42,640 --> 00:01:46,400
there's a pretty obvious way to turn it into a policy, which is just argmax.

30
00:01:46,960 --> 00:01:51,040
You look at all of the moves with your intuition, or evaluate them all with the network.

31
00:01:51,040 --> 00:01:54,640
Find the best-looking move, the move that's highest rated, and use that.

32
00:01:55,280 --> 00:01:59,360
But if you have more time to think, or more computational resources, you can do better.

33
00:02:00,080 --> 00:02:03,600
Rather than just going with your first instinct about what you think is good,

34
00:02:03,600 --> 00:02:06,000
you could play forward a few moves in your head.

35
00:02:06,000 --> 00:02:10,960
You might think, okay, from this board state, it looks like this move would be good.

36
00:02:10,960 --> 00:02:12,640
What does the board look like if I play that?

37
00:02:13,280 --> 00:02:18,320
And then you can apply your action value function again, from the perspective of your opponent.

38
00:02:18,320 --> 00:02:20,640
Often, there'll be more than one move that looks promising,

39
00:02:20,640 --> 00:02:23,440
so you might want to consider some of the best-looking moves,

40
00:02:23,440 --> 00:02:25,920
and then apply your action value function again

41
00:02:25,920 --> 00:02:28,320
to think about how you might respond to each of them.

42
00:02:28,320 --> 00:02:30,400
And so on, exploring the tree.

43
00:02:30,400 --> 00:02:33,760
So what you're effectively doing here is tree search, right?

44
00:02:33,760 --> 00:02:37,200
You have a game tree of possible moves, and you're searching through it,

45
00:02:37,200 --> 00:02:40,880
deciding which branches to search down using your action value function.

46
00:02:40,880 --> 00:02:43,040
You can keep doing this for however much time you have.

47
00:02:43,600 --> 00:02:47,360
It might be that you think far enough ahead that you actually get to the end of the game,

48
00:02:47,360 --> 00:02:51,120
and you can see that some move is clearly good because it wins you the game,

49
00:02:51,120 --> 00:02:54,560
or some other move is clearly bad because it causes your opponent to win the game.

50
00:02:54,560 --> 00:02:57,920
Or you might just look a little bit ahead and try to evaluate where you are.

51
00:02:58,560 --> 00:03:01,680
You might look at the general quality of the moves that you have available

52
00:03:01,680 --> 00:03:06,480
to get a feel for whether this is a state you want to be in, or one you want to avoid.

53
00:03:06,480 --> 00:03:08,240
And after you've done all this thinking,

54
00:03:08,240 --> 00:03:11,840
you might have learned things that contradict your initial intuition.

55
00:03:11,840 --> 00:03:14,720
There might be some move which seemed good to you when you first thought of it,

56
00:03:14,720 --> 00:03:18,320
but then once you actually think through what your opponent would do if you made that move,

57
00:03:18,320 --> 00:03:21,040
and what you would do in response to that, and so on,

58
00:03:21,040 --> 00:03:22,960
that the move actually doesn't look good at all.

59
00:03:22,960 --> 00:03:27,120
So you do all of this thinking ahead, and then you have some way of taking what you've learned

60
00:03:27,120 --> 00:03:30,080
and getting a new set of ratings for the moves you could make.

61
00:03:30,080 --> 00:03:33,520
And this can be more accurate than your original action value function.

62
00:03:33,520 --> 00:03:38,880
For a human, this is this kind of fuzzy process of thinking about moves and their consequences.

63
00:03:38,880 --> 00:03:43,600
And in a program like AlphaGo or AlphaZero, this is done with Monte Carlo tree search,

64
00:03:43,600 --> 00:03:47,760
where there's a structured way of extracting information from this tree search process.

65
00:03:47,760 --> 00:03:52,400
So there's a sense in which this whole process of using the action value function repeatedly,

66
00:03:52,400 --> 00:03:57,920
and searching the tree, represents something of the same type as the original action value function.

67
00:03:59,040 --> 00:04:03,520
It takes a board state as input, and it gives you move evaluations.

68
00:04:03,520 --> 00:04:08,560
It allows us to take our original action value function, which on its own is a weak player,

69
00:04:08,560 --> 00:04:11,600
and by applying it lots of times in this structured way,

70
00:04:11,600 --> 00:04:14,960
we can amplify that weak player to create a stronger player.

71
00:04:14,960 --> 00:04:19,920
So now, our amplified action value function is the same type of thing as our unamplified one.

72
00:04:20,560 --> 00:04:21,440
How do they compare?

73
00:04:22,720 --> 00:04:26,320
Well, the amplified one is much bigger, so it's more expensive.

74
00:04:26,960 --> 00:04:29,120
For a human, it takes more thinking time.

75
00:04:29,120 --> 00:04:31,680
As a program, it needs more computational resources.

76
00:04:32,400 --> 00:04:36,960
But it's also better than just going with a single network or the single human intuition.

77
00:04:36,960 --> 00:04:38,880
Its move evaluations are more accurate.

78
00:04:39,680 --> 00:04:40,880
So that's pretty neat.

79
00:04:40,880 --> 00:04:43,360
We can take a fast but not very good player,

80
00:04:43,360 --> 00:04:46,400
and amplify it to get a more expensive but stronger player.

81
00:04:46,400 --> 00:04:48,160
There's something else we can do though,

82
00:04:48,160 --> 00:04:51,200
which is we can take what we've learned as part of this process

83
00:04:51,200 --> 00:04:53,920
to improve our original action value function.

84
00:04:53,920 --> 00:04:58,000
We can compare the outputs of the fast process and the amplified version,

85
00:04:58,000 --> 00:05:02,480
and say, hmm, the quick process gives this move a high rating,

86
00:05:02,480 --> 00:05:06,640
but when we think it all through with the amplified system, it turns out not to be a good move.

87
00:05:07,200 --> 00:05:10,720
So where did the quick system go wrong, and how do we fix it?

88
00:05:10,720 --> 00:05:13,360
If you're a human, you can maybe do this explicitly.

89
00:05:13,360 --> 00:05:17,360
Perhaps you can spot the mistake that you made that caused you to think this was a good move,

90
00:05:17,360 --> 00:05:19,120
and try to keep it in mind next time.

91
00:05:19,120 --> 00:05:21,280
You'll also learn unconsciously.

92
00:05:21,280 --> 00:05:24,960
Your general pattern matching ability will pick up some information

93
00:05:24,960 --> 00:05:28,240
about the value of making that kind of move from that kind of position.

94
00:05:28,240 --> 00:05:29,360
And with a neural network,

95
00:05:29,360 --> 00:05:33,280
you can just use the output of the amplified process as training data for the network.

96
00:05:33,280 --> 00:05:34,480
As you keep doing this,

97
00:05:34,480 --> 00:05:39,440
the small fast system will come to reflect some of what you've learned by exploring the game tree.

98
00:05:40,080 --> 00:05:44,720
So this process is kind of like distilling down this big amplified system

99
00:05:44,720 --> 00:05:46,560
into the quick, cheap-to-run system.

100
00:05:46,560 --> 00:05:48,960
And the thing that makes this really powerful is,

101
00:05:48,960 --> 00:05:50,720
we can do the whole thing again, right?

102
00:05:51,280 --> 00:05:55,360
Now that we've got slightly better intuitions or slightly better weights for our network,

103
00:05:55,360 --> 00:05:58,320
we can then amplify that new action value function.

104
00:05:58,320 --> 00:05:59,840
And this will give us better results.

105
00:05:59,840 --> 00:06:03,760
Firstly, because obviously, if your move evaluations are more accurate than before,

106
00:06:03,760 --> 00:06:07,440
then the move evaluations at the end of this process will be more accurate than before.

107
00:06:07,440 --> 00:06:09,360
Better quality in, better quality out.

108
00:06:09,840 --> 00:06:13,440
Secondly, it also allows you to search the tree more efficiently.

109
00:06:13,440 --> 00:06:15,760
If your intuitions about move quality are better,

110
00:06:15,760 --> 00:06:18,960
you can spend more of your time looking at better parts of the tree,

111
00:06:18,960 --> 00:06:20,880
and less time examining in detail

112
00:06:20,880 --> 00:06:24,080
the consequences of bad moves that aren't going to get played anyway.

113
00:06:24,080 --> 00:06:26,320
So using the same extra resources,

114
00:06:26,320 --> 00:06:30,480
the new amplified system is better than the previous amplified system.

115
00:06:30,480 --> 00:06:34,720
And that means that when it comes to the distillation phase of learning from the exploration,

116
00:06:34,720 --> 00:06:38,320
there's more to learn and your action value function can improve again.

117
00:06:38,320 --> 00:06:40,320
So it's a cycle with two stages.

118
00:06:40,320 --> 00:06:45,120
First, you amplify by using extra computational resources to make the system more powerful.

119
00:06:45,120 --> 00:06:49,440
And then you distill by training the fast system with the output of the amplified system.

120
00:06:49,440 --> 00:06:50,480
And then you repeat.

121
00:06:50,480 --> 00:06:52,560
So the system will keep on improving.

122
00:06:52,560 --> 00:06:54,000
So when does this process end?

123
00:06:54,880 --> 00:06:56,960
Well, it depends on your implementation.

124
00:06:56,960 --> 00:06:59,440
But eventually, you'll reach a fixed point

125
00:06:59,440 --> 00:07:03,520
where the fast system isn't able to learn anything more from the amplified system.

126
00:07:03,520 --> 00:07:07,520
For simple problems, this might happen because the unamplified system becomes

127
00:07:07,520 --> 00:07:11,200
so good that there's nothing to be gained by the amplification process.

128
00:07:11,200 --> 00:07:14,320
If your action value function always suggests the optimal move,

129
00:07:14,320 --> 00:07:18,400
then the amplified system is always just going to agree and no more learning happens.

130
00:07:18,400 --> 00:07:19,440
For harder problems, though,

131
00:07:19,440 --> 00:07:23,520
it's much more likely that you'll reach the limits of your action value function implementation.

132
00:07:23,520 --> 00:07:27,040
You hit a point where a neural network of that size and architecture

133
00:07:27,040 --> 00:07:31,600
just isn't able to learn how to be better than that by being trained on amplified gameplay.

134
00:07:31,600 --> 00:07:34,960
As a human, even if you could study Go for infinite time,

135
00:07:34,960 --> 00:07:37,440
eventually you'll hit the limits of what your brain can do.

136
00:07:37,520 --> 00:07:40,560
The point is that the strength of the end result of this process

137
00:07:40,560 --> 00:07:43,520
isn't limited by the strength of the initial action value function.

138
00:07:44,240 --> 00:07:46,320
The limit is determined by the architecture.

139
00:07:46,320 --> 00:07:49,600
It's a fixed point of the amplification and distillation process.

140
00:07:49,600 --> 00:07:53,120
A version of AlphaGo that starts out trained on amateur-level games

141
00:07:53,120 --> 00:07:55,120
might take longer to train to a given level

142
00:07:55,120 --> 00:07:58,320
than one that started out trained on grandmaster-level games.

143
00:07:58,320 --> 00:08:02,080
But after enough training, they'd both end up around the same strength.

144
00:08:02,080 --> 00:08:05,440
And in fact, AlphaZero ended up even stronger than AlphaGo,

145
00:08:05,440 --> 00:08:09,040
even though it started from zero, using no human games at all.

146
00:08:09,040 --> 00:08:12,800
So that's how you can use amplification and distillation to get better at Go.

147
00:08:13,360 --> 00:08:16,400
And why, as a software system, you can keep getting better

148
00:08:16,400 --> 00:08:18,800
even when you have no external source to learn from.

149
00:08:18,800 --> 00:08:22,640
Even once you leave humans behind and you're the best Go player in the universe,

150
00:08:22,640 --> 00:08:25,840
so there's nobody who can teach you, you can still keep learning,

151
00:08:25,840 --> 00:08:28,480
because you can learn from the amplified version of yourself.

152
00:08:29,040 --> 00:08:31,680
Okay, so why is this relevant for AI safety?

153
00:08:31,680 --> 00:08:35,440
Well, we've just talked about one example of iterated distillation and amplification.

154
00:08:36,000 --> 00:08:38,160
The idea is actually much more general than that.

155
00:08:38,720 --> 00:08:40,240
It's not just for playing Go,

156
00:08:40,240 --> 00:08:43,120
and it's not just for Monte Carlo tree search and neural networks.

157
00:08:43,120 --> 00:08:46,880
Amplification might be this kind of process of thinking ahead if you're a human being.

158
00:08:46,880 --> 00:08:50,800
It might be Monte Carlo tree search or something like it if you're a software system,

159
00:08:50,800 --> 00:08:51,920
but it might be something else.

160
00:08:52,720 --> 00:08:55,120
If you are, for example, an AGI,

161
00:08:55,120 --> 00:08:59,440
it might involve spinning up lots of copies of yourself to collaborate with or delegate to,

162
00:08:59,440 --> 00:09:03,920
so that the team of copies can be better at solving the problem than you would be on your own.

163
00:09:03,920 --> 00:09:07,680
For some types of problem, it might just involve running your mind at a faster rate

164
00:09:07,680 --> 00:09:10,320
to work on the problem for a long period of subjective time.

165
00:09:10,320 --> 00:09:15,680
The core characteristic is that amplification uses the original process as a starting point

166
00:09:15,680 --> 00:09:19,360
and applies more computational resources to create a more powerful agent.

167
00:09:19,360 --> 00:09:22,720
In the same way, distillation can be any process whereby we

168
00:09:22,720 --> 00:09:25,280
compress this more expensive amplified agent

169
00:09:25,280 --> 00:09:28,480
into something that we can call cheaply, just as we called the original system.

170
00:09:28,480 --> 00:09:32,800
For a human playing Go, this can be the way your intuition gets better as you play.

171
00:09:32,800 --> 00:09:34,240
For a neural network playing Go,

172
00:09:34,240 --> 00:09:38,160
we can train the action value network to give the same outputs as the tree search process.

173
00:09:38,160 --> 00:09:42,640
For an AGI, it could involve the AGI learning, in whatever way it learns,

174
00:09:42,640 --> 00:09:46,160
how to predict and imitate the team of copies of itself,

175
00:09:46,160 --> 00:09:50,000
or the accelerated version of itself, or whatever the amplified system is.

176
00:09:50,000 --> 00:09:52,080
The core characteristic is that the cheaper,

177
00:09:52,080 --> 00:09:56,800
faster agent learns to approximate the behavior of the more expensive amplified agent.

178
00:09:56,800 --> 00:10:01,680
So these two processes together define a way of training a stronger agent from a weaker one.

179
00:10:01,680 --> 00:10:07,520
The hope for safety research is that we can find designs for the amplify and distill procedures

180
00:10:07,520 --> 00:10:09,360
which preserve alignment.

181
00:10:09,360 --> 00:10:14,240
By which I mean that if the agent we amplify is aligned with our goals and values,

182
00:10:14,240 --> 00:10:16,320
then the amplified agent will be aligned as well.

183
00:10:17,120 --> 00:10:19,440
And if the amplified agent is aligned,

184
00:10:19,440 --> 00:10:23,120
then the agent we distill it down to will be aligned as well.

185
00:10:23,120 --> 00:10:26,560
In the next video, we'll talk about some ideas for how this might be done.

186
00:10:27,600 --> 00:10:42,320
I want to end this video with a big thank you to all of my wonderful patrons.

187
00:10:42,960 --> 00:10:45,680
That's all of these fantastic people here,

188
00:10:45,680 --> 00:10:48,640
who have been just so generous and so patient with me.

189
00:10:49,200 --> 00:10:50,720
Thank you all so much.

190
00:10:50,720 --> 00:10:53,840
In this video, I'm especially thanking Saeed Polat,

191
00:10:53,840 --> 00:10:58,080
who joined in December, just before the start of this gap in the uploads.

192
00:10:58,080 --> 00:11:03,600
And the reason for that is I've recently really had to focus on the Road to AI Safety Excellence,

193
00:11:03,600 --> 00:11:05,760
the online course I've been working on.

194
00:11:05,760 --> 00:11:09,840
In fact, the video you just watched is the first lecture from our module on IDA,

195
00:11:09,840 --> 00:11:10,960
which hasn't been released yet.

196
00:11:10,960 --> 00:11:13,360
So I also want to thank everyone at The Raise Project

197
00:11:13,360 --> 00:11:16,240
for their work on the script and the research for this video,

198
00:11:16,240 --> 00:11:17,840
and really the whole Raise team.

199
00:11:17,840 --> 00:11:20,080
I'm still making content just for this channel as well.

200
00:11:20,080 --> 00:11:22,000
And in fact, I have one that's nearly ready to go.

201
00:11:22,000 --> 00:11:22,880
So look out for that.

202
00:11:22,880 --> 00:11:25,600
Thanks again for watching, and I'll see you soon.

