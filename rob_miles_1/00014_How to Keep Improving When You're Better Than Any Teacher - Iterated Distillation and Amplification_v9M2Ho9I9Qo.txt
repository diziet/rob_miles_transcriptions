Hi. Today we're going to talk about iterated distillation and amplification. So let's say we want to play Go, and we want to be really good at it. So we're trying to create a function which, if we're given a Go board in a particular state, will take that board state as input and return a very high quality move that you can make from that position. What we're trying to create is a policy, a function which maps states onto actions. Suppose that what we have, though, is something just slightly different. Suppose what we have is our intuition about moves, which takes a board state, and it gives us, for each of the possible moves we could make, some sense of how good that move would be. We can think of this as an action value function, which assigns a real number to each move, which represents how good we think that move is. Alternatively, we can think of it as outputting a distribution over all possible moves. So for a human player, this represents your intuition. The Go player looks at the board state and says, eh, it looks like maybe this move might be good, this move probably is a bad idea, this one looks okay. You could also have a neural network which takes the board state and a possible move as input, and outputs how good it thinks that move is. Okay, so how do you get the understanding of the game that allows you to evaluate moves? Well, as a human, you can study the rules and watch some games played by people who are better at Go than you are. If you have a neural network, then it's also fairly straightforward. You can train the network with a large number of high-quality human-played games, until its output gives a good prediction of what a skilled human would do. So strictly speaking, in that case, the network isn't evaluating how good a move is, it's evaluating how likely a good player would be to make that move. But that can be used as a proxy for how good the move is. Once we have this action value function, there's a pretty obvious way to turn it into a policy, which is just argmax. You look at all of the moves with your intuition, or evaluate them all with the network. Find the best-looking move, the move that's highest rated, and use that. But if you have more time to think, or more computational resources, you can do better. Rather than just going with your first instinct about what you think is good, you could play forward a few moves in your head. You might think, okay, from this board state, it looks like this move would be good. What does the board look like if I play that? And then you can apply your action value function again, from the perspective of your opponent. Often, there'll be more than one move that looks promising, so you might want to consider some of the best-looking moves, and then apply your action value function again to think about how you might respond to each of them. And so on, exploring the tree. So what you're effectively doing here is tree search, right? You have a game tree of possible moves, and you're searching through it, deciding which branches to search down using your action value function. You can keep doing this for however much time you have. It might be that you think far enough ahead that you actually get to the end of the game, and you can see that some move is clearly good because it wins you the game, or some other move is clearly bad because it causes your opponent to win the game. Or you might just look a little bit ahead and try to evaluate where you are. You might look at the general quality of the moves that you have available to get a feel for whether this is a state you want to be in, or one you want to avoid. And after you've done all this thinking, you might have learned things that contradict your initial intuition. There might be some move which seemed good to you when you first thought of it, but then once you actually think through what your opponent would do if you made that move, and what you would do in response to that, and so on, that the move actually doesn't look good at all. So you do all of this thinking ahead, and then you have some way of taking what you've learned and getting a new set of ratings for the moves you could make. And this can be more accurate than your original action value function. For a human, this is this kind of fuzzy process of thinking about moves and their consequences. And in a program like AlphaGo or AlphaZero, this is done with Monte Carlo tree search, where there's a structured way of extracting information from this tree search process. So there's a sense in which this whole process of using the action value function repeatedly, and searching the tree, represents something of the same type as the original action value function. It takes a board state as input, and it gives you move evaluations. It allows us to take our original action value function, which on its own is a weak player, and by applying it lots of times in this structured way, we can amplify that weak player to create a stronger player. So now, our amplified action value function is the same type of thing as our unamplified one. How do they compare? Well, the amplified one is much bigger, so it's more expensive. For a human, it takes more thinking time. As a program, it needs more computational resources. But it's also better than just going with a single network or the single human intuition. Its move evaluations are more accurate. So that's pretty neat. We can take a fast but not very good player, and amplify it to get a more expensive but stronger player. There's something else we can do though, which is we can take what we've learned as part of this process to improve our original action value function. We can compare the outputs of the fast process and the amplified version, and say, hmm, the quick process gives this move a high rating, but when we think it all through with the amplified system, it turns out not to be a good move. So where did the quick system go wrong, and how do we fix it? If you're a human, you can maybe do this explicitly. Perhaps you can spot the mistake that you made that caused you to think this was a good move, and try to keep it in mind next time. You'll also learn unconsciously. Your general pattern matching ability will pick up some information about the value of making that kind of move from that kind of position. And with a neural network, you can just use the output of the amplified process as training data for the network. As you keep doing this, the small fast system will come to reflect some of what you've learned by exploring the game tree. So this process is kind of like distilling down this big amplified system into the quick, cheap-to-run system. And the thing that makes this really powerful is, we can do the whole thing again, right? Now that we've got slightly better intuitions or slightly better weights for our network, we can then amplify that new action value function. And this will give us better results. Firstly, because obviously, if your move evaluations are more accurate than before, then the move evaluations at the end of this process will be more accurate than before. Better quality in, better quality out. Secondly, it also allows you to search the tree more efficiently. If your intuitions about move quality are better, you can spend more of your time looking at better parts of the tree, and less time examining in detail the consequences of bad moves that aren't going to get played anyway. So using the same extra resources, the new amplified system is better than the previous amplified system. And that means that when it comes to the distillation phase of learning from the exploration, there's more to learn and your action value function can improve again. So it's a cycle with two stages. First, you amplify by using extra computational resources to make the system more powerful. And then you distill by training the fast system with the output of the amplified system. And then you repeat. So the system will keep on improving. So when does this process end? Well, it depends on your implementation. But eventually, you'll reach a fixed point where the fast system isn't able to learn anything more from the amplified system. For simple problems, this might happen because the unamplified system becomes so good that there's nothing to be gained by the amplification process. If your action value function always suggests the optimal move, then the amplified system is always just going to agree and no more learning happens. For harder problems, though, it's much more likely that you'll reach the limits of your action value function implementation. You hit a point where a neural network of that size and architecture just isn't able to learn how to be better than that by being trained on amplified gameplay. As a human, even if you could study Go for infinite time, eventually you'll hit the limits of what your brain can do. The point is that the strength of the end result of this process isn't limited by the strength of the initial action value function. The limit is determined by the architecture. It's a fixed point of the amplification and distillation process. A version of AlphaGo that starts out trained on amateur-level games might take longer to train to a given level than one that started out trained on grandmaster-level games. But after enough training, they'd both end up around the same strength. And in fact, AlphaZero ended up even stronger than AlphaGo, even though it started from zero, using no human games at all. So that's how you can use amplification and distillation to get better at Go. And why, as a software system, you can keep getting better even when you have no external source to learn from. Even once you leave humans behind and you're the best Go player in the universe, so there's nobody who can teach you, you can still keep learning, because you can learn from the amplified version of yourself. Okay, so why is this relevant for AI safety? Well, we've just talked about one example of iterated distillation and amplification. The idea is actually much more general than that. It's not just for playing Go, and it's not just for Monte Carlo tree search and neural networks. Amplification might be this kind of process of thinking ahead if you're a human being. It might be Monte Carlo tree search or something like it if you're a software system, but it might be something else. If you are, for example, an AGI, it might involve spinning up lots of copies of yourself to collaborate with or delegate to, so that the team of copies can be better at solving the problem than you would be on your own. For some types of problem, it might just involve running your mind at a faster rate to work on the problem for a long period of subjective time. The core characteristic is that amplification uses the original process as a starting point and applies more computational resources to create a more powerful agent. In the same way, distillation can be any process whereby we compress this more expensive amplified agent into something that we can call cheaply, just as we called the original system. For a human playing Go, this can be the way your intuition gets better as you play. For a neural network playing Go, we can train the action value network to give the same outputs as the tree search process. For an AGI, it could involve the AGI learning, in whatever way it learns, how to predict and imitate the team of copies of itself, or the accelerated version of itself, or whatever the amplified system is. The core characteristic is that the cheaper, faster agent learns to approximate the behavior of the more expensive amplified agent. So these two processes together define a way of training a stronger agent from a weaker one. The hope for safety research is that we can find designs for the amplify and distill procedures which preserve alignment. By which I mean that if the agent we amplify is aligned with our goals and values, then the amplified agent will be aligned as well. And if the amplified agent is aligned, then the agent we distill it down to will be aligned as well. In the next video, we'll talk about some ideas for how this might be done. I want to end this video with a big thank you to all of my wonderful patrons. That's all of these fantastic people here, who have been just so generous and so patient with me. Thank you all so much. In this video, I'm especially thanking Saeed Polat, who joined in December, just before the start of this gap in the uploads. And the reason for that is I've recently really had to focus on the Road to AI Safety Excellence, the online course I've been working on. In fact, the video you just watched is the first lecture from our module on IDA, which hasn't been released yet. So I also want to thank everyone at The Raise Project for their work on the script and the research for this video, and really the whole Raise team. I'm still making content just for this channel as well. And in fact, I have one that's nearly ready to go. So look out for that. Thanks again for watching, and I'll see you soon. 