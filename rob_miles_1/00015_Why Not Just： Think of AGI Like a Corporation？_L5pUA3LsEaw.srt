1
00:00:00,000 --> 00:00:03,600
Hi. So I sometimes see people saying things like,

2
00:00:03,600 --> 00:00:08,400
OK, so your argument is that at some point in the future, we're going to develop intelligent

3
00:00:08,400 --> 00:00:12,400
agents that are able to reason about the world in general and take actions in the world to

4
00:00:12,400 --> 00:00:16,960
achieve their goals. These agents might have superhuman intelligence that allows them to be

5
00:00:16,960 --> 00:00:21,120
very good at achieving their goals. And this is a problem because they might have different goals

6
00:00:21,120 --> 00:00:26,480
from us. But don't we kind of have that already? Corporations can be thought of as super intelligent

7
00:00:26,480 --> 00:00:30,720
agents. They're able to think about the world in general and they can outperform individual

8
00:00:30,720 --> 00:00:36,000
humans across a range of cognitive tasks. And they have goals, namely maximizing profits or

9
00:00:36,000 --> 00:00:40,720
shareholder value or whatever. And those goals aren't the same as the overall goals of humanity.

10
00:00:40,720 --> 00:00:44,800
So corporations are a kind of misaligned super intelligence. The people who say this,

11
00:00:44,800 --> 00:00:49,760
having established the metaphor, at this point tend to diverge mostly along political lines.

12
00:00:49,760 --> 00:00:54,240
Some say corporations are therefore a clear threat to human values and goals

13
00:00:54,240 --> 00:00:57,680
in the same way that misaligned super intelligences are, and they need to be much

14
00:00:57,680 --> 00:01:01,600
more tightly controlled, if not destroyed altogether. Others say corporations are like

15
00:01:01,600 --> 00:01:06,640
misaligned super intelligences, but corporations have been instrumental in the huge increases

16
00:01:06,640 --> 00:01:10,160
in human wealth and well-being that we've seen over the last couple of centuries,

17
00:01:10,160 --> 00:01:14,400
with pretty minor negative side effects overall. If that's the effect of misaligned super

18
00:01:14,400 --> 00:01:18,960
intelligences, I don't see why we should be concerned about AI. And others say corporations

19
00:01:18,960 --> 00:01:22,960
certainly have their problems, but we seem to have developed systems that keep them under control

20
00:01:23,040 --> 00:01:27,280
well enough that they're able to create value and do useful things without literally killing

21
00:01:27,280 --> 00:01:32,960
everyone. So perhaps we can learn something about how to control or align super intelligences by

22
00:01:32,960 --> 00:01:37,680
looking at how we handle corporations. So we're going to let the first two fight amongst themselves

23
00:01:37,680 --> 00:01:42,480
and we'll talk to the third guy. So how good is this metaphor? Are corporations really like

24
00:01:42,480 --> 00:01:47,200
misaligned artificial general super intelligences? Quick note before we start, we're going to be

25
00:01:47,200 --> 00:01:52,480
comparing corporations to AI systems. And this gets a lot more complicated when you consider that

26
00:01:52,480 --> 00:01:57,920
corporations in fact use AI systems. So for the sake of simplicity, we're going to assume that

27
00:01:57,920 --> 00:02:03,120
corporations don't use AI systems because otherwise the problem gets recursive and like not in a cool

28
00:02:03,120 --> 00:02:09,120
way. First off, are corporations agents in the relevant way? I would say, yeah, pretty much.

29
00:02:09,120 --> 00:02:13,680
I think that it's reasonably productive to think of a corporation as an agent. They do seem to make

30
00:02:13,680 --> 00:02:17,280
decisions and take actions in the world in order to achieve goals in the world. But I think you

31
00:02:17,280 --> 00:02:21,520
face a similar problem thinking of corporations as agents, as you do when you try to think of

32
00:02:21,520 --> 00:02:26,000
human beings as agents. In economics, it's common to model human beings as agents that want to

33
00:02:26,000 --> 00:02:30,480
maximize their money in some sense. And you can model corporations in the same way, and this is

34
00:02:30,480 --> 00:02:36,240
useful, but it is kind of a simplification in that human beings in practice want things that aren't

35
00:02:36,240 --> 00:02:41,120
just money. And while corporations are more directly aligned with profit maximizing than

36
00:02:41,120 --> 00:02:45,840
individual human beings are, it's not quite that simple. So yes, we can think of corporations as

37
00:02:45,840 --> 00:02:51,440
agents, but we can't treat their stated goals as being exactly equivalent to their actual goals in

38
00:02:51,440 --> 00:02:57,200
practice. More on that later. So corporations are more or less agents. Are they generally intelligent

39
00:02:57,200 --> 00:03:02,880
agents? Again, yeah, I think so. I mean, corporations are made up of human beings, so they have all the

40
00:03:02,880 --> 00:03:07,600
same general intelligence capabilities that human beings have. So then the question is, are they

41
00:03:07,600 --> 00:03:13,920
super intelligent? This is where things get interesting because the answer is kind of. Like

42
00:03:13,920 --> 00:03:18,640
SpaceX is able to design a better rocket than any individual human engineer could design.

43
00:03:18,640 --> 00:03:23,200
Rocket design is a cognitive task, and SpaceX is better at that than any human being, therefore

44
00:03:23,200 --> 00:03:28,880
SpaceX is a super intelligence in the domain of rocket design. But a calculator is a super

45
00:03:28,880 --> 00:03:33,040
intelligence in the domain of arithmetic. That's not enough. Are corporations general super

46
00:03:33,040 --> 00:03:38,480
intelligences? Do they outperform humans across a wide range of cognitive tasks as an AGI could?

47
00:03:38,480 --> 00:03:43,600
In practice, it depends on the task. Consider playing a strategy game. For the sake of

48
00:03:43,600 --> 00:03:49,600
simplicity, let's use a game that humans still beat AI systems at, like StarCraft. If a corporation

49
00:03:49,600 --> 00:03:54,640
for some reason had to win at StarCraft, it could perform about as well as the best human players.

50
00:03:54,640 --> 00:04:00,800
It would do that by hiring the best human players. But you won't achieve superhuman play that way.

51
00:04:00,800 --> 00:04:05,920
A human player acting on behalf of a corporation is just a human player, and the corporation doesn't

52
00:04:05,920 --> 00:04:10,560
really have a way to do much better than that. A team of reasonably good StarCraft players

53
00:04:10,640 --> 00:04:15,840
working together to control one army will still lose to a single very good player working alone.

54
00:04:15,840 --> 00:04:20,320
This seems to be true for a lot of strategy games. The classic example is the game of Kasparov

55
00:04:20,320 --> 00:04:25,360
versus the world, where Garry Kasparov played against the entire rest of the world cooperating

56
00:04:25,360 --> 00:04:30,480
on the internet. The game was kind of weird, but Kasparov ended up winning. And the kind of real

57
00:04:30,480 --> 00:04:35,600
world strategy that corporations have to do seems like it might be similar as well. When companies

58
00:04:35,600 --> 00:04:40,240
outsmart their competition, it's usually because they have a small number of decision makers who

59
00:04:40,240 --> 00:04:44,800
are unusually smart, rather than because they have a hundred reasonably smart people working

60
00:04:44,800 --> 00:04:49,440
together. For at least some tasks, teams of humans are not able to effectively combine their

61
00:04:49,440 --> 00:04:54,640
intelligence to achieve highly superhuman performance. So corporations are limited to

62
00:04:54,640 --> 00:04:59,680
around human level intelligence at those tasks. To break down why this is, let's look at some

63
00:04:59,680 --> 00:05:04,720
different options corporations have for ways to combine human intelligences. One obvious way is

64
00:05:04,720 --> 00:05:09,680
specialization. If you can divide the task into parts that people can specialize in, you can

65
00:05:09,680 --> 00:05:14,720
outperform individuals. You can have one person who's skilled at engine design, one who's great

66
00:05:14,720 --> 00:05:18,640
at aerodynamics, one who knows a lot about structural engineering, and one who's good at

67
00:05:19,440 --> 00:05:24,240
avionics. Can you tell I'm not a rocket surgeon? Anyway, if these people with their different

68
00:05:24,240 --> 00:05:28,880
skills are able to work together well with each person doing what they're best at, the resulting

69
00:05:28,880 --> 00:05:34,560
agent will in a sense have superhuman intelligence. No single human could ever be so good at so many

70
00:05:34,560 --> 00:05:40,800
different things. But this mechanism doesn't get you superhumanly high intelligence, just superhumanly

71
00:05:40,800 --> 00:05:47,520
broad intelligence. Whereas superintelligence software AGI might look like this. So specialization

72
00:05:47,520 --> 00:05:52,880
yields a fairly limited form of superintelligence if you can split your task up. But that's not easy

73
00:05:52,880 --> 00:05:58,320
for all tasks. For example, the task of coming up with creative ideas or strategies isn't easy to

74
00:05:58,320 --> 00:06:03,600
split up. You either have a good idea or you don't. But as a team, you can get everyone to suggest a

75
00:06:03,600 --> 00:06:09,040
strategy or idea and then pick the best one. That way, a group can perform better than any individual

76
00:06:09,040 --> 00:06:14,000
human. How much better though? And how does that change with the size of the team? I got curious

77
00:06:14,000 --> 00:06:18,320
about exactly how this works. So I came up with a toy model. Now, I'm not a statistician, I'm a

78
00:06:18,320 --> 00:06:23,600
computer scientist. So rather than working it out properly, I just simulated it 100 million times

79
00:06:23,600 --> 00:06:28,480
because that was quicker. Okay, so here's the idea quality distribution for an individual human.

80
00:06:28,480 --> 00:06:32,720
We'll model it as a normal distribution with a mean of 100 and a standard deviation of 20. So

81
00:06:32,720 --> 00:06:37,440
what this means is you ask a human for a suggestion and sometimes they do really well and come up with

82
00:06:37,440 --> 00:06:43,360
a 130 level strategy. Sometimes they screw up and can only give you a 70 idea, but most of the time

83
00:06:43,360 --> 00:06:48,320
it's around 100. Now suppose we add a second person whose intelligence is the same as the first.

84
00:06:49,040 --> 00:06:52,480
We have both of them come up with ideas and we keep whichever idea is better.

85
00:06:53,200 --> 00:06:58,480
The resulting team of two people combined looks like this. On average, the ideas are better. The

86
00:06:58,480 --> 00:07:04,880
mean is now, what, 107? And as we keep adding people, the performance gets better. Here's five

87
00:07:04,880 --> 00:07:12,000
people, 10, 20, 50, 100. Remember, these are probability distributions. So the height doesn't

88
00:07:12,000 --> 00:07:16,400
really matter. The point is that the distributions move to the right and get thinner. The average

89
00:07:16,400 --> 00:07:21,440
idea quality goes up and the standard deviation goes down. So we're coming up with better ideas

90
00:07:21,440 --> 00:07:26,560
and more reliably. But you see how the progress is slowing down. We're using 100 times as much

91
00:07:26,560 --> 00:07:32,960
brainpower here, but our average ideas are only like 25% better. What if we use a thousand people?

92
00:07:32,960 --> 00:07:38,720
10 times more resources again only gets us up to around 135. Diminishing returns. So what does this

93
00:07:38,720 --> 00:07:44,400
mean for corporations? Well, first off, to be fair, this team of a thousand people is clearly super

94
00:07:44,400 --> 00:07:49,280
intelligent. The worst ideas it ever has are still so good that an individual human will hardly ever

95
00:07:49,280 --> 00:07:53,360
manage to think of them. But it's still pretty limited. There's all this space off to the right

96
00:07:53,360 --> 00:07:57,760
of the graph that it would take vast team sizes to ever get into. If you're wondering how this would

97
00:07:57,760 --> 00:08:02,880
look with 7 billion humans, well, you have to work out the statistical solution yourself. The point is

98
00:08:02,880 --> 00:08:07,440
the team isn't that super intelligent because it's never going to think of an idea that no human

99
00:08:07,440 --> 00:08:12,960
could think of, which is kind of obvious when you think about it. But AGI isn't limited in that way.

100
00:08:12,960 --> 00:08:17,520
And in practice, even this model is way too optimistic for corporations. Firstly, because it

101
00:08:17,520 --> 00:08:22,080
assumes that the quality of suggestions for a particular problem is uncorrelated between humans,

102
00:08:22,080 --> 00:08:26,800
which is clearly not true. And secondly, because you have to pick out the best suggestion.

103
00:08:26,800 --> 00:08:31,600
But how can you be sure that you'll know the best idea when you see it? It happens to be true a lot

104
00:08:31,600 --> 00:08:36,960
of the time for a lot of problems that we care about, that evaluating solutions is easier than

105
00:08:36,960 --> 00:08:42,960
coming up with them. You know, Homer, it's very easy to criticize. Fun too. Machine learning relies

106
00:08:42,960 --> 00:08:47,600
pretty heavily on this, like writing a program that differentiates pictures of cats and dogs

107
00:08:47,600 --> 00:08:52,560
is really hard, but evaluating such a program is fairly simple. You just show it lots of pictures

108
00:08:52,560 --> 00:08:56,800
of cats and dogs and see how well it does. The clever bit is in figuring out how to take a

109
00:08:56,800 --> 00:09:01,920
method for evaluating solutions and use that to create good solutions. Anyway, this assumption

110
00:09:01,920 --> 00:09:07,280
isn't always true. And even when it is, the fact that evaluation is easier or cheaper than generation

111
00:09:07,280 --> 00:09:11,680
doesn't mean that evaluation is easy or cheap. Like I couldn't generate a good rocket design

112
00:09:11,680 --> 00:09:18,160
myself, but I can tell you that this one needs work. So evaluation is easier than generation,

113
00:09:18,960 --> 00:09:23,520
but that's a very expensive way to find out. And I wouldn't have been able to do it the cheap way

114
00:09:23,520 --> 00:09:28,480
by just looking at the blueprints. The skills needed to evaluate in advance whether a given

115
00:09:28,480 --> 00:09:32,960
rocket design will explode are very closely related to the skills needed to generate a

116
00:09:32,960 --> 00:09:37,840
non-exploding rocket design. So yeah, even if a corporation could somehow get around being

117
00:09:37,840 --> 00:09:42,480
limited to the kind of ideas that humans are able to generate, they're still limited to the kind of

118
00:09:42,480 --> 00:09:47,840
ideas that humans are able to recognize as good ideas. Just how serious is this limitation? How

119
00:09:47,840 --> 00:09:52,480
good are the strategies and ideas that corporations are missing out on? Well, take a minute to think

120
00:09:52,480 --> 00:10:00,000
of an idea that's too good for any human to recognize it as good. Got one? Well, that was

121
00:10:00,000 --> 00:10:05,040
worth a shot. We actually do have an example of this kind of thing in Move 37 from AlphaGo's

122
00:10:05,040 --> 00:10:12,720
2016 match with world champion Lee Sedol. Is this kind of evaluation, uh, value?

123
00:10:15,360 --> 00:10:21,280
That's a very, that's a very surprising move. I thought, I thought it was, I thought it was a mistake.

124
00:10:23,760 --> 00:10:28,000
Yeah, that turned out to be pretty much the move that won the game. But your GoPlaying

125
00:10:28,000 --> 00:10:32,880
corporation is never going to make Move 37. Even if someone happens to suggest it, it's almost

126
00:10:32,880 --> 00:10:38,320
certainly not going to be chosen. Normally human, we never play this one because it's bad. It's not

127
00:10:38,320 --> 00:10:43,440
enough for someone in your corporation to have a great idea. The people at the top need to recognize

128
00:10:43,440 --> 00:10:47,680
that it's a great idea. That means that there's a limit on the effective creative or strategic

129
00:10:47,680 --> 00:10:51,920
intelligence of a corporation, which is determined by the intelligence of the decision makers and

130
00:10:51,920 --> 00:10:56,640
their ability to know a good idea when they see one. Okay. What about speed? That's one of the

131
00:10:56,640 --> 00:11:01,360
things that makes AI systems so powerful. And one of the ways that software AGI is likely to be

132
00:11:01,360 --> 00:11:07,200
super intelligent. The general trend is we go from computers can't do this at all to computers

133
00:11:07,200 --> 00:11:11,600
can do this much faster than people. Not always, but in general. So I wouldn't be surprised if that

134
00:11:11,600 --> 00:11:17,120
pattern continues with AGI. How does the corporation rate on speed? Again, it kind of depends. This is

135
00:11:17,120 --> 00:11:22,400
closely related to something we've talked about before. Parallelizability. Some tasks are easy

136
00:11:22,400 --> 00:11:27,360
to split up and work on in parallel and some aren't. For example, if you've got a big list of

137
00:11:27,360 --> 00:11:31,680
a thousand numbers and you need to add them all up, it's very easy to parallelize. If you have

138
00:11:31,680 --> 00:11:36,240
10 people, you can just say, okay, you take the first hundred numbers, you take the second hundred,

139
00:11:36,240 --> 00:11:40,480
you take the third and so on. Have everybody add up their part of the list. And then at the end,

140
00:11:40,480 --> 00:11:44,960
you add up everyone's totals. However long the list is, you can throw more people at it and get

141
00:11:44,960 --> 00:11:49,920
it done faster, much faster than any individual human could. This is the kind of task where it's

142
00:11:49,920 --> 00:11:54,800
easy for corporations to achieve superhuman speed. But suppose instead of summing a list,

143
00:11:54,800 --> 00:11:58,960
you have a simple simulation that you want to run for say a thousand seconds.

144
00:11:58,960 --> 00:12:03,120
You can't say, okay, you work out the first hundred seconds of the simulation. You do the

145
00:12:03,120 --> 00:12:07,600
next hundred and you do the next hundred and so on. Because obviously the person who's simulating

146
00:12:07,600 --> 00:12:12,080
second 100 needs to know what happened at the end of second 99 before they can get started.

147
00:12:12,080 --> 00:12:17,120
So this is what's called an inherently serial task. You can't easily do it much faster by

148
00:12:17,120 --> 00:12:21,680
adding more people. You can't get a baby in less than nine months by hiring two pregnant women.

149
00:12:21,680 --> 00:12:26,480
Most real world tasks are somewhere in between. You get some benefits from adding more people,

150
00:12:26,480 --> 00:12:31,040
but again, you hit diminishing returns. Some parts of the task can be split up and worked

151
00:12:31,040 --> 00:12:36,560
on in parallel. Some parts need to happen one after the other. So yes, corporations can achieve

152
00:12:36,560 --> 00:12:40,720
superhuman speed at some important cognitive tasks. But really, if you want to talk about

153
00:12:40,720 --> 00:12:45,120
speed in a principled way, you need to differentiate between throughput, how much

154
00:12:45,120 --> 00:12:50,000
goes through the system within a certain time, and latency, how long it takes a single thing

155
00:12:50,000 --> 00:12:53,920
to go through the system. These ideas are most often used in things like networking,

156
00:12:53,920 --> 00:12:58,160
and I think that's the easiest way to explain it. So basically, let's say you need to send someone

157
00:12:58,160 --> 00:13:02,320
a large file and you can either send it over a dial-up internet connection, or you can send

158
00:13:02,320 --> 00:13:07,440
them a physical disk through the postal system. The dial-up connection is low latency. Each bit

159
00:13:07,440 --> 00:13:11,920
of the file goes through the system quickly, but it's also low throughput. The rate at which you

160
00:13:11,920 --> 00:13:17,840
can send data is pretty low. Whereas sending the physical disk is high latency, it might take days

161
00:13:17,840 --> 00:13:22,560
for the first bit to arrive. But it's also high throughput. You can put vast amounts of data on

162
00:13:22,560 --> 00:13:27,760
the disk, so your average data sent per second could actually be very good. Corporations are

163
00:13:27,760 --> 00:13:33,120
able to combine human intelligences to achieve superhuman throughput, so they can complete large,

164
00:13:33,120 --> 00:13:38,480
complex tasks faster than individual humans could. But the thing is, a system can't have lower

165
00:13:38,480 --> 00:13:44,640
latency than its slowest component. And corporations are made of humans, so corporations aren't able to

166
00:13:44,640 --> 00:13:48,800
achieve superhuman latency. And in practice, as you've no doubt experienced, it's quite the

167
00:13:48,800 --> 00:13:53,760
opposite. So corporate intelligence is kind of like sending the physical disk. Corporations can

168
00:13:53,760 --> 00:13:58,800
get a lot of cognitive work done in a given time, but they're slow to react. And that's a big part

169
00:13:58,800 --> 00:14:04,400
of what makes corporations relatively controllable. They tend to react so slowly that even governments

170
00:14:04,400 --> 00:14:08,400
are sometimes able to move fast enough to deal with them. Software superintelligences, on the

171
00:14:08,400 --> 00:14:13,680
other hand, could have superhuman throughput and superhuman latency, which is something we've never

172
00:14:13,760 --> 00:14:19,200
experienced before in a general intelligence. So are corporations superintelligent agents?

173
00:14:20,000 --> 00:14:25,600
Well, they're pretty much generally intelligent agents, which are somewhat superintelligent in

174
00:14:25,600 --> 00:14:33,120
some ways and somewhat below human performance in others. So yeah, kind of. The next question is,

175
00:14:33,120 --> 00:14:37,120
are they misaligned? But this video is already like 14 and a half minutes long,

176
00:14:37,120 --> 00:14:46,560
so we'll get to that in the next video.

177
00:14:56,160 --> 00:14:59,600
I want to end the video by saying a big thank you to my excellent patrons,

178
00:14:59,600 --> 00:15:04,640
it's all of these people here. In this video, I'm especially thanking Pablo Eder,

179
00:15:04,640 --> 00:15:09,520
or Pablo Eder, nobody knows for sure. Recently I've been putting a lot of time into some projects

180
00:15:09,520 --> 00:15:13,840
that I'm not able to talk about, but as soon as I can, patrons will be the first to know.

181
00:15:13,840 --> 00:15:24,080
Thank you again so much for your generosity, and thank you all for watching. I'll see you next time.

