1
00:00:00,000 --> 00:00:05,320
Hi, this is the latest video in a series about the paper Concrete Problems in AI Safety.

2
00:00:05,320 --> 00:00:08,240
You don't need to have seen the previous videos for this, but I'd recommend checking

3
00:00:08,240 --> 00:00:10,440
them out anyway, there's a link in the description.

4
00:00:10,440 --> 00:00:13,280
Today we're going to talk about safe exploration.

5
00:00:13,280 --> 00:00:18,400
So in an earlier video, we talked about the trade-off between exploration and exploitation.

6
00:00:18,400 --> 00:00:21,840
This is kind of an inherent trade-off that all agents face, which just comes from the

7
00:00:21,840 --> 00:00:25,040
fact that you're trying to do two jobs at the same time.

8
00:00:25,040 --> 00:00:30,640
One, figure out what things give you reward, and two, do the things that give you reward.

9
00:00:30,640 --> 00:00:32,000
Like imagine you're in a restaurant.

10
00:00:32,000 --> 00:00:35,640
You've been to this restaurant before, so you've already tried some of the dishes.

11
00:00:35,640 --> 00:00:39,700
Now you can either order something you've already had that you know is quite good, i.e.

12
00:00:39,700 --> 00:00:43,520
you can exploit your current knowledge, or you can try ordering something new off the

13
00:00:43,520 --> 00:00:45,460
menu that you've never had before.

14
00:00:45,460 --> 00:00:47,780
You can explore to gain more knowledge.

15
00:00:47,780 --> 00:00:51,780
If you focus too much on exploring, then you're spending all of your time trying random things

16
00:00:51,780 --> 00:00:54,400
when actually you may have already found the thing that's best for you.

17
00:00:54,400 --> 00:00:57,860
But if you don't explore enough, then you might end up missing out on something great.

18
00:00:57,860 --> 00:01:00,060
Finding the right balance is an interesting problem.

19
00:01:00,060 --> 00:01:04,460
Now the most naive form of reinforcement learning is just to always do whichever action you

20
00:01:04,460 --> 00:01:06,700
expect will give you the most reward.

21
00:01:06,700 --> 00:01:10,620
But agents that work this way end up actually not doing very well, because as soon as they

22
00:01:10,620 --> 00:01:14,780
find something that works a bit, they just always do that forever and never try anything

23
00:01:14,780 --> 00:01:15,780
else.

24
00:01:15,780 --> 00:01:18,480
Like someone who just always orders the same thing at the restaurant, even though they

25
00:01:18,480 --> 00:01:20,900
haven't tried most of the other things on the menu.

26
00:01:20,900 --> 00:01:24,880
In the Gridworlds video, I explained that one approach to exploration in reinforcement

27
00:01:24,880 --> 00:01:29,200
learning is to have an exploration rate, so the system will choose an action which it

28
00:01:29,200 --> 00:01:34,440
thinks will give it the highest reward something like 99% of the time, but a random 1% of the

29
00:01:34,440 --> 00:01:37,220
time it will just pick an action completely at random.

30
00:01:37,220 --> 00:01:41,040
This way the system is generally doing whatever will maximise its reward, but it will still

31
00:01:41,040 --> 00:01:43,020
try new things from time to time.

32
00:01:43,020 --> 00:01:47,280
This is a pretty basic approach, and I think you can see how that could cause safety problems.

33
00:01:47,280 --> 00:01:51,500
Imagine a self-driving car, which 99% of the time does what it thinks is the best choice

34
00:01:51,500 --> 00:01:56,420
of action, and 1% of the time sets the steering wheel or the accelerator or the brake to a

35
00:01:56,420 --> 00:01:59,100
random value just to find out what would happen.

36
00:01:59,100 --> 00:02:03,660
That system might learn some interesting things about vehicle handling, but at what cost?

37
00:02:03,660 --> 00:02:05,540
Clearly, this is an unsafe approach.

38
00:02:05,540 --> 00:02:08,260
Okay, so that's a very simple way of doing exploration.

39
00:02:08,260 --> 00:02:09,440
There are other ways of doing it.

40
00:02:09,440 --> 00:02:11,980
One approach is a sort of artificial optimism.

41
00:02:11,980 --> 00:02:16,540
So rather than implicitly giving unknown actions zero expected reward, or whatever your best

42
00:02:16,540 --> 00:02:20,580
guess of the expected reward of taking a random action would be, you artificially

43
00:02:20,580 --> 00:02:25,140
give them high expected reward, so that the system is sort of irrationally optimistic

44
00:02:25,140 --> 00:02:26,500
about unknown things.

45
00:02:26,500 --> 00:02:30,020
Whenever there's anything it hasn't tried before, it will assume that it's good until

46
00:02:30,020 --> 00:02:31,860
it's tried it and found out that it isn't.

47
00:02:31,860 --> 00:02:35,960
So you end up with a system that's like those people who say, oh, I'll try anything once.

48
00:02:35,960 --> 00:02:38,340
That's not always a great approach in real life.

49
00:02:38,340 --> 00:02:41,320
There are a lot of things that you shouldn't try even once.

50
00:02:41,320 --> 00:02:44,980
And hopefully you can see that that kind of approach is unsafe for AI systems as well.

51
00:02:44,980 --> 00:02:48,620
You can't safely assume that anything you haven't tried must be good.

52
00:02:48,620 --> 00:02:52,800
Now it's worth noting that in more complex problems, these kinds of exploration methods

53
00:02:52,800 --> 00:02:57,900
that involve occasionally doing individual exploratory actions don't perform very well.

54
00:02:57,900 --> 00:03:02,460
In a complex problem space, you're pretty unlikely to find new and interesting approaches

55
00:03:02,460 --> 00:03:06,460
just by taking your current approach and applying some random permutation to it.

56
00:03:06,460 --> 00:03:11,460
So one approach that people use is to actually modify the goals of the system temporarily,

57
00:03:11,460 --> 00:03:14,820
to bring the system into new areas of the space that it hasn't been in before.

58
00:03:14,820 --> 00:03:18,260
Imagine that you're learning to play chess by playing against the computer, and you're

59
00:03:18,260 --> 00:03:20,060
kind of in a rut with your strategy.

60
00:03:20,060 --> 00:03:22,060
You're always playing similar looking games.

61
00:03:22,060 --> 00:03:26,020
So you might want to say to yourself, okay, this game, rather than my normal strategy,

62
00:03:26,020 --> 00:03:31,180
I'll just try to take as many of the opponent's pieces as possible, or this game, I'll just

63
00:03:31,180 --> 00:03:35,220
try to move my pieces as far across the board as possible, or I'll just try to capture the

64
00:03:35,220 --> 00:03:37,460
queen at all costs or something like that.

65
00:03:37,460 --> 00:03:41,860
You temporarily follow some new policy, which is not the one you'd usually think is best.

66
00:03:41,900 --> 00:03:45,140
And in doing that, you can end up visiting board states that you've never seen before

67
00:03:45,140 --> 00:03:49,460
and learning new things about the game, which in the long run can make you a better player.

68
00:03:49,460 --> 00:03:53,340
Temporarily modifying your goals allows you to explore the policy space better than you

69
00:03:53,340 --> 00:03:55,540
could by just sometimes playing a random move.

70
00:03:55,540 --> 00:03:58,860
But you can see how implementing this kind of thing on a real world AI system could be

71
00:03:58,860 --> 00:04:02,580
much more dangerous than just having your system sometimes choose random actions.

72
00:04:02,580 --> 00:04:06,380
If your cleaning robot occasionally makes totally random motor movements in an attempt

73
00:04:06,380 --> 00:04:10,100
to do exploration, that's mostly just going to make it less effective.

74
00:04:10,100 --> 00:04:14,780
It might drop things or fall over, that could be a bit dangerous, but what if it sometimes

75
00:04:14,780 --> 00:04:19,300
exhibited coherent goal-directed behavior towards randomly chosen goals?

76
00:04:19,300 --> 00:04:23,540
What if, as part of its exploration, it occasionally picks a new goal at random, and then puts

77
00:04:23,540 --> 00:04:27,020
together intelligent, multi-step plans to pursue that goal?

78
00:04:27,020 --> 00:04:29,900
That could be much more dangerous than just doing random things.

79
00:04:29,900 --> 00:04:33,420
And the problem doesn't come from the fact that the new goals are random, just that they're

80
00:04:33,420 --> 00:04:35,540
different from the original goals.

81
00:04:35,540 --> 00:04:37,660
Choosing non-randomly might not be any better.

82
00:04:37,660 --> 00:04:41,460
You might imagine an AI system where some part of the architecture is sort of implicitly

83
00:04:41,460 --> 00:04:46,460
reasoning something like, part of my goal is to avoid breaking this vase.

84
00:04:46,460 --> 00:04:50,000
But we've never actually seen the vase being broken, so the system doesn't have a very

85
00:04:50,000 --> 00:04:51,660
good understanding of how that happens.

86
00:04:51,660 --> 00:04:55,700
So maybe we should explore by temporarily replacing the goal with one that values breaking

87
00:04:55,700 --> 00:04:59,780
vases, just so that the system can break a bunch of vases and get a sense for how that

88
00:04:59,780 --> 00:05:01,220
works.

89
00:05:01,220 --> 00:05:04,940
Temporarily replacing the goal can make for good learning and effective exploration, but

90
00:05:04,940 --> 00:05:06,080
it's not safe.

91
00:05:06,080 --> 00:05:10,240
So the sorts of simple exploration methods that we're using with current systems can

92
00:05:10,240 --> 00:05:12,880
be dangerous when directly applied to the real world.

93
00:05:12,880 --> 00:05:14,880
Now that vase example was kind of silly.

94
00:05:14,880 --> 00:05:18,820
A system that's sophisticated enough to reason about its state of knowledge like that probably

95
00:05:18,820 --> 00:05:22,280
wouldn't need an architecture that swaps out its goals to force it to explore.

96
00:05:22,280 --> 00:05:25,040
It could just pursue exploration as an instrumental goal.

97
00:05:25,040 --> 00:05:28,480
And in fact, we'd expect exploration to be a convergent instrumental goal.

98
00:05:28,480 --> 00:05:31,520
And if you don't know what that means, watch the video on instrumental convergence.

99
00:05:31,520 --> 00:05:35,640
But basically, a general intelligence should choose exploratory actions just as a normal

100
00:05:35,640 --> 00:05:40,440
part of pursuing its goals, rather than having exploration hard-coded into the system's

101
00:05:40,440 --> 00:05:41,440
architecture.

102
00:05:41,440 --> 00:05:45,120
Such a system should be able to find ways to learn more about vases without actually

103
00:05:45,120 --> 00:05:46,120
smashing any.

104
00:05:46,120 --> 00:05:49,620
Perhaps it could read a book or watch a video and work things out from that.

105
00:05:49,620 --> 00:05:54,240
So I would expect unsafe exploration to mostly be a problem with relatively narrow systems

106
00:05:54,240 --> 00:05:56,080
operating in the real world.

107
00:05:56,080 --> 00:05:59,600
Our current AI systems and their immediate descendants, rather than something we need

108
00:05:59,600 --> 00:06:02,680
to worry about AGIs and superintelligence is doing.

109
00:06:02,680 --> 00:06:06,560
Given that this is more of a near-term problem, it's actually relatively well explored already.

110
00:06:06,560 --> 00:06:08,320
People have spent some time thinking about this.

111
00:06:08,320 --> 00:06:10,480
So what are the options for safe exploration?

112
00:06:10,480 --> 00:06:14,780
Well, one obvious thing to try is figuring out what unsafe actions your system might

113
00:06:14,780 --> 00:06:17,920
take while exploring, and then blacklisting those actions.

114
00:06:17,920 --> 00:06:21,800
So let's say you've got some kind of drone, like an AI-controlled quadcopter, that's

115
00:06:21,800 --> 00:06:25,200
flying around, and you want it to be able to explore the different ways it could fly.

116
00:06:25,200 --> 00:06:29,120
But this is unsafe because the system might explore manoeuvres like flying full speed

117
00:06:29,120 --> 00:06:30,120
into the ground.

118
00:06:30,120 --> 00:06:33,760
So what you can do is have the system take exploratory actions in whatever way you'd

119
00:06:33,760 --> 00:06:35,200
usually do it.

120
00:06:35,200 --> 00:06:40,160
But if the system enters a region of space that's too close to the ground, another system

121
00:06:40,160 --> 00:06:44,960
detects that and overrides the learning algorithm, flying the quadcopter higher, and then handing

122
00:06:44,960 --> 00:06:46,960
control back to the learning algorithm again.

123
00:06:46,960 --> 00:06:51,160
Kind of like the second set of controls they use when training humans to safely operate vehicles.

124
00:06:51,160 --> 00:06:55,520
Now bear in mind that here, for simplicity, I'm talking about blacklisting unsafe regions

125
00:06:55,520 --> 00:06:58,080
of the physical space that the quadcopter is in.

126
00:06:58,080 --> 00:07:00,040
But really this approach is broader than that.

127
00:07:00,040 --> 00:07:03,840
You're really blacklisting unsafe regions of the configuration space for the agent and

128
00:07:03,840 --> 00:07:04,840
its environment.

129
00:07:04,840 --> 00:07:09,200
It's not just about navigating a physical space, your system is navigating an abstract

130
00:07:09,200 --> 00:07:13,240
space of possibilities, and you can have a safety subsystem that takes over if the system

131
00:07:13,240 --> 00:07:15,800
tries to enter an unsafe region of that space.

132
00:07:15,800 --> 00:07:19,320
This can work quite well, as long as you know all of the unsafe things your system might

133
00:07:19,320 --> 00:07:21,040
do, and how to avoid them.

134
00:07:21,040 --> 00:07:24,480
Like okay, now it's not going to hit the ground, but it could still hit a tree, so

135
00:07:24,480 --> 00:07:27,840
your system would have to also keep track of where the trees are, and have a routine

136
00:07:27,840 --> 00:07:30,160
for safely moving out of that area as well.

137
00:07:30,160 --> 00:07:34,840
But the more complex the problem is, the harder it is to list out and specify every possible

138
00:07:34,840 --> 00:07:36,520
unsafe region of the space.

139
00:07:36,520 --> 00:07:40,720
So given that it might be extremely hard to specify every region of unsafe behaviour,

140
00:07:40,720 --> 00:07:42,600
you could try the opposite.

141
00:07:42,600 --> 00:07:44,120
Specify a region of safe behaviour.

142
00:07:44,120 --> 00:07:48,880
You could say, okay, the safe zone is anywhere above this altitude, the height of the tallest

143
00:07:48,880 --> 00:07:53,760
obstacles you might hit, and below this altitude, like the altitude of the lowest aircraft you

144
00:07:53,760 --> 00:07:59,080
might hit, and within this boundary, which is like the border of some empty field somewhere.

145
00:07:59,080 --> 00:08:01,600
Anywhere in this space is considered to be safe.

146
00:08:01,600 --> 00:08:06,240
So the system explores as usual in this area, and if it ever moves outside the area, the

147
00:08:06,240 --> 00:08:10,240
safety subsystem overrides it and takes it back into the safe area.

148
00:08:10,240 --> 00:08:14,320
Specifying a whitelisted area can be safer than specifying blacklisted areas, because

149
00:08:14,320 --> 00:08:17,920
you don't need to think of every possible bad thing that can happen, you just need to

150
00:08:17,920 --> 00:08:19,120
find a safe region.

151
00:08:19,120 --> 00:08:23,520
The problem is, your ability to check the space and ensure that it's safe is limited.

152
00:08:23,520 --> 00:08:26,840
Again, this needn't be a physical space, it's a configuration space.

153
00:08:26,840 --> 00:08:31,720
And as the system becomes more and more complicated, the configuration space becomes much larger,

154
00:08:31,720 --> 00:08:36,520
so the area that you're able to really know is safe becomes a smaller and smaller proportion

155
00:08:36,520 --> 00:08:39,260
of the actual available configuration space.

156
00:08:39,260 --> 00:08:42,800
This means you might be severely limiting what your system can do, since it can only

157
00:08:42,800 --> 00:08:45,120
explore a small corner of the options.

158
00:08:45,120 --> 00:08:48,320
If you try to make your safe region larger than the area that you're able to properly

159
00:08:48,320 --> 00:08:53,500
check, you risk including some dangerous configurations, so your system can then behave unsafely.

160
00:08:53,500 --> 00:08:56,940
But if you limit the safe region to the size that you're able to actually confirm is

161
00:08:56,940 --> 00:09:00,860
safe, your systems will be much less capable, since there are probably all kinds of good

162
00:09:00,860 --> 00:09:04,580
strategies that it's never going to be able to find, because they happen to lie outside

163
00:09:04,580 --> 00:09:06,940
of the space, despite being perfectly safe.

164
00:09:06,940 --> 00:09:10,540
The extreme case of this is where you have an expert demonstration, and then you have

165
00:09:10,540 --> 00:09:13,900
the system just try to copy what the expert did as closely as possible.

166
00:09:13,900 --> 00:09:17,280
Or perhaps you allow some small region of deviation from the expert demonstration.

167
00:09:17,280 --> 00:09:20,860
But that system is never going to do much better than the human expert, because it can't

168
00:09:20,860 --> 00:09:23,340
try anything too different from what humans do.

169
00:09:23,340 --> 00:09:27,180
In this case, you've removed almost all of the problems of safe exploration, by removing

170
00:09:27,180 --> 00:09:28,860
almost all of the exploration.

171
00:09:28,860 --> 00:09:32,820
So you can see this is another place where we have a trade-off between safety and capability.

172
00:09:32,820 --> 00:09:35,140
Alright, what other approaches are available?

173
00:09:35,140 --> 00:09:37,860
Well, human oversight is one that's often used.

174
00:09:37,860 --> 00:09:41,780
Self-driving cars have a human in them who can override the system, in principle.

175
00:09:41,780 --> 00:09:43,560
You can do the same with exploration.

176
00:09:43,560 --> 00:09:47,060
Have the system check with a human before doing each exploratory action.

177
00:09:47,060 --> 00:09:51,940
But as we talked about in the scalable supervision videos, this doesn't scale very well.

178
00:09:51,940 --> 00:09:55,560
The system might need to make millions of exploratory actions, and it's not practical

179
00:09:55,560 --> 00:09:57,940
to have a human check all of those.

180
00:09:57,940 --> 00:10:01,300
Or it might be a high-speed system that needs inhumanly fast oversight.

181
00:10:01,300 --> 00:10:05,540
If you need to make decisions about exploration in a split second, a human will be too slow

182
00:10:05,540 --> 00:10:07,260
to provide that supervision.

183
00:10:07,260 --> 00:10:08,460
So there's a synergy there.

184
00:10:08,460 --> 00:10:12,380
If we can improve the scalability of human supervision, that could help with safe exploration

185
00:10:12,380 --> 00:10:13,380
as well.

186
00:10:13,380 --> 00:10:16,260
And the last approach I'm going to talk about is simulation.

187
00:10:16,260 --> 00:10:19,220
This is a very popular approach, and it works quite well.

188
00:10:19,220 --> 00:10:22,540
If you do your exploration in a simulation, then even if it goes horribly wrong, it's

189
00:10:22,540 --> 00:10:23,580
not a problem.

190
00:10:23,580 --> 00:10:27,260
You can crash your simulated quadcopter right into your own simulated face, and it's no

191
00:10:27,260 --> 00:10:28,260
big deal.

192
00:10:28,260 --> 00:10:31,980
The problems with simulation probably deserve a whole video to themselves.

193
00:10:31,980 --> 00:10:34,580
But basically, there's always a simulation gap.

194
00:10:34,580 --> 00:10:39,060
It's extremely difficult to get simulations that accurately represent the problem domain.

195
00:10:39,060 --> 00:10:41,580
And the more complex the problem is, the harder this becomes.

196
00:10:41,580 --> 00:10:45,260
So learning in a simulation can limit the capabilities of your AI system.

197
00:10:45,300 --> 00:10:48,940
For example, when researchers were trying to see if an evolutionary algorithm could

198
00:10:48,940 --> 00:10:52,900
invent an electronic oscillator, a circuit that would generate a signal that repeats

199
00:10:52,900 --> 00:10:57,540
at a particular frequency, their system developed a very weird looking thing that clearly was

200
00:10:57,540 --> 00:11:01,940
not an oscillator circuit, but which somehow mysteriously produced a good oscillating

201
00:11:01,940 --> 00:11:02,940
output anyway.

202
00:11:02,940 --> 00:11:06,540
Now, you would think it was a bug in the simulation, but they weren't using simulation.

203
00:11:06,540 --> 00:11:07,940
The circuits physically existed.

204
00:11:07,940 --> 00:11:11,620
This circuit produced exactly the output they'd asked for, but they had no idea how it did

205
00:11:11,620 --> 00:11:12,620
it.

206
00:11:12,620 --> 00:11:15,420
Eventually, they figured out that it was actually a radio.

207
00:11:15,420 --> 00:11:20,380
It was picking up the very faint radio signals put out by the electronics of a nearby computer,

208
00:11:20,380 --> 00:11:22,380
and using that to generate the correct signal.

209
00:11:22,380 --> 00:11:26,900
The point is, this is a cool, unexpected solution to the problem, which would almost certainly

210
00:11:26,900 --> 00:11:28,500
not have been found in a simulation.

211
00:11:28,500 --> 00:11:33,060
I mean, would you think to include ambient radio noise in your oscillator circuit simulation?

212
00:11:33,060 --> 00:11:37,340
By doing its learning in a simulator, a system is only able to use the aspects of the world

213
00:11:37,340 --> 00:11:41,500
that we think are important enough to include in the simulation, which limits its ability

214
00:11:41,500 --> 00:11:44,020
to come up with things that we wouldn't have thought of.

215
00:11:44,020 --> 00:11:46,700
And that's a big part of why we want such systems in the first place.

216
00:11:46,700 --> 00:11:48,620
And this goes the other way as well, of course.

217
00:11:48,620 --> 00:11:52,860
It's not just that things in reality may be missing from your simulation, but your simulation

218
00:11:52,860 --> 00:11:56,900
will probably have some things that reality doesn't, i.e. bugs.

219
00:11:56,900 --> 00:12:00,860
The thing that makes this worse is that if you have a smart AI system, it's likely to

220
00:12:00,860 --> 00:12:05,540
end up actually seeking out the inaccuracies in your simulation, because the best solutions

221
00:12:05,540 --> 00:12:08,180
are likely to involve exploiting those bugs.

222
00:12:08,180 --> 00:12:11,660
Like if your physics simulation has any bugs in it, there's a good chance those bugs can

223
00:12:11,660 --> 00:12:16,260
be exploited to violate conservation of momentum, or to get free energy, or whatever.

224
00:12:16,260 --> 00:12:20,380
So it's not just that the simulation may not be accurate to reality, it's that most

225
00:12:20,380 --> 00:12:24,780
of the best solutions will lie in the parts of the configuration space where the simulation

226
00:12:24,780 --> 00:12:26,860
is the least accurate to reality.

227
00:12:26,860 --> 00:12:32,140
The general tendency for optimization to find the edges of systems, to find their limits,

228
00:12:32,140 --> 00:12:36,140
means that it's hard to be confident that actions which seem safe in a simulation will

229
00:12:36,140 --> 00:12:38,220
actually be safe in reality.

230
00:12:38,220 --> 00:12:43,460
At the end of the day, exploration is inherently risky, because almost by definition, it involves

231
00:12:43,460 --> 00:12:46,380
trying things without knowing exactly how it'll turn out.

232
00:12:46,380 --> 00:12:50,580
But there are ways of managing and minimizing that risk, and we need to find them so that

233
00:12:50,580 --> 00:12:52,740
our AI systems can explore safely.

234
00:12:52,740 --> 00:13:05,940
I want to end this video by saying thank you so much to my amazing patrons.

235
00:13:05,940 --> 00:13:08,420
It's all all of these people here.

236
00:13:08,420 --> 00:13:11,220
And in this video, I especially want to thank Scott Worley.

237
00:13:11,220 --> 00:13:14,820
Thank you all so much for sticking with me through this giant gap in uploads.

238
00:13:14,820 --> 00:13:18,660
When I do upload videos to this channel or the second channel, patrons get to see them

239
00:13:18,660 --> 00:13:20,260
a few days before everyone else.

240
00:13:20,260 --> 00:13:24,780
And I'm also posting the videos I make for the online AI safety course that I'm helping

241
00:13:24,780 --> 00:13:27,460
to develop, and occasional behind the scenes videos too.

242
00:13:27,460 --> 00:13:31,060
Like right now, I'm putting together a video about my visit to the Electromagnetic Field

243
00:13:31,060 --> 00:13:34,900
Festival this year, where I gave a talk and actually met some of you in person, which

244
00:13:34,900 --> 00:13:35,900
was fun.

245
00:13:35,900 --> 00:13:37,660
Anyway, thank you again for your support.

246
00:13:37,660 --> 00:13:38,660
And thank you all for watching.

247
00:13:38,660 --> 00:13:39,540
I'll see you soon.

