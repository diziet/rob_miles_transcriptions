Hi, this is the latest video in a series about the paper Concrete Problems in AI Safety. You don't need to have seen the previous videos for this, but I'd recommend checking them out anyway, there's a link in the description. Today we're going to talk about safe exploration. So in an earlier video, we talked about the trade-off between exploration and exploitation. This is kind of an inherent trade-off that all agents face, which just comes from the fact that you're trying to do two jobs at the same time. One, figure out what things give you reward, and two, do the things that give you reward. Like imagine you're in a restaurant. You've been to this restaurant before, so you've already tried some of the dishes. Now you can either order something you've already had that you know is quite good, i.e. you can exploit your current knowledge, or you can try ordering something new off the menu that you've never had before. You can explore to gain more knowledge. If you focus too much on exploring, then you're spending all of your time trying random things when actually you may have already found the thing that's best for you. But if you don't explore enough, then you might end up missing out on something great. Finding the right balance is an interesting problem. Now the most naive form of reinforcement learning is just to always do whichever action you expect will give you the most reward. But agents that work this way end up actually not doing very well, because as soon as they find something that works a bit, they just always do that forever and never try anything else. Like someone who just always orders the same thing at the restaurant, even though they haven't tried most of the other things on the menu. In the Gridworlds video, I explained that one approach to exploration in reinforcement learning is to have an exploration rate, so the system will choose an action which it thinks will give it the highest reward something like 99% of the time, but a random 1% of the time it will just pick an action completely at random. This way the system is generally doing whatever will maximise its reward, but it will still try new things from time to time. This is a pretty basic approach, and I think you can see how that could cause safety problems. Imagine a self-driving car, which 99% of the time does what it thinks is the best choice of action, and 1% of the time sets the steering wheel or the accelerator or the brake to a random value just to find out what would happen. That system might learn some interesting things about vehicle handling, but at what cost? Clearly, this is an unsafe approach. Okay, so that's a very simple way of doing exploration. There are other ways of doing it. One approach is a sort of artificial optimism. So rather than implicitly giving unknown actions zero expected reward, or whatever your best guess of the expected reward of taking a random action would be, you artificially give them high expected reward, so that the system is sort of irrationally optimistic about unknown things. Whenever there's anything it hasn't tried before, it will assume that it's good until it's tried it and found out that it isn't. So you end up with a system that's like those people who say, oh, I'll try anything once. That's not always a great approach in real life. There are a lot of things that you shouldn't try even once. And hopefully you can see that that kind of approach is unsafe for AI systems as well. You can't safely assume that anything you haven't tried must be good. Now it's worth noting that in more complex problems, these kinds of exploration methods that involve occasionally doing individual exploratory actions don't perform very well. In a complex problem space, you're pretty unlikely to find new and interesting approaches just by taking your current approach and applying some random permutation to it. So one approach that people use is to actually modify the goals of the system temporarily, to bring the system into new areas of the space that it hasn't been in before. Imagine that you're learning to play chess by playing against the computer, and you're kind of in a rut with your strategy. You're always playing similar looking games. So you might want to say to yourself, okay, this game, rather than my normal strategy, I'll just try to take as many of the opponent's pieces as possible, or this game, I'll just try to move my pieces as far across the board as possible, or I'll just try to capture the queen at all costs or something like that. You temporarily follow some new policy, which is not the one you'd usually think is best. And in doing that, you can end up visiting board states that you've never seen before and learning new things about the game, which in the long run can make you a better player. Temporarily modifying your goals allows you to explore the policy space better than you could by just sometimes playing a random move. But you can see how implementing this kind of thing on a real world AI system could be much more dangerous than just having your system sometimes choose random actions. If your cleaning robot occasionally makes totally random motor movements in an attempt to do exploration, that's mostly just going to make it less effective. It might drop things or fall over, that could be a bit dangerous, but what if it sometimes exhibited coherent goal-directed behavior towards randomly chosen goals? What if, as part of its exploration, it occasionally picks a new goal at random, and then puts together intelligent, multi-step plans to pursue that goal? That could be much more dangerous than just doing random things. And the problem doesn't come from the fact that the new goals are random, just that they're different from the original goals. Choosing non-randomly might not be any better. You might imagine an AI system where some part of the architecture is sort of implicitly reasoning something like, part of my goal is to avoid breaking this vase. But we've never actually seen the vase being broken, so the system doesn't have a very good understanding of how that happens. So maybe we should explore by temporarily replacing the goal with one that values breaking vases, just so that the system can break a bunch of vases and get a sense for how that works. Temporarily replacing the goal can make for good learning and effective exploration, but it's not safe. So the sorts of simple exploration methods that we're using with current systems can be dangerous when directly applied to the real world. Now that vase example was kind of silly. A system that's sophisticated enough to reason about its state of knowledge like that probably wouldn't need an architecture that swaps out its goals to force it to explore. It could just pursue exploration as an instrumental goal. And in fact, we'd expect exploration to be a convergent instrumental goal. And if you don't know what that means, watch the video on instrumental convergence. But basically, a general intelligence should choose exploratory actions just as a normal part of pursuing its goals, rather than having exploration hard-coded into the system's architecture. Such a system should be able to find ways to learn more about vases without actually smashing any. Perhaps it could read a book or watch a video and work things out from that. So I would expect unsafe exploration to mostly be a problem with relatively narrow systems operating in the real world. Our current AI systems and their immediate descendants, rather than something we need to worry about AGIs and superintelligence is doing. Given that this is more of a near-term problem, it's actually relatively well explored already. People have spent some time thinking about this. So what are the options for safe exploration? Well, one obvious thing to try is figuring out what unsafe actions your system might take while exploring, and then blacklisting those actions. So let's say you've got some kind of drone, like an AI-controlled quadcopter, that's flying around, and you want it to be able to explore the different ways it could fly. But this is unsafe because the system might explore manoeuvres like flying full speed into the ground. So what you can do is have the system take exploratory actions in whatever way you'd usually do it. But if the system enters a region of space that's too close to the ground, another system detects that and overrides the learning algorithm, flying the quadcopter higher, and then handing control back to the learning algorithm again. Kind of like the second set of controls they use when training humans to safely operate vehicles. Now bear in mind that here, for simplicity, I'm talking about blacklisting unsafe regions of the physical space that the quadcopter is in. But really this approach is broader than that. You're really blacklisting unsafe regions of the configuration space for the agent and its environment. It's not just about navigating a physical space, your system is navigating an abstract space of possibilities, and you can have a safety subsystem that takes over if the system tries to enter an unsafe region of that space. This can work quite well, as long as you know all of the unsafe things your system might do, and how to avoid them. Like okay, now it's not going to hit the ground, but it could still hit a tree, so your system would have to also keep track of where the trees are, and have a routine for safely moving out of that area as well. But the more complex the problem is, the harder it is to list out and specify every possible unsafe region of the space. So given that it might be extremely hard to specify every region of unsafe behaviour, you could try the opposite. Specify a region of safe behaviour. You could say, okay, the safe zone is anywhere above this altitude, the height of the tallest obstacles you might hit, and below this altitude, like the altitude of the lowest aircraft you might hit, and within this boundary, which is like the border of some empty field somewhere. Anywhere in this space is considered to be safe. So the system explores as usual in this area, and if it ever moves outside the area, the safety subsystem overrides it and takes it back into the safe area. Specifying a whitelisted area can be safer than specifying blacklisted areas, because you don't need to think of every possible bad thing that can happen, you just need to find a safe region. The problem is, your ability to check the space and ensure that it's safe is limited. Again, this needn't be a physical space, it's a configuration space. And as the system becomes more and more complicated, the configuration space becomes much larger, so the area that you're able to really know is safe becomes a smaller and smaller proportion of the actual available configuration space. This means you might be severely limiting what your system can do, since it can only explore a small corner of the options. If you try to make your safe region larger than the area that you're able to properly check, you risk including some dangerous configurations, so your system can then behave unsafely. But if you limit the safe region to the size that you're able to actually confirm is safe, your systems will be much less capable, since there are probably all kinds of good strategies that it's never going to be able to find, because they happen to lie outside of the space, despite being perfectly safe. The extreme case of this is where you have an expert demonstration, and then you have the system just try to copy what the expert did as closely as possible. Or perhaps you allow some small region of deviation from the expert demonstration. But that system is never going to do much better than the human expert, because it can't try anything too different from what humans do. In this case, you've removed almost all of the problems of safe exploration, by removing almost all of the exploration. So you can see this is another place where we have a trade-off between safety and capability. Alright, what other approaches are available? Well, human oversight is one that's often used. Self-driving cars have a human in them who can override the system, in principle. You can do the same with exploration. Have the system check with a human before doing each exploratory action. But as we talked about in the scalable supervision videos, this doesn't scale very well. The system might need to make millions of exploratory actions, and it's not practical to have a human check all of those. Or it might be a high-speed system that needs inhumanly fast oversight. If you need to make decisions about exploration in a split second, a human will be too slow to provide that supervision. So there's a synergy there. If we can improve the scalability of human supervision, that could help with safe exploration as well. And the last approach I'm going to talk about is simulation. This is a very popular approach, and it works quite well. If you do your exploration in a simulation, then even if it goes horribly wrong, it's not a problem. You can crash your simulated quadcopter right into your own simulated face, and it's no big deal. The problems with simulation probably deserve a whole video to themselves. But basically, there's always a simulation gap. It's extremely difficult to get simulations that accurately represent the problem domain. And the more complex the problem is, the harder this becomes. So learning in a simulation can limit the capabilities of your AI system. For example, when researchers were trying to see if an evolutionary algorithm could invent an electronic oscillator, a circuit that would generate a signal that repeats at a particular frequency, their system developed a very weird looking thing that clearly was not an oscillator circuit, but which somehow mysteriously produced a good oscillating output anyway. Now, you would think it was a bug in the simulation, but they weren't using simulation. The circuits physically existed. This circuit produced exactly the output they'd asked for, but they had no idea how it did it. Eventually, they figured out that it was actually a radio. It was picking up the very faint radio signals put out by the electronics of a nearby computer, and using that to generate the correct signal. The point is, this is a cool, unexpected solution to the problem, which would almost certainly not have been found in a simulation. I mean, would you think to include ambient radio noise in your oscillator circuit simulation? By doing its learning in a simulator, a system is only able to use the aspects of the world that we think are important enough to include in the simulation, which limits its ability to come up with things that we wouldn't have thought of. And that's a big part of why we want such systems in the first place. And this goes the other way as well, of course. It's not just that things in reality may be missing from your simulation, but your simulation will probably have some things that reality doesn't, i.e. bugs. The thing that makes this worse is that if you have a smart AI system, it's likely to end up actually seeking out the inaccuracies in your simulation, because the best solutions are likely to involve exploiting those bugs. Like if your physics simulation has any bugs in it, there's a good chance those bugs can be exploited to violate conservation of momentum, or to get free energy, or whatever. So it's not just that the simulation may not be accurate to reality, it's that most of the best solutions will lie in the parts of the configuration space where the simulation is the least accurate to reality. The general tendency for optimization to find the edges of systems, to find their limits, means that it's hard to be confident that actions which seem safe in a simulation will actually be safe in reality. At the end of the day, exploration is inherently risky, because almost by definition, it involves trying things without knowing exactly how it'll turn out. But there are ways of managing and minimizing that risk, and we need to find them so that our AI systems can explore safely. I want to end this video by saying thank you so much to my amazing patrons. It's all all of these people here. And in this video, I especially want to thank Scott Worley. Thank you all so much for sticking with me through this giant gap in uploads. When I do upload videos to this channel or the second channel, patrons get to see them a few days before everyone else. And I'm also posting the videos I make for the online AI safety course that I'm helping to develop, and occasional behind the scenes videos too. Like right now, I'm putting together a video about my visit to the Electromagnetic Field Festival this year, where I gave a talk and actually met some of you in person, which was fun. Anyway, thank you again for your support. And thank you all for watching. I'll see you soon. 