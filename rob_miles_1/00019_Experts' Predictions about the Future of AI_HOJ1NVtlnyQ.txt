Hi, there's a lot of disagreement about the future of AI, but there's also a lot of disagreement about what the experts think about the future of AI. I sometimes hear people saying that all of this concern about AI risk just comes from watching too much sci-fi and the actual AI researchers aren't worried about it at all. When it comes to timelines, some people will claim that the experts agree that AGI is hundreds of years away. Prediction, as they say, is very difficult, especially about the future, and that's because we don't have data about it yet. But expert opinion about the future exists in the present, so we can do science on it. We can survey the experts. We can find the expert consensus, and that's what this paper is trying to do. It's called, When Will AI Exceed Human Performance? Evidence from AI Experts. So these researchers from the Future of Humanity Institute at the University of Oxford, the AI Impacts Project, and Yale University ran a survey. They asked every researcher who published in ICML or NIPS in 2015. Those two are pretty much the most prestigious AI conferences right now. So this survey got 352 of the top AI researchers and asked them all sorts of questions about the future of AI, and the experts all agreed that they did not agree with each other, and Robert Allman didn't even agree with that. There was a lot of variation in people's predictions, but that's to be expected, and the paper uses statistical methods to aggregate these opinions into something we can use. For example, here's the graph showing when the respondents think we'll achieve high-level machine intelligence, which is defined as when unaided machines can accomplish every task better and more cheaply than human workers. So that's roughly equivalent to what I mean when I say superintelligence. You can see these gray lines show how the graph would look with different randomly chosen subsets of the forecasts, and there's a lot of variation there. But the aggregate forecast in red shows that, overall, the experts think we'll pass 50% chance of achieving high-level machine intelligence about 45 years from now. Well, that's from 2016, so more like 43 years from now. And they give a 10% chance of it happening within nine years, which is seven years now. So it's probably not too soon to be concerned about it. A quick side point about surveys, by the way. In a 2010 poll, 44% of Americans said that they supported homosexuals serving openly in the military. In the same poll, 58% of respondents said that they supported gay men and lesbians serving openly in the military. Implicitly, 14% of respondents supported gay men and lesbians, but did not support homosexuals. Something similar seems to be going on in this survey, because when the researchers were asked when they thought all occupations would be fully automatable, defined as, for any occupation, machines could be built to carry out the task better and more cheaply than human workers, they gave their 50% estimate at 122 years, compared to 45 for high-level machine intelligence. These are very similar questions. From this, we can conclude that AI experts are really uncertain about this, and precise wording in surveys can have a surprisingly big effect on the results. Figure two in the paper shows the median estimates for lots of different AI milestones. This is really interesting because it gives a nice overview of how difficult AI researchers expect these different things to be. For example, human-level StarCraft play seems like it will take about as long as human-level laundry folding. Also interesting here is the game of Go. Remember, this is before AlphaGo. The AI experts expected Go to take about 12 years, and that's why AlphaGo was such a big deal. It was about 11 years ahead of people's expectations. But what milestone is at the top? What task do the AI researchers think will take the longest to achieve, longer even than high-level machine intelligence that's able to do all human tasks? That's right, it's AI research. Anyway, onto questions of safety and risk. This section is for those who think that people like me should stop making a fuss about AI safety, because the AI experts all agree that it's not a problem. First of all, the AI experts don't all agree about anything, but let's look at the questions. This one asks about the expected outcome of high-level machine intelligence. The researchers are fairly optimistic overall, giving on average a 25% chance for a good outcome and a 20% chance for an extremely good outcome. But they nonetheless gave a 10% chance for a bad outcome and 5% for an outcome described as extremely bad, for example, human extinction. 5% chance of human extinction-level badness is a cause for concern. Moving on, this question asks the experts to read Stuart Russell's argument for why highly advanced AI might pose a risk. This is very closely related to the arguments I've been making on YouTube. It says, the primary concern with highly advanced AI is not spooky emergent consciousness, but simply the ability to make high-quality decisions. Here, quality refers to the expected outcome utility of actions taken. Now we have a problem. One, the utility function may not be perfectly aligned with the values of the human race, which are, at best, very difficult to pin down. Two, any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources, not for their own sake, but to succeed in its assigned task. A system that is optimizing a function of n variables, where the objective depends on a subset of size k less than n, will often set the remaining unconstrained variables to extreme values. If one of those unconstrained values is actually something we care about, the solution may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas. You get exactly what you ask for, not what you want. So do the AI experts agree with that? Well, 11% of them think, no, it's not a real problem. 19% think, no, it's not an important problem. But the remainder, 70% of the AI experts, agree that this is at least a moderately important problem. And how much do the AI experts think that society should prioritize AI safety research? Well, 48% of them think we should prioritize it more than we currently are. And only 11% think we should prioritize it less. So there we are. AI experts are very unclear about what the future holds, but they think that catastrophic risks are possible, and that this is an important problem. So we need to do more AI safety research. I want to end the video by saying thank you so much to my excellent Patreon supporters. These people. And in this video, I'm especially thanking Jason Heiss, who's been a patron for a while now. We've had some quite interesting discussions over Patreon chat. It's been fun. So thank you, Jason, and thank you all for watching. I'll see you next time. 