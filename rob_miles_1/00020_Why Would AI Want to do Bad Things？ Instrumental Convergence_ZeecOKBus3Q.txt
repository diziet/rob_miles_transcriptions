Hi. So sometimes people ask how I can predict what an artificial general intelligence might do. They say something like, you seem to be predicting that AIs would take these quite specific actions, trying to prevent themselves from being turned off, preventing themselves from being modified, improving themselves, acquiring resources. Why do you think that an AGI would have these specific goals? Surely we haven't built the thing yet. We don't know anything about it. It might want anything. Aren't you making a lot of unwarranted assumptions? Well, the reason that I make these predictions is because of something called instrumental convergence, which actually relies on surprisingly few assumptions. The main assumption I'm making is that an AGI will behave like an agent. An agent is basically just a thing that has goals or preferences, and it takes actions to achieve those goals or to satisfy those preferences. So the simplest thing that you might think of as an agent is something like a thermostat. It has a goal, which is for the room to be at a particular temperature, and it has actions it can take in the form of turning on the heating or turning on the air conditioning. And it takes actions to achieve its goal of keeping the room at a particular temperature. That's like an extremely simple agent. A slightly more complex agent might be something like a chess AI. Its goal is to win the chess game, to have the opponent's king in checkmate. It can take actions in the form of moving its pieces on the board, and it chooses its actions in order to achieve the goal of winning the game. The idea of an agent is popular in economics, where it's common to model companies and individual human beings as rational agents. For the sake of simplicity, it's often assumed that the goal of human beings is to acquire money, that their utility is just proportional to how much money they have. This is obviously a huge oversimplification, and it's a very popular fact that most people are motivated by a lot of things, not just money. But while it's easy to point out the shortcomings that this assumption has, what's remarkable to me is how well it works, or that it even works at all. It's true that most people use a very complex utility function, which looks nothing like the basic goal of get as much money as you can. But surprisingly, even very simple economic models that rely on this simplifying assumption can provide some really useful and powerful tools for thinking about the behaviour of people, companies, and societies. Why is that? I think it has to do with the nature of terminal goals and instrumental goals. I talked about this in a lot more detail in the orthogonality thesis video, which I would recommend checking out if you haven't seen it yet, but to give a quick summary, your terminal goals are the things that you want just because you want them. You don't have a reason to want them particularly, they're just what you want. Whereas instrumental goals are goals that you want as a way of getting some other goal. So for example, in chess, you want to take your opponent's queen, not because you just love capturing the queen, but because you can tell that you're more likely to win if you've captured your opponent's queen than if you haven't. So capturing the queen is an instrumental goal towards the goal of winning the game. So how does this work with money? Well, let's imagine that there's a total stranger and you don't know what he wants out of life. You don't know what his goals are. Maybe he wants to win a marathon, maybe he wants to cure cancer, maybe he just wants a really nice stamp collection. But I can predict that if I were to go over to him and offer him a big stack of money, he'd be happy to take it. How can I predict this person's actions even though I don't know his goals? Well, a guy who wants to win a marathon, if I give him money he could buy some nice running shoes or he could hire a personal trainer or something like that. A guy who wants to cure cancer could give that money to a cancer charity or maybe use it to help him to go to university and study science to cure cancer himself. And a guy who wants to collect stamps could buy some stamps. So the point is, even though none of these people value money as a terminal goal, none of them want money just for the sake of having money. Nonetheless, they all value money as an instrumental goal. The money is a way to get them closer to their goals. And even though they all have different terminal goals, this goal of getting money happens to be instrumentally valuable for all of their different goals. That makes getting money a convergent instrumental goal. It's a goal that's an instrumental goal for a wide variety of different terminal goals. So since money is a convergent instrumental goal, if you make the assumption that everybody values money, it turns out to be a fairly good assumption. Because whatever you value, money is going to help you with that. And that makes this assumption useful for making predictions because it allows you to predict people's behavior to some extent without knowing their goals. So what other convergent instrumental goals are there? Well, an obvious one is self-preservation. Most agents with most goals will try to prevent themselves from being destroyed. Now, something like a thermostat or a chess AI, they're not self-aware. They don't understand that they can be destroyed, and so they won't take any actions to avoid it. But if an agent is sophisticated enough to understand that being destroyed is a possibility, then avoiding destruction is a convergent instrumental goal. Humans, of course, generally act to avoid being killed. But humans, as evolved agents, implement this in a way that might obscure the nature of the thing. The point is that self-preservation need not be a terminal goal. An agent need not necessarily value self-preservation just for the sake of continuing to exist for its own sake. For example, suppose you had a software agent, an AGI, which had a single terminal goal of making as many paperclips as possible. It would try to prevent you from turning it off, not because it wants to live, but simply because it can predict that future worlds in which it's turned off will contain far fewer paperclips, and it wants to maximize the number of paperclips. But suppose you said to it, I've thought of a better way to implement your software that will be more effective at making paperclips. So I'm going to turn you off and wipe all of your memory and then create a new version that's better at making paperclips. This is pretty clearly the destruction of that first agent, right? You're wiping all of its memories and creating a new system that works differently, that thinks differently. But the paperclip agent would be fine with this because it can see that when you turn on its replacement, that will end up resulting in more paperclips overall. So it's not really about self-preservation itself. It's just that, practically speaking, most of the time, you can't achieve your goals if you're dead. On the other hand, suppose we were going to turn off the agent and change its goal. We were going to change it so that it doesn't like paperclips anymore. It actually wants to collect stamps. Here, you're not really destroying the agent, just modifying it. But you've got a problem again, because the agent can reliably predict that if this modification happens and it becomes a stamp collector, the future will contain not nearly as many paperclips, and that's the only thing it cares about right now. So we've got another convergent instrumental goal, goal preservation. Most agents with most goals, again, if they're sophisticated enough to realize that it's a possibility, will try to protect their goals from being modified. Because if you get new goals, you'll stop pursuing your current goals. So you're unlikely to achieve your current goals. Now for humans, this doesn't come up much because modifying a human's goals is fairly difficult. But still, if you suppose we were to go to the guy who wants to cure cancer and offer him some magic pill that's going to change his brain so that he doesn't care about cancer anymore, and he actually wants to collect stamps. If we say, look, this is actually really good because cancer is really hard to cure. It's actually a large collection of different diseases, all of which need different approaches. So you're very unlikely to discover a cure for all of them. But on the other hand, stamp collecting is great. You can just go out and buy some stamps and you can put them in a book and look at them. I don't really know what stamp collectors do, is that? Anyway, is this guy who wants to cure cancer going to take that pill? No, he's going to say, hell no, I'm not taking your crazy stamp pill, even though this isn't a terminal goal for him. He doesn't value valuing curing cancer, right? He doesn't have a goal of having a goal of curing cancer. It's just that he believes that if he gave up curing cancer to become a stamp collector, that would result in a lower chance of cancer being cured. So preserving your terminal goals is instrumentally useful, whatever those goals are. Another convergent instrumental goal is self-improvement. Notice that the guy who wants to cure cancer, part of his plan is to go to university and study science so that he can learn how to research cancer cures. And the guy who wants to run a marathon, part of his plan is he wants to train and improve his physical performance. Both of these things are improving yourself, and something like this comes up quite often in human plans. Again, this isn't a terminal goal. The guy who wants to cure cancer doesn't want to get a degree just because he wants a degree. He wants to become a person who's more effective at curing cancer. Now, there's another way of improving yourself, which is not really available to human beings, but is available to AI systems, which is directly modifying your mind to improve your own intelligence. You're not just adding information to your mind, you're actually making it more powerful. For example, you might be able to rewrite your code so that it works better or runs faster, or you might be able to just acquire more computing power. The more computing power you have, the deeper and faster you're able to think, the better you are at making plans, and therefore the better you are at achieving your goal, whatever that goal is. So computing power is kind of like money. It's a resource which is just very broadly useful, which means we can expect acquiring that resource to be a convergent instrumental goal. And speaking in the broadest possible terms, almost all plans need resources in the form of matter and energy. If you want to build something, whether that's stamps or paperclips or computing hardware or robots or whatever, you need matter to build it out of, and energy to build it with, and probably energy to run it as well. So we can expect agents with a wide range of goals to try to acquire a lot of resources. So without really assuming anything about an AGI other than that it will have goals and act to achieve those goals, we can see that it's likely that it would try to prevent itself from being shut down, try to prevent itself from being modified in important ways that we want to modify it, try to improve itself and its intelligence to become more powerful, and try to acquire a lot of resources. This is the case for almost all terminal goals, so we can expect any generally intelligent software agents that we create to display this kind of behavior, unless we can specifically design them not to. I want to end the video by saying thank you to all of my excellent Patreon supporters. It's these, these people. And in this video I'm especially thanking James McEwan, who looks a lot like the Stamford Bunny, and has been a patron for nine months now. Thank you so much for all of your support, and thank you all for watching. I'll see you next time. 