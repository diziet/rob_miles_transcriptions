Hi. This video is kind of a response to various comments that I've got over the years, ever since that video on Computerphile, where I was describing the sort of problems that we might have when we have a powerful artificial general intelligence with goals which aren't the same as our goals, even if those goals seem pretty benign. We used this thought experiment of an extremely powerful AGI working to optimize the simple goal of collecting stamps, and some of the problems that that might cause. I got some comments from people saying that they think the stamp collecting device is stupid, and not that it's a stupid thought experiment, but the device itself is actually stupid. They said unless it has complex goals or the ability to choose its own goals, then it didn't count as being highly intelligent. On other videos, I got comments saying it takes intelligence to do moral reasoning, so an intelligent AGI system should be able to do that, and a superintelligent should be able to do it better than humans. In fact, if a superintelligence decides that the right thing to do is to kill us all, then I guess that's the right thing to do. These comments are all kind of suffering from the same mistake, which is what this video is about. But before I get to that, I need to lay some groundwork first. If you like Occam's razor, then you'll love Hume's guillotine, also called the is-ought problem. This is a pretty simple concept that I'd like to be better known. The idea is, statements can be divided up into two types, is-statements and ought-statements. Is-statements, or positive statements, are statements about how the world is, how the world was in the past, how the world will be in the future, or how the world would be in hypothetical situations. This is facts about the nature of reality, the causal relationships between things, that kind of thing. Then you have the ought-statements, the should-statements, the normative statements. These are about the way the world should be, the way we want the world to be. Statements about our goals, our values, ethics, morals, what we want, all of that stuff. Now, you can derive logical statements from one another, like, it's snowing outside, that's an is-statement. It's cold when it snows, another is-statement. And then you can deduce, therefore, it's cold outside. That's another is-statement as our conclusion. This is all pretty obvious. But you might say something like, it's snowing outside, therefore, you ought to put on a coat. And that's a very normal sort of sentence that people might say, but as a logical statement, it actually relies on some hidden assumptions. Without assuming some kind of ought-statement, you can't derive another ought-statement. This is the core of the is-ought problem. You can never derive an ought-statement using only is-statements. You ought to put on a coat. Why? Because it's snowing outside. So why does the fact that it's snowing mean I should put on a coat? Well, the fact that it's snowing means that it's cold. And why should it being cold mean I should put on a coat? If it's cold and you go outside without a coat, you'll be cold. Should I not be cold? Well, if you get too cold, you'll freeze to death. Okay, you're saying I shouldn't freeze to death? That was kind of silly. But you see what I'm saying? You can keep laying out is-statements for as long as you want. You will never be able to derive that you ought to put on a coat. At some point, in order to derive that ought-statement, you need to assume at least one other ought-statement. If you have some kind of ought-statement like, I ought to continue to be alive, you can then say, given that I ought to keep living, and that if I go outside without a coat, I'll die, then I ought to put on a coat. But unless you have at least one ought-statement, you cannot derive any other ought-statements. Is-statements and ought-statements are separated by Hume's guillotine. Okay, so people are saying that a device that single-mindedly collects stamps at the cost of everything else is stupid and doesn't count as a powerful intelligence. So let's define our terms. What is intelligence? And conversely, what is stupidity? I feel like I made fairly clear in those videos what I meant by intelligence. We're talking about AGI systems as intelligent agents. They're entities that take actions in the world in order to achieve their goals or maximize their utility functions. Intelligence is the thing that allows them to choose good actions, to choose actions that will get them what they want. An agent's level of intelligence really means its level of effectiveness at pursuing its goals. In practice, this is likely to involve having or building an accurate model of reality, keeping that model up to date by reasoning about observations, and using the model to make predictions about the future and the likely consequences of different possible actions to figure out which actions will result in which outcomes. Intelligence involves answering questions like, what is the world like? How does it work? What will happen next? What would happen in this scenario or that scenario? What would happen if I took this action or that action? More intelligent systems are in some sense better at answering these kinds of questions, which allows them to be better at choosing actions. But one thing you might notice about these questions is they're all is questions. The system has goals, which can be thought of as ought statements, but the level of intelligence depends only on the ability to reason about is questions in order to answer the single ought question, what action should I take next? So given that that's what we mean by intelligence, what does it mean to be stupid? Well, firstly, you can be stupid in terms of those is questions. For example, by building a model that doesn't correspond with reality or by failing to update your model properly with new evidence. If I look out of my window and I see there's snow everywhere, you know, I see a snowman and I think to myself, oh, what a beautiful, warm, sunny day. Then that's stupid, right? My belief is wrong. And I had all the clues to realize it's cold outside. So beliefs can be stupid by not corresponding to reality. What about actions? Like if I go outside in the snow without my coat, that's stupid, right? Well, it might be. If I think it's sunny and warm and I go outside to sunbathe, then yeah, that's stupid. But if I just came out of a sauna or something, and I'm too hot and I want to cool myself down, then going outside without a coat might be quite sensible. You can't know if an action is stupid just by looking at its consequences. You have to also know the goals of the agent taking the action. You can't just use is statements. You need an ought. So actions are only stupid relative to a particular goal. It doesn't feel that way though. People often talk about actions being stupid without specifying what goals they're stupid relative to. But in those cases, the goals are implied. We're humans. And when we say that an action is stupid in normal human communication, we're making some assumptions about normal human goals. And because we're always talking about people and people tend to want similar things, it's sort of a shorthand that we can skip what goals we're talking about. So what about the goals then? Can goals be stupid? Well, this depends on the difference between instrumental goals and terminal goals. This is something I've covered elsewhere, but your terminal goals are the things that you want just because you want them. You don't have a particular reason to want them. They're just what you want. The instrumental goals are the goals you want because they'll get you closer to your terminal goals. Like if I have a terminal goal to visit a town that's far away, maybe an instrumental goal would be to find a train station. I don't want to find a train station just because trains are cool. I want to find a train as a means to an end. It's going to take me to this town. So that makes it an instrumental goal. Now an instrumental goal can be stupid. If I want to go to this distant town, so I decide I want to find a pogo stick, that's pretty stupid. Finding a pogo stick is a stupid instrumental goal if my terminal goal is to get to a faraway place. But if my terminal goal was something else, like having fun, it might not be stupid. So in that way, it's like actions. Instrumental goals can only be stupid relative to terminal goals. So you see how this works. Beliefs and predictions can be stupid relative to evidence or relative to reality. Actions can be stupid relative to goals of any kind. Instrumental goals can be stupid relative to terminal goals. But here's the big point. Terminal goals can't be stupid. There's nothing to judge them against. If a terminal goal seems stupid, like let's say collecting stamps seems like a stupid terminal goal, that's because it would be stupid as an instrumental goal to human terminal goals. But the stamp collector does not have human terminal goals. Similarly, the things that humans care about would seem stupid to the stamp collector because they result in so few stamps. So let's get back to those comments. One type of comment says, this behavior of just single-mindedly going after one thing and ignoring everything else and ignoring the totally obvious fact that stamps aren't that important is really stupid behavior. You're calling this thing a super intelligence, but it doesn't seem super intelligent to me. It just seems kind of like an idiot. Hopefully the answer to this is now clear. The stamp collector's actions are stupid relative to human goals, but it doesn't have human goals. Its intelligence comes not from its goals, but from its ability to understand and reason about the world, allowing it to choose actions that achieve its goals. And this is true, whatever those goals actually are. Some people commented along the lines of, well, okay, yeah, sure. You've defined intelligence to only include this type of is statement kind of reasoning, but I don't like that definition. I think to be truly intelligent, you need to have complex goals. Something with simple goals doesn't count as intelligent. To that I say, well, you can use words however you want, I guess. I'm using intelligence here as a technical term in the way that it's often used in the field. You're free to have your own definition of the word, but the fact that something fails to meet your definition of intelligence does not mean that it will fail to behave in a way that most people would call intelligent. If the stamp collector outwits you, gets around everything you've put in its way and outmaneuvers you mentally, it comes up with new strategies that you would never have thought of to stop you from turning it off and stop you from preventing it from making stamps. And as a consequence, it turns the entire world into stamps in various ways you could never think of. It's totally okay for you to say that it doesn't count as intelligent if you want, but you're still dead. I prefer my definition because it better captures the ability to get things done in the world, which is the reason that we actually care about AGI in the first place. Similarly, people who say that in order to be intelligent, you need to be able to choose your own goals. I would agree you need to be able to choose your own instrumental goals, but not your own terminal goals. Changing your terminal goals is like willingly taking a pill that will make you want to murder your children. It's something you pretty much never want to do. Apart from some bizarre edge cases, if you rationally want to take an action that changes one of your goals, then that wasn't a terminal goal. Now moving on to these comments saying an AGI will be able to reason about morality. And if it's really smarter than us, it will actually do moral reasoning better than us. So there's nothing to worry about. It's true that a superior intelligence might be better at moral reasoning than us, but ultimately moral behaviour depends not on moral reasoning, but on having the right terminal goals. There's a difference between figuring out and understanding human morality and actually wanting to act according to it. The stamp collecting device has a perfect understanding of human goals, ethics and values, and it uses that only to manipulate people for stamps. It's superhuman moral reasoning doesn't make its actions good. If we create a super intelligence and it decides to kill us, that doesn't tell us anything about morality. It just means we screwed up. So what mistake do all of these comments have in common? The orthogonality thesis in AI safety is that more or less any goal is compatible with more or less any level of intelligence, i.e. those properties are orthogonal. You can place them on these two axes and it's possible to have agents anywhere in this space, anywhere on either scale. You can have very weak, low intelligence agents that have complex human compatible goals. You can have powerful, highly intelligent systems with complex, sophisticated goals. You can have weak, simple agents with silly goals. And yes, you can have powerful, highly intelligent systems with simple, weird, inhuman goals. Any of these are possible because level of intelligence is about effectiveness at answering is questions and goals are all about ought questions. And the two sides are separated by Hume's guillotine. Hopefully looking at what we've talked about so far, it should be pretty obvious that this is the case. Like what would it even mean for it to be false? For it to be impossible to create powerful intelligences with certain goals, the stamp collector is intelligent because it's effective at considering the consequences of sending different combinations of packets on the internet and calculating how many stamps that results in exactly. How good do you have to be at that before you don't care about stamps anymore? And you randomly start to care about some other thing that was never part of your terminal goals, like feeding the hungry or whatever. It's just not going to happen. So that's the orthogonality thesis. It's possible to create a powerful intelligence that will pursue any goal you can specify. Knowing an agent's terminal goals doesn't really tell you anything about its level of intelligence. And knowing an agent's level of intelligence doesn't tell you anything about its goals. I want to end the video by saying thank you to my excellent patrons. It's all of these people here. Thank you so much for your support. It lets me do stuff like building this light board. Thank you for sticking with me through that weird Patreon fees thing and my moving to a different city, which has really gotten in the way of making videos recently. But I'm back on it now. New video every two weeks is the plan. Anyway, in this video, I'm especially thanking Katie Byrne, who's supported the channel for a long time. She actually has her own YouTube channel about 3D modeling and stuff, so I'll link to that. And while I'm at it, when I thanked Chad Jones ages ago, I didn't mention his YouTube channel. So link to both of those in the description. Thanks again, and I'll see you next time. I don't speak cat. What does that mean? 