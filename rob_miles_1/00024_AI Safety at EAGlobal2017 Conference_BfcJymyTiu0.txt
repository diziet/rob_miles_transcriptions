This weekend I went to Imperial College London to attend the Effective Altruism Global Conference. The conference isn't actually about AI, it's about charity. The idea is, like, if you want to save human lives and you've got Â£100 to spend on that, you have to make a decision about which charity to give that money to. And they'll all say that they're good, but which charity is going to save the most lives per pound on average? It's a difficult question to answer. But it turns out that there are popular charities trying to solve the same problem, where one charity is 100 or 1,000 times more effective than the other. It's kind of insane, but it can happen because, apart from these guys, nobody's really paying attention. You know, people don't really do the work to figure out which charities are actually effective at what they're trying to do. So that's pretty interesting, but it's not why I attended. See, there's an argument that, if people like me are right about artificial intelligence, then giving money to help fund AI safety research might actually be an effective way to use charitable donations to help the world. Not everybody agrees, of course, but they take the issue seriously enough that they invited a bunch of experts to speak at the conference to help people understand the issue better. So this charity conference turns out to be a great place to hear the perspectives of a lot of AI safety experts. Victoria Krikovna from DeepMind's safety team and Awain Evans from the Future of Humanity Institute gave a talk together about careers in technical AI safety research, which is basically what this channel is about. I'm not going to include much from these talks because they were professionally recorded, and they'll go live on YouTube at some point. I'll put a link in the description as and when that happens. But yeah, Vika talked about what the problems are, what the field involves, and what it's like to work in AI safety. And Awain talked about the places you can go, the things you should do, what things you'll need to study, what qualifications you might need or not need, as the case may be. They answered questions afterwards. The sound I recorded for this really sucks. But yeah, the general consensus was there are lots of interesting problems and hardly anyone's working on them, and we need at least 10 times as many AI safety researchers as we've got. DeepMind is hiring. The Future of Humanity Institute is hiring. Actually, there'll be a link in the description to a specific job posting that they have right now. And Awain is working on a new thing called Ort.org, which isn't up yet but will be hiring soon. Lots of opportunities here. Oh, some people were there arguing that if animals can experience suffering in a way that's morally relevant, then maybe factory farming is actually the biggest cause of preventable suffering and death on Earth, and fixing that would be an effective way to use our charity money. So I tried out their virtual reality thing that lets you experience the inside of a slaughterhouse from the perspective of a cow. Worst VR experience of my life. 7.8 out of 10. Helen Toner, an analyst at the Open Philanthropy Project, talked about their work on artificial intelligence, analyzing how likely different scenarios are and thinking about strategy and policy, how we can tackle this problem as a civilization, and how they're helping to fund the technical research that we'll need. In the questions, she had some advice about talking to people about this subject. It's important to be able to disambiguate the types of concerns that AI researchers actually have from this concern that the robots are going to wake up and come and kill everyone. And about doing the work yourself. If you're interested in doing this machine learning side of AI safety, it's probably more important to get really, really good at machine learning and become a really, really talented machine learning researcher. That's probably more important than trying to work on safety as soon as you can. Here's Alan Defoe, also from the Open Philanthropy Project, who went into some detail about their analysis of the landscape for AI in the coming years. I really recommend this talk to help people understand the difference between when people are trying to tell interesting stories about what might happen in the future and when people are seriously and diligently trying to figure out what might happen in the future because they want to be ready for it. Some really interesting things in that talk, and I'd strongly recommend checking that out when it goes up online. Probably my favorite talk was from Shahar Avin, from the Center for the Study of Existential Risk at the University of Cambridge. He was there talking about a report that they're going to release very soon about preventing and mitigating the misuse of artificial intelligence. Really interesting stuff. Dr. Avin is very wise and correct about everything. Here are concrete problems in AI safety, and that's kind of a very good paper, but a bit longish, so if you want to consume it in a more video-engaging way, Rob Miles has a series of videos about this on YouTube that I strongly recommend. That's all for now. The next video will be the next section of Concrete Problems in AI Safety, Scalable Supervision. So subscribe and click the bell if you want to be notified when that comes out, and I'll see you next time. Cashews. There's cashews everywhere. This is a great conference. I want to thank my wonderful patrons who make this channel possible by supporting me on Patreon. All of these excellent people. In this video, I'm especially thanking Kyle Scott, who's done more for this channel than just about anyone else. You guys should see some big improvements to the channel over the coming months, and a lot of that is down to Kyle. So, thank you so, so much. Okay, well there's cashews here. This is a great conference. 