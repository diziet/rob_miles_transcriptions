Dear fellow scholars, this is Papers in Two Minutes with Robert Samuel Koenigsberg-Miles. Okay, I'm making this video for a few reasons. Firstly, I've had a lot of comments from people saying they'd like me to do videos like Karoli Jonae-Fahir over at Two Minute Papers, so this is that. If you're confused, link in the description, check out that channel, it's amazing. Secondly, this video is a follow-up to a video of me that recently went out on Computerphile about generative adversarial networks. Definitely check that out if you haven't yet, again link in the description. This video will make a lot more sense if you've seen that one. So on the Computerphile video, there were a fairly large number of comments about there not being enough pictures in that video, not enough sort of demonstrations or visualizations of the actual images being produced by these networks. And that's largely my fault, I told Sean I would send him links of the papers I was talking about, and I forgot to do that, but we can talk about them here. So at the end of that video I was talking about doing arithmetic on the vectors in the latent space. If you take your men wearing sunglasses vector, subtract the man vector, and add the woman vector, you get a point in your space. And if you run that through the generator, you get a woman wearing sunglasses. And people were asking if that was a real thing or hypothetical, and if they could see pictures of it and so on. So that came from this paper, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks by Radford and Metz. And I was talking specifically about figure 7. There's a link to this paper in the description as well. So you can see here you have a bunch of images of men wearing sunglasses, and then the average of all of those latent vectors is this image of a man with glasses. Then we do the same thing for a man without glasses and a woman without glasses. And then we can do arithmetic on those input vectors and find that man with glasses minus man without glasses plus woman without glasses gives us images of a woman with glasses. They've also got another one here in this same figure that does the same thing with smiling. So you take a smiling woman vector, subtract the vector for a woman with a neutral expression, and then add the vector for a man with a neutral expression, and you get a smiling man, which is pretty cool. So we can see that movements in the latent space have meaning in human understandable aspects of the image. I also mentioned that if you sort of take that point and smoothly move it around the latent space, you get a smoothly varying picture of a cat. Now when I said that, I'd never actually seen anyone do it. I just figured from mathematics that it was possible. But just after that video went live, this paper was made available, which included as part of their demo video exactly that, smoothly moving around the latent space to produce smoothly varying cat pictures. And the results are terrifying, actually. I like how the network decided that black bordered white text in impact font is an important component of a cat image. Wonder how that happened. But the core point of this paper relates to something else I said in the computer file video. They're fairly low resolution right now. Pro tip, whenever you mention some limitation of AI, always add right now or yet because there's probably someone out there at that very moment working on something that'll prove you wrong. Anyway, this new paper uses a fascinating technique of growing the neural network as it's being trained. So new layers of neurons are added as the training progresses to allow very large networks without having to train such a large number of neurons from the very beginning. This allows the system to generate unprecedentedly high resolution images. I mean, look at these results. It's just, just beautiful. It's nice to be able to take a break from being deeply concerned about the impact of AI on the future of humanity and just be deeply concerned about the output of this network. What is that? What is that? Anyway, I'm sure you're now wondering, assuming I can get this video out before everyone's already seen this, what it looks like to smoothly move around the latent space for this celebrity faces network. It looks like this. I'm just going to let this run. I think it's completely mesmerizing. There's a link in the description to the video that I got this from, which has a lot more examples of the things that they can do with this technique, and it's really, really excellent. There's also a link to the paper. You can read that as well. I want to thank my generous patrons, these people. And in this video, I'm especially thanking Alexander Hartwig-Nielsen, who's supported the channel for a really long time. Thank you so much. I want to apologize to Two Minute Papers and say, thank you for watching and for your generous support, and I'll see you next time. 