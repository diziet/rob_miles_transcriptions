Hi. A long time ago on Computerphile, we introduced the Stamp Collector, this hypothetical superintelligence which just has crazy extreme powers by magic, effectively. It's a thought experiment. Then we brought things back down to earth and started looking at this paper, Concrete Problems in AI Safety, which looks at how we can improve the safety of current AI systems, hopefully in a way that will be applicable to future general AI systems and superintelligences. Let's talk a little bit about what I expect those AGI systems to be like. This is purely speculation, of course, but here are some of my thoughts about what kind of capabilities we might expect from a human-level AGI and some of my intuitions about why it might not stay human-level for long. Suppose we figure out how to implement general intelligence. We have a system with that magic ability that humans have to tackle a wide variety of different cognitive tasks. And let's assume that it's not just anthropomorphic, it's not a direct one-to-one emulation of a human brain, but it's a general intelligence. It can, in principle, learn to do any cognitive task that a human can. And let's assume that it's more or less human-level. It can do any cognitive task a human can, about as well as the best humans. But note that this assumption still makes our AGI pretty stupid in a lot of domains, because humans are stupid in a lot of domains. If the AGI is only about as good as the best humans at something like mental arithmetic, it can be outperformed by a human with a calculator, right? If an AGI that's about as good as the best humans at chess plays the best human chess player, it should be pretty close. But in practice, such an AGI would just download and run one of the existing freely available chess programs that already far outperforms humans. And if it needs to do mental arithmetic, it should be able to just use its own existing computing hardware. So let's revise our definitions and say that our AGI can perform any cognitive task as well as the best humans, or if our existing narrow AI outperforms humans, then the AGI's performance will be closer to what our best narrow AI systems can do. So that's one way that a human-level AGI almost immediately becomes sort of superhuman. It can outperform humans at things that computers already outperform humans at. Does that really count, though? Like, is a human with a calculator really an arithmetic superintelligence? Kind of. It depends where you draw your boundaries. The human calculator system is much better at arithmetic than any human, so it kind of counts. And this brings us to some of the other advantages a human-level AGI might get pretty much free of charge. Because if you imagine the human has a calculator implant in their brain that lets them just think of equations and the solutions immediately pop into their mind, that feels a lot more superintelligent. And that's much closer to what the AGI would be working with. Because humans are at a big disadvantage when it comes to interacting with other systems. They are made out of meat. For humans, interacting with computers is slow. You have a number in your brain, it's some pattern of electrical signals, but to get that into the machine, your brain has to send signals to some nerves that pass it on to more nerves across synapses that work by chemical diffusion that then activate some muscle fibers that start to accelerate some meat. And you have to send loads of these signals to lots of different muscles to get the bit of meat to land on the right button. And then that completes the circuit, and the process can go along at a reasonable speed again. But you've got to do that for every digit. Getting data out of a human brain and into another system is glacially slow by software standards. And getting data in is pretty slow as well. You can't just experience a number, it has to be presented as a pattern of light, which excites some light-sensitive cells in your eye that sends a signal to your optic nerve, which goes to your brain, which then does a load of processing to figure out that you're looking at a three or whatever. You know, it's really low bandwidth, high latency. The round-trip time, measured as the time it takes a human to press a button as soon as they see a light flash, is about 200 milliseconds, a fifth of a second. That is a long time in software terms, as any systems programmer will tell you. Gamers will know this too. Think about playing a game with 200 milliseconds of ping. That's what your brain's working with all the time. So getting rid of that lag and increasing the bandwidth should make an AGI much faster at a lot of tasks, and much better at integrating other software systems into its thinking, even if that AGI is not actually smarter than a human in any other way. The other problem with our input system is that we can only really take in information in certain forms. Text, images, sound. We can't easily parse most types of information. There's way too much information to decode the matrix. Which is why we make graphs and charts and data visualizations. You have to convert the data into something that can be parsed by systems designed for hunting and gathering. Our AGI might not be limited in that way. It might be able to experience different kinds of data much more directly without having to first map it into an existing sensory modality. It would probably have specialized systems for processing images or sound, but it wouldn't have to convert every input into images or sound and then back again before it could use it. It could just use the data. Imagine a software developer who's about as smart as the best human software developers, but they can perceive data directly without having to convert it into symbols and visually read it. And they don't have to type or manually navigate the files. They can write code as fast as they can think. Even without being smarter than a human, the improvement to I.O. could make such an AGI much more effective than a human. If it couldn't download a chess program, it might be able to write its own pretty quickly. Another advantage I want to talk about is the sort of cognitive flexibility you can get when you're made of software. It could allow an AGI much more control over its mind's operation. For example, as a human, you have one auditory system, which pretty much means you can process one stream of audio at once. Like, you can't really listen to two people saying different things in each ear, even if in principle you have enough processing power to do it. You can shut your eyes to avoid being distracted, and that might make it easier. But you can't then say, well, I'm not using my visual cortex right now, so I'll have those neurons process one of the audio streams. Brains don't work that way. But an AGI might be able to do just that, diverting computational resources from one task to another as needed, increasing its effectiveness. It might even be capable of superhuman multitasking by splitting off simpler versions of itself to handle large numbers of simple tasks. And this is all assuming that the core general intelligence part of the AI system operates at the same speed as a human brain, which might not be true. I mean, name a cognitive task that computers can do as well as humans, but the computers do it slower. Not easy, right? The general trend is we go from computers can't do this at all to computers can do this much faster than people. Not always, but in general. So I wouldn't be surprised if that pattern continues with AGI. And being faster is valuable in general intelligence. Imagine your brain stayed exactly the same, but the world slowed down to 1 10th or 100th of speed. Once you got used to controlling your body at that speed, you'd effectively be much more intelligent since you could afford to put lots of careful thought and planning into everything you do. When someone says something to you in a conversation, you'd have time to think for a minute or two about what the other person said, consider the different meanings they might've intended, you know, decide on what kind of thing you want to say, write a draft, go through and check it, make a couple of different versions, you know, think about like how each one might be interpreted by each of the people present, where it might take the conversation next and so on. You'd seem way smarter and you'd effectively be much smarter, but your brain hasn't changed at all. The world just slowed down. So in a way, speed is a form of superintelligence. And I'd expect AGIs to get faster over time as well. The obvious way is if you make a human speed AGI, a year later, there'll be new, faster computers to run it on and it will get faster, but you can also speed it up by giving it more hardware. Note that it's not always the case that a system can be sped up by running it on multiple computers. You can't get a baby in less than nine months by hiring two pregnant women. You know, some algorithms inherently require a large number of sequential steps. Each step needs to use the output of the previous step. So there's no way to break the problem down into parts that can be given to different machines to work on in parallel. Maybe AGI will turn out to be one of those, but I don't think so. I'd expect it to be parallelizable for two reasons. Firstly, just about all of the current state-of-the-art AI systems are. They're built on frameworks like TensorFlow that are designed from the ground up to efficiently run these algorithms on large numbers of processors. If we find AGI along the path we're currently going down, it'll almost certainly be a parallelizable algorithm. Secondly, we know that general intelligence does not inherently require a large number of sequential steps because human brains have general intelligence and brains can't do large numbers of sequential steps, at least not quickly. The fastest a neuron can fire is around 200 times a second. So that has to be the most sequential steps we can consistently execute in a second. Anything your brain can do every second must be doable in less than 200 sequential steps, because otherwise neurons simply aren't fast enough to do it. So whenever the brain is doing something computationally impressive quickly, it has to be because it's using extremely large numbers of neurons working in parallel. We know it must be possible to parallelize a general intelligence algorithm because it's already been done. So give an AGI more hardware and it should run faster. And you just know that as soon as a system starts to exhibit behavior that looks like general intelligence, people are going to want to run that on as many processors as they can get their hands on. So everything I've talked about in this video assumes that we can't actually make an algorithm that's truly smarter than a human, but just something that has faster I.O. and can be run faster. And that's already capable of some pretty impressive things. One question I often get is, what can an AGI really do if it's just a computer with no body? Hopefully the capabilities I've talked about here give some impression of the kind of things it might be able to do. But I intend to elaborate on that in a later video. Thank you so much to my excellent Patreon supporters. That's these people. In this video I'm especially thanking Fabrizio Pisani. He's the one on the left, I think. Anyway, I've recently been thinking about ways to improve my Patreon. Currently patrons get early access to the videos, occasional like behind-the-scenes stuff, sometimes I send out the diagrams and stuff I've drawn in the videos, and of course you can send me messages on Patreon and I reply to those before I do like YouTube comments or Facebook comments. But if you have any other ideas of cool stuff I could do, send me a message if you're a patron. Or even if you're not but you're considering it, let me know your thoughts in the YouTube comments. Thanks for watching and I'll see you next time. The fastest a neuron can fire is- ah! I gotta buy some real sound damping. 