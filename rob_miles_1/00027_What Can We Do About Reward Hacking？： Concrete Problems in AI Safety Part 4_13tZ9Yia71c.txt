Hi, welcome back to this video series on the paper Concrete Problems in AI Safety. In the previous video, I talked about reward hacking, some of the ways that can happen and some related ideas, like adversarial examples, where it's possible to find unexpected inputs to the reward system that falsely result in very high reward. Partially observed goals, where the fact that the reward system has to work with imperfect information about the environment incentivizes an agent to deceive itself by compromising its own sensors to maximize its reward. Wireheading, where the fact that the reward system is a physical object in the environment means that the agent can get very high reward by directly, physically modifying the reward system itself. And Goodhart's Law, the observation that when a measure becomes a target, it ceases to be a good measure. There's a link to that video in the doobly-doo, and I'm going to start this video with a bit about responses and reactions to that one. There were a lot of comments about my school exams example of Goodhart's Law. Some people sent me this story that was in the news when that video came out. A school kicked out some students because they weren't getting high enough marks. It's a classic example. In order to increase the school's average marks, i.e. the numbers that are meant to measure how well the school teaches students, the school refused to teach some students. This is extra funny because that's actually my school, that's the school I went to. Anyway, some people pointed out that the school exams example of Goodhart's Law has been separately documented as Campbell's Law. People pointed out other related ideas, like the Cobra effect, perverse incentives, the law of unintended consequences. There are a lot of people holding different parts of this elephant, as it were. All of these concepts are sort of related. And that's true of the examples I used as well. I used a hypothetical Super Mario bot that exploited glitches in the game to give itself maximum score as an example of reward hacking. But you could call it wireheading, since the score-calculating part of the reward system sort of exists in the game environment, and it's sort of being replaced. The cleaning robot with a bucket on its head was an example of partially observed goals, but you could call it an instance of Goodhart's Law, since amount of mess observed stops being a good measure of amount of mess there is once it's used as a target. So all of these examples might seem quite different on the surface, but once you look at them through the framework of reward hacking, you can see that there's a lot of similarities and overlap. The paper proposes 10 different approaches for preventing reward hacking. Some will work only for certain specific types. For example, very soon after the problem of adversarial examples in neural networks was discovered, people started working on better ways to train their nets to make them resistant to this kind of thing. That's only really useful for adversarial examples. But given the similarities between different reward hacking issues, maybe there are some things we could do that would prevent lots of these possible problems at once. Careful engineering is one. So AI can't hack its reward function by exploiting bugs in your code, if there are no bugs in your code. There's a lot of work out there on ways to build extremely reliable software. Like there are ways you can construct your programs so that you're able to formally verify that their behavior will have certain properties. You can prove your software's behavior with absolute logical certainty, but only given certain assumptions about, for example, the hardware that the software will run on. Those assumptions might not be totally true, especially if there's a powerful AGI doing its best to make them not true. Of course, careful engineering isn't just about formal verification. There are lots of different software testing and quality assurance systems and approaches out there. And I expect there's a lot AI safety can learn from people working in aerospace, computer security, anywhere that's very focused on writing extremely reliable software. It's something we can work on for AI in general, but I wouldn't rely on this as the main line of defense against reward hacking. Another approach is adversarial reward functions. So part of the problem is that the agent and the reward system are in this kind of adversarial relationship. It's like they're competing. The agent is trying to trick the reward system into giving it as much reward as possible. When you have a powerful, intelligent agent up against a reward system that's a simple, passive piece of software or hardware, you can expect the agent to reliably find ways to trick, subvert, or destroy the reward system. So maybe if the reward system were more powerful, more of an agent in its own right, it would be harder to trick and more able to defend itself. If we can make the reward agent in some sense smarter or more powerful than the original agent, it could be able to keep it from reward hacking. Though then you have the problem of ensuring that the reward agent is safe as well. The paper also mentions the possibility of having more than two agents so that they can all watch each other and keep each other in check. There's kind of an analogy here to the way that the legislative, the executive, and the judiciary branches of government keep one another in check, ensuring that the government as a whole always serves the interests of the citizens. But seriously, I'm not that hopeful about this approach. Firstly, it's not clear how it handles self-improvement. You can't have any of the agents being significantly more powerful than the others, and that gets much harder to make sure of if the system is modifying and improving itself. And in general, I don't feel that comfortable with this kind of internal conflict between powerful systems. It kind of feels like you have a problem with your toaster incinerating the toast with a flamethrower, so you add another system that blasts the bread with a powerful jet of liquid nitrogen as well, so that the two opposing systems can keep each other in check. Instead of systems that want to hack their reward functions but figure they can't get away with it, I'd rather a system that didn't want to mess with its reward function in the first place. This next approach has a chance of providing that, an approach the paper calls model look-ahead. You might remember this from a while back on Computerphile. You have kids, right? Suppose I were to offer you a pill or something you could take, and this pill will completely rewire your brain, so that you would just absolutely love to kill your kids, right? Whereas right now, what you want is very complicated and quite difficult to achieve, and it's hard work for you, and you're probably never going to be done. You're never going to be truly happy, right? In life, nobody is. You can't achieve everything you want. Whereas in this case, it just changes what you want. What you want is to kill your kids, and if you do that, you will be just perfectly happy and satisfied with life, right? Okay. You want to take this pill? No, I don't. You'll be so happy, though. Yeah. But you don't want to do it. And so not only will you not take that pill, you will probably fight pretty hard to avoid having that pill administered to you, because it doesn't matter how that future version of you would feel. You know that right now, you love your kids, and you're not going to take any action right now, which leads to them coming to harm. So it's the same thing. If you have an AI that, for example, values stamps, values collecting stamps, and you go, oh, wait, hang on a second, I didn't quite do that right, let me just go in and change this so that you don't like stamps quite so much, it's going to say, but the only important thing is stamps. If you change me, I'm not going to collect as many stamps, which is something I don't want. There's a real tendency for AGI to try and prevent you from modifying it once it's running. In almost any situation, being given a new utility function is going to rate very low on your current utility function. So there's an interesting contrast here. The reinforcement learning agents we're talking about in this video might fight you in order to change their reward function. But the utility maximizers we were talking about in that video might fight you in order to keep their utility function the same. The utility maximizer reasons that changing its utility function will result in low utility outcomes according to its current utility function. So it doesn't allow it to change. But the reinforcement learner's utility function is effectively just to maximize the output of the reward system. So it has no problem with modifying that to get high reward. Model Lookahead tries to give a reinforcement learning agent some of that forward thinking ability. By having the reward system give rewards not just for the agent's actual actions and its observations of actual world states, but for the agent's planned actions and anticipated future world states. So the agent receives negative reward for planning to modify its reward system. When the robot considers the possibility of putting a bucket on its head, it predicts that this would result in the messes staying there and not being cleaned up. And it receives negative reward, teaching it not to implement that kind of plan. There are several other approaches in the paper, but that's all for now. I want to end with a quick thanks to all my wonderful patrons. These people here. And in this video, I'm especially thanking the Guru of Vision. I don't know who you are or why you call yourself that, but you've supported the channel since May, and I really appreciate it. Anyway, it's thanks to my supporters that I'm starting up a second channel for things that aren't related to AI safety. I'm still going to produce just as much AI safety content on this channel, and I'll use the second channel for quicker, fun stuff that doesn't need as much research. I want to get into the habit of making lots of videos quickly, which should improve my ability to make quality AI safety content quickly as well. If you want an idea of what kind of stuff I'm going to put on the second channel, check out a video I made at the beginning of this channel titled, Where Do We Go Now? There's a link to that video in the description, along with a link to the new second channel. So head on over and subscribe if you're interested. And of course, my patrons will get access to those videos before the rest of the world, just like they do with this channel. Thanks again, and I'll see you next time. 