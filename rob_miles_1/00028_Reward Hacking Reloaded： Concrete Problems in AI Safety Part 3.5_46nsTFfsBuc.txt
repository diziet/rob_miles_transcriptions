Hi, in the previous video we introduced the idea of reward hacking. An AI system that works by maximizing its reward, like a reinforcement learning agent, will go for whatever strategy it expects will result in the highest reward. And there's a tendency for the strategies that have the very highest rewards to be quite different from the kinds of strategies the AI designers were planning for. For example, if you're using the score as the reward in Super Mario World, the highest reward strategy might involve exploiting a load of glitches to directly update the score value, rather than properly playing the game. We talked about some ways that this can happen, exploiting bugs in the software like in Mario, or adversarial examples in neural networks. If you're confused right now, check the video description for the earlier videos in this series. But in this video, we're going to look at some more ways that reward hacking can happen, and how they relate to one another. So let's start by drawing a diagram. I can't believe I already got this far into the subject without drawing this diagram. Anyway, here's your agent. Here's the environment. The agent can take actions to affect the environment, and it can observe the environment to get information about what state the environment's in. There's also a reward system, which uses information from the environment to determine what reward to give the agent. So if the agent is Pac-Man, the environment is the maze, and the reward system is just looking at the score. The agent takes an action, the action affects the environment, the change in the environment creates new observations, and also provides new information to the reward system, which decides what reward to give the agent. And the agent uses the observation and the reward to decide which action to take next. And this kind of goes in a cycle. Reward hacking is a class of problems that can happen around that reward system. Like in the previous video, we were talking about adversarial examples, and how they can be an issue when your reward system relies on a neural network. But that's not the only way this kind of problem can happen. The economist Charles Goodhart once said, any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes. But despite being true, that was not very catchy. So it was changed to, when a measure becomes a target, it ceases to be a good measure. Much better, isn't it? That's Goodhart's law, and it shows up everywhere. Like, if you want to find out how much students know about a subject, you can ask them questions about it, right? You design a test. And if it's well designed, it can be a good measure of the student's knowledge. But if you then use that measure as a target, by using it to decide which students get to go to which universities, or which teachers are considered successful, then things will change. Students will study exam technique. Teachers will teach only what's on the test. So student A, who has a good broad knowledge of the subject, might not do as well as student B, who studied just exactly what's on the test and nothing else. So the test isn't such a good way to measure the student's real knowledge anymore. The thing is, student B only decided to do that because the test is being used to decide university places. You made your measure into a target, and now it's not a good measure anymore. The problem is, the measure is pretty much never a perfect representation of what you care about. And any differences can cause problems. This happens with people. It happens with AI systems. It even happens with animals. At the Institute for Marine Mammal Studies, the trainers wanted to keep the pools clear of dropped litter, so they trained the dolphins to do it. Every time a dolphin came to a trainer with a piece of litter, they would get a fish in return. So of course, the dolphins would hide pieces of waste paper and then tear off little bits to trade for fish. Tearing the paper up allowed the dolphins to get several fish for one dropped item. This is kind of Goodhart's law again. If you count the number of pieces of litter removed from the pool, that's a good measure for the thing you care about, the amount of litter remaining in the pool. But when you make the measure a target, the differences between the measure and the thing you're trying to change get amplified. The fact that there are a lot of pieces of litter coming out of the pool no longer means there's no litter in the pool. So that's Goodhart's law, and you can see how that kind of situation could result in reward hacking. Your reward system needs to use some kind of measure, but that turns the measure into a target, so it will probably stop being a good measure. With dolphins, this can be cute. With people, it can cause serious problems. And with advanced AI systems, well, let's just try to keep that from happening. Another way that reward hacking can happen comes from partially observed goals. In our Super Mario world or Pac-Man examples, the goal is fully observed. The reward is the score, and the AI can just read the score out of memory and it knows its reward. But if we have an AI system acting as an agent in the real world, the reward depends on the state of the environment around it, and the AI only has partial knowledge of that through the robot's limited senses. The goal is only partially observed. Suppose we have a cleaning robot with its mop and bucket, and we want it to clean the office that it's in. So we can set it up so that it gets more reward the less mess there is, like we subtract a bit of reward for each bit of mess. And the way it determines the level of mess is to look around the room with its cameras. What does this robot do? Well, the answer is obvious to anyone who's ever run for parliament in Maidenhead. The old Skyrim shoplifters trick. If it covers up its cameras, say by putting its bucket on its head, it won't see any mess, so it won't lose any reward. If I don't see it, it's not illegal! You've probably heard about experiments on rats where scientists implanted electrodes into the rat's brains, allowing them to directly stimulate their reward centers. And if the rats are able to press a button to activate the electrode, they never do anything else. People call that wireheading, and it's relevant here because if we take our Pac-Man reinforcement learning diagram and change it to the cleaning robot, it's not quite right, is it? In Pac-Man, the reward system is just a little bit of code that runs separately from the game program and just reads the score out. But for the cleaning robot, the reward system is a real thing in the real world. It's got cameras, sensors, circuitry. It physically exists as an object in the office. So maybe the diagram should look more like this. Because unlike in Pac-Man, now the reward system is part of the environment, which means it can be affected by the actions of the agent. The agent isn't just limited to messing with the environment to affect the information going into the reward system, like putting a bucket on its head. It can mess with the reward system itself. If it's able to take the thing apart and make it just return maximum reward regardless of what the environment is like, well, that's an extremely high reward strategy. So some AI designs are prone to deliberately tampering with their reward systems to wirehead themselves. But that's not the worst of it. There are a lot of AGI design proposals out there where the reward is determined by humans smiling or being happy or saying certain things, hitting a certain button or whatever. These designs effectively make the human a component in the reward system. But whatever the reward system is, the agent is incentivized to manipulate or modify it to get the highest reward it can. With powerful general AI systems, we don't just have to worry about the AI wireheading itself. I want to end the video with a quick thank you to my excellent Patreon supporters. All of these people. In this video, I especially want to thank Robert van Duursen, who's supported the channel for a long time. You know, just the other day, my phone completely broke and the phone is actually pretty important because I use it to shoot all of the behind the scenes stuff, anything when I'm traveling, that kind of thing. So I had to get a new one and I was able to use Patreon money to do that. So I just want to say thank you so much for your support. Thanks again, and I'll see you next time. 