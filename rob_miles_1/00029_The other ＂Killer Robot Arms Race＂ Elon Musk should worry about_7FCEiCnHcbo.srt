1
00:00:00,000 --> 00:00:02,080
Hi, this is a new thing I'm trying

2
00:00:02,080 --> 00:00:03,720
where I make quick topical videos

3
00:00:03,720 --> 00:00:05,280
about AI safety in the news.

4
00:00:05,280 --> 00:00:07,280
So somebody linked me a news article today

5
00:00:07,280 --> 00:00:10,280
and the headline is Tesla's Elon Musk leads,

6
00:00:10,280 --> 00:00:11,880
oh, Terminator picture.

7
00:00:11,880 --> 00:00:14,120
Everyone playing the AI news coverage drinking game

8
00:00:14,120 --> 00:00:14,960
has to take a shot.

9
00:00:14,960 --> 00:00:15,800
You know the rules.

10
00:00:17,200 --> 00:00:18,620
Picture of the Terminator.

11
00:00:19,520 --> 00:00:20,760
It means you've got to drink a shot

12
00:00:20,760 --> 00:00:22,620
out of a glass shaped like a skull.

13
00:00:28,500 --> 00:00:29,340
Where was I?

14
00:00:29,340 --> 00:00:31,120
Yeah, so the headline.

15
00:00:31,120 --> 00:00:33,520
Tesla's Elon Musk leads tech experts

16
00:00:33,520 --> 00:00:36,540
in demanding end to killer robots arms race.

17
00:00:36,540 --> 00:00:37,440
This is really interesting

18
00:00:37,440 --> 00:00:40,000
because it looks like it's going to be really relevant

19
00:00:40,000 --> 00:00:42,860
to what this channel is about, AI safety research.

20
00:00:42,860 --> 00:00:45,120
But I read it and it's actually nothing to do with that.

21
00:00:45,120 --> 00:00:47,340
The headline is much more literal than I expected.

22
00:00:47,340 --> 00:00:51,120
It's about an actual arms race for actual killer robots.

23
00:00:51,120 --> 00:00:53,200
I.e. it's about using the UN

24
00:00:53,200 --> 00:00:54,640
to form international agreements

25
00:00:54,640 --> 00:00:56,800
about not deploying autonomous weapon systems.

26
00:00:56,800 --> 00:00:59,080
I thought it was going to be about the other arms race

27
00:00:59,860 --> 00:01:01,160
that might cause robots to kill us.

28
00:01:03,040 --> 00:01:05,440
Okay, so if there's one thing that I hope this channel

29
00:01:05,440 --> 00:01:08,000
and my computer file videos have made clear,

30
00:01:08,000 --> 00:01:11,060
it's that AI safety is a difficult problem.

31
00:01:11,060 --> 00:01:13,900
That quite reasonable looking AGI designs

32
00:01:13,900 --> 00:01:15,940
generally end up going horribly wrong

33
00:01:15,940 --> 00:01:18,460
for subtle and hard to predict reasons.

34
00:01:18,460 --> 00:01:21,060
Developing artificial general intelligence

35
00:01:21,060 --> 00:01:23,180
needs to be done very carefully.

36
00:01:23,180 --> 00:01:25,300
Double and triple checking everything,

37
00:01:25,300 --> 00:01:27,220
running it past lots of people,

38
00:01:27,240 --> 00:01:29,280
ironing out all of the possible problems

39
00:01:29,280 --> 00:01:31,400
before the thing is actually switched on.

40
00:01:31,400 --> 00:01:34,820
To do this safely is going to take a lot of smart people,

41
00:01:34,820 --> 00:01:38,680
a lot of patience, diligence, and time.

42
00:01:38,680 --> 00:01:41,800
But whoever makes AGI first has a huge advantage

43
00:01:41,800 --> 00:01:43,880
since it probably creates a new period

44
00:01:43,880 --> 00:01:45,960
of much faster progress.

45
00:01:45,960 --> 00:01:47,040
Everyone wants to be first

46
00:01:47,040 --> 00:01:49,060
to publish new scientific results anyway,

47
00:01:49,060 --> 00:01:52,000
but the chances are that there are really no prizes

48
00:01:52,000 --> 00:01:54,160
for being the second team to develop AGI.

49
00:01:54,160 --> 00:01:55,840
Even if you're just a few months behind,

50
00:01:55,840 --> 00:01:58,700
a lot can change in a few months in a world with AGI.

51
00:01:58,700 --> 00:02:00,500
So there's an arms race going on

52
00:02:00,500 --> 00:02:02,780
between different teams, different companies,

53
00:02:02,780 --> 00:02:06,140
different countries to be the first to develop AGI.

54
00:02:06,140 --> 00:02:08,980
But developing AGI safely takes a lot of care

55
00:02:08,980 --> 00:02:11,100
and patience and time.

56
00:02:11,100 --> 00:02:12,500
You see the problem here.

57
00:02:12,500 --> 00:02:14,100
The team that gets there first

58
00:02:14,100 --> 00:02:16,740
is probably not the team that's spending the most time

59
00:02:16,740 --> 00:02:20,420
on ensuring they've got the very best AI safety practices.

60
00:02:20,420 --> 00:02:21,860
The team that gets there first

61
00:02:21,860 --> 00:02:23,660
is probably going to be rushing,

62
00:02:23,680 --> 00:02:27,240
cutting corners, and ignoring safety concerns.

63
00:02:27,240 --> 00:02:28,600
Hey, remember a while back I said

64
00:02:28,600 --> 00:02:30,000
I was gonna make a video about

65
00:02:30,000 --> 00:02:32,360
why I think Elon Musk's approach to AI safety

66
00:02:32,360 --> 00:02:34,780
might end up doing more harm than good?

67
00:02:34,780 --> 00:02:35,800
I guess this is that.

68
00:02:35,800 --> 00:02:38,080
So there's a school of thought which says that

69
00:02:38,080 --> 00:02:40,640
because AGI is a very powerful technology,

70
00:02:40,640 --> 00:02:43,020
it will grant whoever controls it a lot of power.

71
00:02:43,020 --> 00:02:44,880
So firstly, it's important

72
00:02:44,880 --> 00:02:47,080
that the people in control of it are good people.

73
00:02:47,080 --> 00:02:50,440
And secondly, we want as many people as possible to have it

74
00:02:50,440 --> 00:02:52,080
so that the power is democratized

75
00:02:52,080 --> 00:02:55,140
and not concentrated in the control of a small elite.

76
00:02:55,140 --> 00:02:56,840
The best of the available alternatives

77
00:02:56,840 --> 00:03:01,260
is that we achieve democratization of AI technology,

78
00:03:01,260 --> 00:03:04,100
meaning that no one company

79
00:03:04,100 --> 00:03:06,720
or small set of individuals

80
00:03:06,720 --> 00:03:09,100
has control over advanced AI technology.

81
00:03:09,100 --> 00:03:11,660
And starting from that fairly reasonable school of thought,

82
00:03:11,660 --> 00:03:14,260
this is a very good and valuable thing to do.

83
00:03:14,260 --> 00:03:16,520
But there's another school of thought which says that

84
00:03:16,520 --> 00:03:19,180
because making AGI is nowhere near as difficult

85
00:03:19,180 --> 00:03:21,380
as making safe AGI,

86
00:03:21,400 --> 00:03:24,080
the bigger risk is not that the wrong person

87
00:03:24,080 --> 00:03:26,240
or wrong people might make an AGI

88
00:03:26,240 --> 00:03:28,800
that's aligned with the wrong human interests,

89
00:03:28,800 --> 00:03:30,360
but that someone might make an AGI

90
00:03:30,360 --> 00:03:33,480
that's not really aligned with any human interests at all.

91
00:03:33,480 --> 00:03:35,040
Thanks to this arms race effect

92
00:03:35,040 --> 00:03:36,720
that will make people want to cut corners

93
00:03:36,720 --> 00:03:38,240
on alignment and safety,

94
00:03:38,240 --> 00:03:40,080
that possibility looks much more likely.

95
00:03:40,080 --> 00:03:43,320
And the thing is, the more competitors there are in the race,

96
00:03:43,320 --> 00:03:45,480
the more of a problem this is.

97
00:03:45,480 --> 00:03:47,560
If there are three companies working on AGI,

98
00:03:47,560 --> 00:03:48,840
maybe they can all get together

99
00:03:48,840 --> 00:03:51,120
and agree to a strict set of safety protocols

100
00:03:51,140 --> 00:03:52,860
that they're all going to stick to.

101
00:03:52,860 --> 00:03:55,220
It's in everyone's interest to be safe

102
00:03:55,220 --> 00:03:58,140
as long as they know their competitors will be safe as well.

103
00:03:58,140 --> 00:04:00,380
But if there are a hundred or a thousand groups

104
00:04:00,380 --> 00:04:02,060
with a shot at making AGI,

105
00:04:02,060 --> 00:04:04,220
there's really no way you're gonna be able to trust

106
00:04:04,220 --> 00:04:07,480
every single one of them to stick to an agreement like that

107
00:04:07,480 --> 00:04:09,820
when breaking it would give them an advantage.

108
00:04:09,820 --> 00:04:12,420
So it might be impossible to make the agreement at all.

109
00:04:12,420 --> 00:04:14,460
And whoever spends the least time on safety

110
00:04:14,460 --> 00:04:16,180
has the biggest advantage.

111
00:04:16,180 --> 00:04:18,140
From the perspective of this school of thought,

112
00:04:18,140 --> 00:04:20,580
making AGI developments open and available

113
00:04:20,600 --> 00:04:22,200
to as many people as possible

114
00:04:22,200 --> 00:04:24,120
might be the last thing we want to do.

115
00:04:24,120 --> 00:04:26,280
Maybe once AGI starts to get closer,

116
00:04:26,280 --> 00:04:28,300
we'll find a way to keep AI research

117
00:04:28,300 --> 00:04:32,320
limited to a small number of safe, careful organizations

118
00:04:32,320 --> 00:04:36,280
while making AI safety research open and widely available.

119
00:04:36,280 --> 00:04:38,680
I don't know, but this might be a situation

120
00:04:38,680 --> 00:04:41,680
where total openness and democratization

121
00:04:41,680 --> 00:04:43,540
is actually a bad idea.

122
00:04:43,540 --> 00:04:44,960
Elon Musk himself has said

123
00:04:44,960 --> 00:04:47,840
that AI is potentially more dangerous than nukes.

124
00:04:47,840 --> 00:04:49,400
And I wanna make it clear

125
00:04:49,400 --> 00:04:50,900
that I have enormous respect for him.

126
00:04:50,900 --> 00:04:52,060
But I just want to point out

127
00:04:52,060 --> 00:04:55,300
that with a not that huge change in assumptions,

128
00:04:55,300 --> 00:04:58,700
the open approach starts to look like saying,

129
00:04:58,700 --> 00:05:00,500
nukes are extremely dangerous,

130
00:05:00,500 --> 00:05:03,020
so we need to empower as many people

131
00:05:03,020 --> 00:05:04,300
as possible to have them.

132
00:05:05,580 --> 00:05:06,580
And to end the video,

133
00:05:06,580 --> 00:05:11,580
a big thank you to all of my excellent Patreon supporters.

134
00:05:11,600 --> 00:05:12,920
These people.

135
00:05:12,920 --> 00:05:15,920
In this video, I especially want to thank Michael Grieve.

136
00:05:15,920 --> 00:05:19,120
I recently uploaded a video that had an audio problem in it.

137
00:05:19,120 --> 00:05:20,760
The sound was only coming out of one ear.

138
00:05:20,760 --> 00:05:22,800
But because my Patreon supporters get access

139
00:05:22,800 --> 00:05:26,120
to every video I make before the rest of the world does,

140
00:05:26,120 --> 00:05:28,480
one of my supporters, Jimmy Gowson, spotted it

141
00:05:28,480 --> 00:05:30,280
and posted it on his Instagram story.

142
00:05:30,280 --> 00:05:32,000
And I'm so grateful to him for that.

143
00:05:32,000 --> 00:05:34,560
And I'm also grateful to all of my Patreon supporters

144
00:05:34,580 --> 00:05:36,500
because one of my supporters, Jimmy Gowson, spotted it

145
00:05:36,500 --> 00:05:39,140
and pointed it out, and I was able to fix that.

146
00:05:39,140 --> 00:05:40,820
And then I was able to use Patreon money

147
00:05:40,820 --> 00:05:42,500
to get a new pair of earphones.

148
00:05:42,500 --> 00:05:45,300
So I want to say again how grateful I am to all of you.

149
00:05:45,300 --> 00:05:47,700
You really are a tremendous help to the channel.

