Hi, this is a new thing I'm trying where I make quick topical videos about AI safety in the news. So somebody linked me a news article today and the headline is Tesla's Elon Musk leads, oh, Terminator picture. Everyone playing the AI news coverage drinking game has to take a shot. You know the rules. Picture of the Terminator. It means you've got to drink a shot out of a glass shaped like a skull. Where was I? Yeah, so the headline. Tesla's Elon Musk leads tech experts in demanding end to killer robots arms race. This is really interesting because it looks like it's going to be really relevant to what this channel is about, AI safety research. But I read it and it's actually nothing to do with that. The headline is much more literal than I expected. It's about an actual arms race for actual killer robots. I.e. it's about using the UN to form international agreements about not deploying autonomous weapon systems. I thought it was going to be about the other arms race that might cause robots to kill us. Okay, so if there's one thing that I hope this channel and my computer file videos have made clear, it's that AI safety is a difficult problem. That quite reasonable looking AGI designs generally end up going horribly wrong for subtle and hard to predict reasons. Developing artificial general intelligence needs to be done very carefully. Double and triple checking everything, running it past lots of people, ironing out all of the possible problems before the thing is actually switched on. To do this safely is going to take a lot of smart people, a lot of patience, diligence, and time. But whoever makes AGI first has a huge advantage since it probably creates a new period of much faster progress. Everyone wants to be first to publish new scientific results anyway, but the chances are that there are really no prizes for being the second team to develop AGI. Even if you're just a few months behind, a lot can change in a few months in a world with AGI. So there's an arms race going on between different teams, different companies, different countries to be the first to develop AGI. But developing AGI safely takes a lot of care and patience and time. You see the problem here. The team that gets there first is probably not the team that's spending the most time on ensuring they've got the very best AI safety practices. The team that gets there first is probably going to be rushing, cutting corners, and ignoring safety concerns. Hey, remember a while back I said I was gonna make a video about why I think Elon Musk's approach to AI safety might end up doing more harm than good? I guess this is that. So there's a school of thought which says that because AGI is a very powerful technology, it will grant whoever controls it a lot of power. So firstly, it's important that the people in control of it are good people. And secondly, we want as many people as possible to have it so that the power is democratized and not concentrated in the control of a small elite. The best of the available alternatives is that we achieve democratization of AI technology, meaning that no one company or small set of individuals has control over advanced AI technology. And starting from that fairly reasonable school of thought, this is a very good and valuable thing to do. But there's another school of thought which says that because making AGI is nowhere near as difficult as making safe AGI, the bigger risk is not that the wrong person or wrong people might make an AGI that's aligned with the wrong human interests, but that someone might make an AGI that's not really aligned with any human interests at all. Thanks to this arms race effect that will make people want to cut corners on alignment and safety, that possibility looks much more likely. And the thing is, the more competitors there are in the race, the more of a problem this is. If there are three companies working on AGI, maybe they can all get together and agree to a strict set of safety protocols that they're all going to stick to. It's in everyone's interest to be safe as long as they know their competitors will be safe as well. But if there are a hundred or a thousand groups with a shot at making AGI, there's really no way you're gonna be able to trust every single one of them to stick to an agreement like that when breaking it would give them an advantage. So it might be impossible to make the agreement at all. And whoever spends the least time on safety has the biggest advantage. From the perspective of this school of thought, making AGI developments open and available to as many people as possible might be the last thing we want to do. Maybe once AGI starts to get closer, we'll find a way to keep AI research limited to a small number of safe, careful organizations while making AI safety research open and widely available. I don't know, but this might be a situation where total openness and democratization is actually a bad idea. Elon Musk himself has said that AI is potentially more dangerous than nukes. And I wanna make it clear that I have enormous respect for him. But I just want to point out that with a not that huge change in assumptions, the open approach starts to look like saying, nukes are extremely dangerous, so we need to empower as many people as possible to have them. And to end the video, a big thank you to all of my excellent Patreon supporters. These people. In this video, I especially want to thank Michael Grieve. I recently uploaded a video that had an audio problem in it. The sound was only coming out of one ear. But because my Patreon supporters get access to every video I make before the rest of the world does, one of my supporters, Jimmy Gowson, spotted it and posted it on his Instagram story. And I'm so grateful to him for that. And I'm also grateful to all of my Patreon supporters because one of my supporters, Jimmy Gowson, spotted it and pointed it out, and I was able to fix that. And then I was able to use Patreon money to get a new pair of earphones. So I want to say again how grateful I am to all of you. You really are a tremendous help to the channel. 