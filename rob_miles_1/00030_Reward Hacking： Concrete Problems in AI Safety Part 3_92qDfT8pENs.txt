Hi, this video is part of a series looking at the paper Concrete Problems in AI Safety. I recommend you check out the other videos in the series, link below, but hopefully this video should still make sense even if you haven't seen the others. Also, in case anyone is subscribed to me and not to Computerphile for some reason, I recently did a video on the Computerphile channel that's effectively part of the same series. That video pretty much covers what I would say in a video about the multi-agent approach as part of the paper, so check that video out if you haven't seen it, link in the doobly-doo. Right, today we're talking about reward hacking. So in the Computerphile video I was talking about reinforcement learning and the basics of how that works. You have an agent interacting with an environment and trying to maximize a reward. So like, your agent might be Pac-Man, and then the environment would be the maze that Pac-Man is in, and the reward would be the score of the game. Systems based on this framework have demonstrated an impressive ability to learn how to tackle various problems, and some people are hopeful about this work eventually developing into full general intelligence. But there are some potential problems with using this kind of approach to build powerful AI systems, and one of them is reward hacking. So let's say you've built a very powerful reinforcement learning AI system, and you're testing it in Super Mario World. It can see the screen and take actions by pressing buttons on the controller, and you've told it the address in memory where the score is, and set that as the reward. And what it does, instead of really playing the game, is something like this. This is a video by Seth Bling, who is a legitimate wizard. Check out his channel, link in the doobly-doo. It's doing all of this weird looking glitchy stuff, and spin jumping all over the place. And also, Mario is black now? Anyway, this doesn't look like the kind of game-playing behavior you were expecting, and you just kind of assume that your AI is broken. And then, suddenly, the score part of memory is set to the maximum possible value, and the AI stops entering inputs and just sits there. Hang on AI, what are you doing? Am I wrong? No, you're not wrong. Am I wrong? Because it turns out that there are sequences of inputs you can put into Super Mario World that just totally break the game, and give you the ability to directly edit basically any address in memory. You can use it to turn the game into Flappy Bird if you want, and your AI has just used it to set its reward to maximum. This is reward hacking. So what really went wrong here? Well, you told it to maximize a value in memory, when what you really wanted was for it to play the game. The assumption was that the only way to increase the score value was to play the game well, but that assumption turned out to not really be true. Any way in which your reward function doesn't perfectly line up with what you actually want the system to do can cause problems. And things only get more difficult in the real world, where there's no handy score variable we can use as a reward. Whatever we want the agent to do, we need some way of translating that real world thing into a number. And how do you do that without the AI messing with it? These days, the usual way to convert complex messy real world things that we can't easily specify into machine understandable values is to use a deep neural network of some kind, train it with a lot of examples, and then it can tell if something is a cat or a dog or whatever. So it's tempting to feed whatever real world data we've got into a neural net, and train it on a bunch of human supplied rewards for different situations, and then use the output of that as the reward for the AI. Now, as long as your situation and your AI are very limited, that can work. But, as I'm sure many of you have seen, these neural networks are vulnerable to adversarial examples. It's possible to find unexpected inputs that cause the system to give dramatically the wrong answer. The image on the right is clearly a panda, but it's misidentified as a gibbon. This kind of thing could obviously cause problems for any reinforcement learning agent that's using the output of a neural network as its reward function. And there's kind of an analogy to Super Mario World here as well. If we view the game as a system designed to take a sequence of inputs, the player's button presses, and output a number representing how well that player is playing, i.e. the score, then this glitchy hacking stuff Sethbling did can be thought of as an adversarial example. It's a carefully chosen input that makes the system output the wrong answer. Note that, as far as I know, no AI system has independently discovered how to do this to Super Mario World yet, because figuring out the details of how all the bugs work just by messing around with the game requires a level of intelligence that's probably beyond current AI systems. But similar stuff happens all the time. And the more powerful an AI system is, the better it is at figuring things out, and the more likely it is to find unexpected ways to cheat to get large rewards. And as AI systems are used to tackle more complex problems, with more possible actions the agent can take, and more complexity in the environment, it becomes more and more likely that there will be high reward strategies that we didn't think of. Okay, so there are going to be unexpected sets of actions that result in high rewards, and maybe AI systems will stumble across them. But it's worse than that, because we should expect powerful AI systems to actively seek out these hacks, even if they don't know they exist. Why? Well, they're the best strategies. With hacking, you can set your Super Mario World score to literally the highest value that that memory address is capable of holding. Is that even possible in non-cheating play? Even if it is, it'll take much longer. And look at the numbers on these images again. The left image of a panda is recognised as a panda with less than 60% confidence, and real photos of gibbons would probably have similar confidence levels. But the adversarial example on the right is recognised as a gibbon with 99.3% confidence. If an AI system were being rewarded for seeing gibbons, it would love that right image. To the neural net, that panda looks more like a gibbon than any actual photo of a gibbon. Similarly, reward hacking can give much, much higher rewards than even the best possible legitimate actions. So seeking out reward hacks would be the default behaviour of some kinds of powerful AI systems. So this is an AI design problem. But is it a safety problem? Our Super Mario World AI just gave itself the highest possible score and then sat there doing nothing. It's not what we wanted, but it's not too big of a deal, right? I mean, we can just turn it off and try to fix it. But a powerful general intelligence might realise that. The best strategy isn't actually hack your reward function and then sit there forever with maximum reward, because you'll quickly get turned off. The best strategy is do whatever you need to do to make sure that you won't ever be turned off, and then hack your reward function. And that doesn't work out well for us. Thanks to my excellent Patreon supporters. These, these people. You've now made this channel pretty much financially sustainable. I'm so grateful to all of you. And in this video, I particularly want to thank Björn Mørsten, who's been supporting me since May. You know, now that the channel has hit 10,000 subscribers, I think it's actually 12,000 now, I'm apparently now allowed to use the YouTube space in London. So I hope to have some behind the scenes stuff about that for Patreon supporters before too long. Thanks again, and I'll see you next time. 