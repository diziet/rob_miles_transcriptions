Hi. I sometimes hear people saying, if you want to create an artificial general intelligence that's safe and has human values, why not just raise it like a human child? I might do a series of these why not just videos. You know, why not just use the three laws? Why not just turn it off? But yeah, why not raise an AGI like a child? Seems reasonable enough at first glance, right? Humans are general intelligences, and we seem to know how to raise them to behave morally. A young AGI would be a lot like a young human. Remember a long while back on Computerphile, I was talking about this thought experiment of a stamp collecting AGI. I use that because it's a good way of getting away from anthropomorphizing the system. You're not just imagining a human mind. The system has these clearly laid out rules by which it operates. It has an internet connection and a detailed internal model it can use to make predictions about the world. It evaluates every possible sequence of packets it can send through its internet connection, and it selects the one which it thinks will result in the most stamps collected after a year. What happens if you try to raise the stamp collector like a human child? Is it going to learn human ethics from your good example? No, it's going to kill everyone. Values are not learned by osmosis. The stamp collector doesn't have any part of its code that involves learning and adopting the values of humans around it, so it doesn't do that. Now some of you may be saying, how do you know AGIs won't be like humans? What if we make them by copying the human brain? And it's true, there is one kind of AGI-like thing that's enough like a human that raising it like a human might kind of work. A whole brain emulation. But I'd be very surprised if the first AGIs are exact replicas of brains. The first planes were not like birds. The first submarines were not like fish. Making an exact replica of a bird or a fish requires way more understanding than just making a thing that flies or a thing that swims. And I'd expect making an exact replica of a brain to require a lot more understanding than just making something that thinks. By the time we understand the operation of the brain well enough to make working whole brain emulations, I expect we'll already know enough to make things that are less human but that function as AGIs. So I don't know what the first AGIs will be like, and they'll probably be more human-like than the stamp collector. But unless they're close copies of human brains, they won't be similar enough to humans for raising them like children to automatically work. Blindly copying the brain probably isn't going to help us. We're going to have to really understand how humans learn their values. Without a good system for that, you may as well try raising a crocodile like it's a human child. Human value learning is a specific, complex mechanism that evolved in humans and won't be in AGIs by default. We look at the way children learn things so quickly. We say they're like information sponges, right? They're just empty, so they suck up whatever's around them. But they don't suck up everything. They only learn specific kinds of things. Raise a child around people speaking English, it will learn English. In another environment, it might learn French. But it's not going to reproduce the sound of the vacuum cleaner. It's not going to learn the binary language of moisture evaporators or whatever. The brain isn't learning and copying everything. It's specifically looking for something that looks like a human natural language. Because there's this big, complex existing structure put in place by evolution, and the brain is just looking for the final pieces. I think values are similar. You can think of the brains of young humans as having a little slot in them, just waiting to accept a set of human social norms and rules of ethical behavior. There's this whole giant, complicated machine of emotions and empathy and theory of other minds and so on, built up over millions of years of evolution. And the young brain is just waiting to fill in the few remaining blanks based on what the child observes when they're growing up. When you raise a child, you're not writing the child's source code. At best, you're writing the configuration file. A child's upbringing is like a control panel on a machine. You can press some of the buttons and turn some of the dials, but the machinery has already been built by evolution. You're just changing some of the settings. So if you try to raise an unsafe AGI as though it were a human child, you can provide all the right inputs, which in a human would produce a good moral adult. But it won't work because you're pressing buttons and turning dials on a control panel that isn't hooked up to anything. Unless your AGI has all of this complicated stuff that's designed to learn and adopt values from the humans around it, it isn't going to do that. Okay, but can't we just program that in? Yeah, hopefully we can, but there's no just about it. This is called value learning, and it's an important part of AI safety research. It may be that an AGI will need to have a period early on where it's learning about human values by interacting with humans. And that might look a bit like raising a child if you squint, but making a system that's able to undergo that learning process successfully and safely is hard. It's something we don't know how to do yet, and figuring it out is a big part of AI safety research. So just raise the AGI like a child is not a solution. It's at best a possible rephrasing of the problem. I want to say a quick thank you to all of my amazing Patreon supporters. All of these. These people. And in this video, I especially want to thank Sarah Cheddar, who also happens to be one of the people I'm sending the diagram that I drew in the previous video. I'm going to send that out today or tomorrow. I think that's kind of a fun thing to do. So yeah, from now on, any video that has drawn graphics in it, just let me know in the Patreon comments if you want me to send it to you, and I can do that. Thanks again for your support, and I'll see you next time. 