Hi, this is part of a series about the paper Concrete Problems in AI Safety, which looks at preventing possible accidents in AI systems. Last time we talked about avoiding negative side effects, and how one way of doing that is to create systems that try not to have too much impact, to not change the environment around them too much. This video is about a slightly more subtle idea than penalising impact, penalising influence. So suppose we have a robot, it's a cleaning robot, so it's got a mop and a bucket and an apron. I'm trying something new here, bear with me. So the robot knows that there's a mess over here that it needs to clean up. But in between the robot and the mess is the server room, which is full of expensive and delicate equipment. Now if an AI system doesn't want to have a large impact, it won't make plans that involve tipping the bucket of water over the servers. But maybe we can be safer than that. We might want our robot to not even want to bring the bucket of water into the server room, to have a preference for going around it instead. We might want it to think something like, not only do I not want to have too big of an impact on my surroundings, I also don't want to put myself in a situation where it would be easy for me to have a big impact on my surroundings. How do we formalise that idea? Well, perhaps we can use information theory. The paper talks about an information-theoretic metric called empowerment, which is a measure of the maximum possible mutual information between the agent's potential future actions and the potential future state. That's equivalent to the capacity of the information channel between the agent's actions and the environment, i.e. the rate that the agent's actions transmit information into the environment, measured in bits. The more information an agent is able to transfer into their environment with their actions, the more control they have over their environment, the more empowered the agent is. So if you're stuck inside a solid locked box, your empowerment is more or less zero. None of the actions you can take will transmit much information into the world outside the box. But if you have the key to the box, your empowerment is much higher, because now you can take actions that will have effects on the world at large. You've got options. People have used empowerment as a reward for experimental AI systems, and it makes them do some interesting things, like picking up keys, avoiding walls, even things like balancing an inverted pendulum or a bicycle. You don't have to tell it to keep the bike balanced, it just learns that if the bike falls over, the agent's actions will have less control over the environment, so it wants to keep the bike upright. So empowerment is a pretty neat metric, because it's very simple, but it captures something that humans and other intelligent agents are likely to want. We want more options, more freedom, more capabilities, more influence, more control over our environment. And maybe that's something we don't want our AI systems to want. Maybe we want to say, clean up that mess, but try not to gain too much control or influence over your surroundings. Don't have too much empowerment. That could make the robot think, if I bring this bucket of water into the server room, I'll have the option to destroy the servers, so I'll go around to avoid that empowerment. Okay, so now we're at that part of the video. What's wrong with this? Why might it not work? Pause the video and take a second to think. Well, there are a few problems. One thing is that, because it's measuring information, we're really measuring precision of control, rather than magnitude of impact. As an extreme example, suppose you've got your robot in a room, and the only thing it has access to is a big button, which, if pressed, will blow up the moon. That actually only counts as one bit of empowerment. The button is either pressed or not pressed, the moon has exploded or not, two choices, so one bit of information, one bit of empowerment. On the other hand, if the robot has an ethernet cable that's feeding out lots of detailed debug information about everything the robot does, and that's all being logged somewhere, that's loads of information transfer. Loads of mutual information with the environment, so loads of empowerment. The robot cares way more about unplugging the debug cable than anything to do with the button. Now we have another possible problem, which is perverse incentives. Okay, so this button is only one bit of empowerment, nowhere near as big a deal as the debug cable, but the robot still cares about it to some extent, and wants to avoid putting itself in this situation where it can blow up the moon. However, if it finds itself already in a situation where it has one bit of empowerment because of this button, the easiest way to reduce that is by pressing the button. Once the button's pressed, the moon is blown up, the button doesn't work anymore, so the robot then has basically zero bits of empowerment. It's just in a box with an unconnected button. And now it's content that it's managed to make itself safe. It finally has no influence over the world. So yeah, in this admittedly contrived scenario, an empowerment-reducing robot will unplug its debug cable and then blow up the moon. That's not safe behavior. Why did we think this might be a good idea? Well, it just makes the point that even very simple information-theoretic metrics can describe interesting abstract properties like influence over the environment. So maybe doing something a little bit cleverer than just penalizing empowerment might actually be useful. A more sophisticated metric, a better architecture around it, you know, there could be some way to make this work. So this is an area that's probably worth looking into by AI safety researchers. So that's all for now. Next thing in the paper is multi-agent approaches, which should be really interesting. Make sure to subscribe and hit the bell if you want to be notified when that's out. Also, make sure you're subscribed to Computerphile, because I'm probably going to make some new videos there as well, since some of the multi-agent stuff is closely related to the stop button problem that I already talked about. So it might be nice to put those together. Thanks for watching. I hope to see you next time. In this video, I want to thank Oystein Fligt, who's supported me on Patreon since April. Thank you. And thank you again to all of my wonderful Patreon supporters. All of these people. I've been setting up a room in my house to be a full-time studio. I might make a behind-the-scenes video about that soon. Oh, and I've got these pictures that I drew while making this video, which I have no use for now. Does anyone want them? God, the internet is weird sometimes, isn't it? But yeah, I can probably post them to supporters if anyone wants one, let me know. I was close. 