Hi, just a quick one today because I've got a lot of other stuff going on right now, more on that later hopefully. This is a follow up to the previous video about avoiding negative side effects, link in the doobly-doo so watch that first if you haven't yet. I wanted to talk about another possible problem you might have with low impact or impact regularizing agents, one I forgot to mention in the last video. Some people definitely mentioned some things close to this in the comments. If you've got this, good job, feel free to brag about it. The problem I want to talk about is avoiding positive side effects. So before we talked about how most side effects are negative, rather than having to figure out how to avoid negative side effects, maybe it's a more tractable problem to just avoid all side effects. But if you look at the side effects of, for example, getting me a cup of tea, that includes effects like me being happy, or me not being thirsty, or me feeling more awake because I've had some caffeine. In other words, every reason I wanted a cup of tea in the first place. If the robot can think of a way of getting me a cup of tea that still results in me being thirsty and tired, just as though I hadn't had a cup of tea, it will prefer that option. Now I can't, off the top of my head, think of any way to do that, assuming we've defined what a cup of tea is well enough, so maybe the robot will conclude that these positive side effects are unavoidable, just like how using up a tea bag is an unavoidable side effect. But it's not great that it's looking for ways to negate the benefits of its work. And again, the more intelligent a system becomes, the more it's going to be able to figure out ways to do that. Or how about this? We've set up our system to try to keep the outcomes close to what it predicts would happen if the robot just sat there and did nothing at all, right? What actually would happen if it just sat there and did nothing instead of getting me a cup of tea? One thing is, I would probably become confused and maybe annoyed, and I would try to debug it and figure out why it wasn't working. So our robot may want to try and find a course of action such that it does get me a cup of tea, but still leaves me confused and annoyed and trying to debug it and figure out why it's not working, because that would have been the outcome of the safe policy. How do we deal with that issue? And all of this is assuming that we can come up with an impact metric or a distance metric between world states that properly captures our intuitions. There are a whole bunch of difficulties there, but that's a topic for another video. So that's it for now. Next time, I think we'll keep talking about the paper Concrete Problems in AI Safety and look at some other approaches to avoiding negative side effects. So be sure to click on the bell if you want to be notified when that comes out. Oh, and if you think this stuff is interesting and you're at a place in your life where you're thinking about your career, the careers advice organization 80,000 Hours has just put up a really good guide to careers in AI safety. I'll put a link to that in the description as well. Highly recommended checking that out, especially if you don't think you're technical enough to directly work on the research. There are a lot of different ways you might want to get involved. Let me know in the comments if you'd like me to make some videos about AI safety careers, you know, if that's something you'd want to see. And to end the video, a quick thank you to my Patreon supporters. That's all of these excellent people right here. I especially want to thank Yonatan R, represented here by this sock. His choice, not mine. Thank you so much for your support. I hope to have some more behind the scenes stuff going up on Patreon this weekend if I can. I hope you like it. 