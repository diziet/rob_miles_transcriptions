Hi, I just finished recording a new video for Computerphile where I talk about this paper, Concrete Problems in AI Safety. I'll put a link in the doobly-doo to the Computerphile video when that comes out. Here's a quick recap of that before we get into this video. AI can cause us all kinds of problems and just recently people have started to get serious about researching ways to make AI safer. A lot of the AI safety concerns are kind of science fiction sounding, problems that could happen with very powerful AI systems that might be a long way off. This makes those problems kind of difficult to study because we don't know what those future AI systems would be like, but there are similar problems with AI systems that are in development today or even out there operating in the real world right now. This paper points to five problems which we can get started working on now that will help us with current AI systems and will hopefully also help us with the AI systems of the future. The Computerphile video gives a quick overview of the five problems laid out in the paper and this video is just about the first of those problems, avoiding negative side effects. I think I'm going to do one video on each of these and make it a series of five. So, avoiding negative side effects. Let's use the example I was talking about in the stop button videos on Computerphile. You've got a robot, you want it to get you a cup of tea but there's something in the way, maybe a baby or a priceless Ming vase on a narrow stand, you know, whatever, and your robot runs into the baby or knocks over the vase on the way to the kitchen and then makes you a cup of tea. So the system has achieved its objective, it's got you some tea, but it's had this side effect which is negative. Now, we have some reasons to expect negative side effects to be a problem with AI systems. Part of the problem comes from using a simple objective function in a complex environment. You think you've defined a nice simple objective function that looks something like this, and that's true, but when you use this in a complex environment you've effectively written an objective function that looks like this, or more like this. Anything in your complex environment not explicitly given value by your objective function is implicitly given zero value, and this is a problem because it means your AI system will be willing to trade arbitrarily huge amounts of any of the things you didn't specify in your objective function for arbitrarily small amounts of any of the things you did specify. If it can increase its ability to get you a cup of tea by 0.0001%, it will happily destroy the entire kitchen to do that. If there's a way to gain a tiny amount of something it cares about, it's happy to sacrifice any amount of any of the things it doesn't care about, and the smarter it is the more of those ways it can think of. So this means we have to expect the possibility of AI systems having very large side effects by default. You could try to fill your whole thing in with values, but it's not practical to specify every possible thing you might care about. You'd need an objective function of similar complexity to the environment. There are just too many things to value, and we don't know them all. You know, you'll miss some, and if any of the things you miss can be traded in for a tiny amount of any of the things you don't miss, well that thing you missed is potentially gone. But at least these side effects tend to be pretty similar. The paper uses examples like a cleaning robot that has to clean an office. In the stop button problem computer file video, I used a robot that's trying to get you a cup of tea. But you can see that the kinds of negative side effects we want to avoid are pretty similar, even though the tasks are different. So maybe, and this is what the paper suggests, maybe there's a single thing we can figure out that would avoid negative side effects in general. One thing we might be able to use is the fact that most side effects are bad. I mean, naively you might think that doing a random action would have a random value, right? Maybe it helps, maybe it hurts, maybe it doesn't matter, but it's random. But actually, the world is already pretty well optimized for human values, especially the human inhabited parts. It's not like there's no way to make our surroundings better, but it's way easier to make them worse. For the most part, things are how they are because we like it that way, and a random change wouldn't be desirable. So rather than having to figure out how to avoid negative side effects, maybe it's a more tractable problem to just avoid all side effects. That's the idea of the first approach the paper presents, defining an impact regularizer. What you do basically is penalize change to the environment. So the system has some model of the world, right? It's keeping track of world state as part of how it does things. So you can define a distance metric between world states so that for any two world states, you can measure how different they are. World states that are very similar have a low distance from each other. World states that are very different have a big distance. And then you just say, okay, you get a bunch of points for getting me a cup of tea, but you lose points according to the new world state's distance from the initial world state. So this isn't a total ban on side effects, or the robot wouldn't be able to change the world enough to actually get you a cup of tea. It's just incentivized to keep the side effects small. There's allowed to be one less tea bag. That's unavoidable in making tea, but breaking the vase that's in the way is an unnecessary change to the world. So the robot will avoid it. The other nice thing about this is the original design wouldn't have cared, but now the robot will put the container of tea back and close the cupboard, you know, put the milk back in the fridge, maybe refill the kettle, try to make the world as close as possible to how it was when it started. So that's pretty neat. Like we've added this one simple rule and the thing's already better than some of the housemates I've had. So how does this go wrong? Think about it for a second. Pause the video. I'll wait. Okay. So the robot steers around the vase to avoid changing the environment too much. And it goes on into the kitchen where it finds your colleague is making herself some coffee. Now that's not okay, right? She's changing the environment. None of these changes are needed for making you a cup of tea. And now the world is going to be different, which reduces the robot's reward. So the robot needs to try to stop that from happening. We didn't program it to minimize its changes to the world. We programmed it to minimize all change to the world. That's not ideal. So how about this? The system has a world model. It can make predictions about the world. So how about you program it with the equivalent of saying, use your world model to predict how the world would be if you did nothing. If you just sent no signals of any kind to any of your motors and just sat there, and then try and make the end result of this action close to what you imagine would happen in that case. Or imagine the range of likely worlds that would happen if you did nothing, and try and make the outcome close to something in that range. So then the bot is thinking, okay, if I just sat here and did nothing at all, that vase would probably still be there. You know, the baby would still be wandering around and not squished, and the person making coffee would make their coffee, and everything in the kitchen would be tidy and in its place. So I have to try to make a cup of tea happen without ending up too far from that. Pretty nice, right? How does that break? Again, take a second, give it some thought, pause the video. How might this go wrong? What situation might this not work in? Okay, well, what if your robot is driving a car? It's doing 70 miles an hour on the motorway. And now it's trying to make sure that things aren't too different to how they would be if it didn't move any of its motors. Yeah, doing nothing is not always a safe policy. But still, if we can define a known safe policy, then this kind of thing is nice, because rather than having to define for each task how to do the task safely, we could maybe come up with one safe policy that doesn't have to do anything except be safe, and have the system always just try to make sure that the outcome of whatever it's trying to do isn't too different from the safe policy's outcome. Oh, and there's another possible cause of issues with this kind of approach, in case the things you guessed were different, maybe it's this. It can be very dependent on the specifics of your world state representation and your distance metric. Like, suppose there's a fan, there's a spinning fan in the room. Is that in a steady state, you know, the fan is on? Or is it in a constantly changing state, like the fan is at 10 degrees? Oh no, it's at 20 degrees. Oh, it's at 30. You know, different world models will represent the same thing, either a steady state or constantly changing state, and there's not necessarily a right answer there. Like, which aspects of an object state are important and which aren't is not necessarily an easy question to reliably answer. Would the robot leave the fan alone, or try and make sure it was at the same angle it was before? Okay, I think that's enough for one video. Probably in the next one we can look at some of the other approaches laid out in the paper for avoiding negative side effects, so be sure to subscribe if you found this interesting, and I hope to see you next time. Hi, I just want to end this video with a quick thank you to my excellent Patreon supporters. All of these, these people, yeah. And today I especially want to thank Joshua Richardson, who's supported me for a really long time. Thank you. You know, it's thanks to your support that I've been able to buy some proper studio lighting now. So I have a proper softbox, which, this is the first time I'm using it, I hope is working okay. It should really reduce my reliance on sunlight, which should make me a lot more flexible about when I can record video, so that's a tremendous help. I'm putting up a little video on Patreon of, you know, unboxing it and putting it together and stuff, which you can check out if you're interested. So thank you again, and I'll see you next time. 