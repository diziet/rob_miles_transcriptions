1
00:00:00,000 --> 00:00:04,400
Hi. In the previous video, I talked about this open letter, which has been signed by a lot of

2
00:00:04,400 --> 00:00:09,840
prestigious people, which talks about how there are risks and possible problems associated with

3
00:00:09,840 --> 00:00:14,640
AI. And it says we need to do more thinking about the future of AI technologies and the impact

4
00:00:14,640 --> 00:00:19,440
they're going to have on society. But it's worth noting that in those 8,000 or so signatories,

5
00:00:19,440 --> 00:00:23,360
there's quite a broad range of opinions about the specific nature of these problems,

6
00:00:24,000 --> 00:00:29,600
which problems are most important, what the time scales are. So to say all of these people are

7
00:00:29,600 --> 00:00:33,680
concerned in some sense with AI safety is not to say that they all agree with each other.

8
00:00:33,680 --> 00:00:37,440
And the document the open letter links to lists a whole bunch of different things. And I'm going

9
00:00:37,440 --> 00:00:42,160
to talk about some of those now. So the first thing is economic impact. Things like technological

10
00:00:42,160 --> 00:00:48,000
unemployment, you know, what if AI puts us all out of a job, or the effects on economic inequality,

11
00:00:48,000 --> 00:00:53,520
in the sense that if the AI technologies produce a huge amount of new wealth, not everyone is going

12
00:00:53,520 --> 00:00:57,360
to benefit from that wealth equally. And this can increase inequality, which can cause other

13
00:00:57,360 --> 00:01:01,760
problems. Also, some people are concerned that even if we manage to set up a situation where

14
00:01:01,760 --> 00:01:05,440
people don't need to be employed in order to get the resources that they need to live, there's a

15
00:01:05,440 --> 00:01:09,760
question of, in a world in which nobody has to work, what are people actually doing with their

16
00:01:09,760 --> 00:01:14,640
time? Is it just leisure? How do people get a feeling of achievement? And ideally, they shouldn't

17
00:01:14,640 --> 00:01:18,240
just have a feeling of achievement, they should have actual achievement, but it's hard to have

18
00:01:18,240 --> 00:01:22,480
actual achievement in a world in which you're outperformed in every way by machines. There's

19
00:01:22,480 --> 00:01:28,160
concern about AI ethics, things like driverless vehicles. This has been done to death, but that's

20
00:01:28,160 --> 00:01:31,360
just because it's an interesting problem that we're not sure how to deal with yet. So you know,

21
00:01:31,360 --> 00:01:35,760
you have a self-driving car, it's in some scenario where it has to hit one person or the other,

22
00:01:35,760 --> 00:01:40,320
and then the question is how does it make that decision? This is a philosophical question,

23
00:01:40,320 --> 00:01:43,760
right? It's an instance of the trolley problem in real life. So there are really two questions

24
00:01:43,760 --> 00:01:48,240
here. The first is the ethical one. How should we design our AI systems to make ethical decisions

25
00:01:48,240 --> 00:01:52,560
in these situations? The interesting thing to me about this is that humans are routinely in

26
00:01:52,560 --> 00:01:57,120
these situations, right? Car crashes happen regularly, but we don't have time to make

27
00:01:57,120 --> 00:02:00,960
ethical decisions. If you're in this type of scenario in which you're forced to hit someone

28
00:02:00,960 --> 00:02:07,120
and you have to choose, no one's going to blame you for choosing one person or the other because

29
00:02:07,120 --> 00:02:10,560
you're in the middle of a car crash, like almost by definition, you have no time to think.

30
00:02:10,560 --> 00:02:14,320
Whereas with a self-driving car, the decision of how you want your car to behave needs to be

31
00:02:14,320 --> 00:02:19,120
made beforehand with all the time in the world and no excuses. So what's new isn't the decision

32
00:02:19,120 --> 00:02:23,680
itself so much as it's having enough time to think. Also, side note, prediction. I'm calling

33
00:02:23,680 --> 00:02:28,640
it now. When self-driving cars are common, we will have a problem with morons deliberately

34
00:02:28,640 --> 00:02:32,240
jumping in front of them for fun. Anyway, that's one question. The other question is legal.

35
00:02:33,120 --> 00:02:37,280
What should the law be about this? Who's liable? The person in the car? The person who owns the

36
00:02:37,280 --> 00:02:40,960
car? The company that wrote the software? And practically speaking, the way the software

37
00:02:40,960 --> 00:02:44,560
actually gets written will be determined by the legal question, not the ethical one.

38
00:02:44,560 --> 00:02:48,960
There's concerns about military use of this technology, autonomous weapons systems and so

39
00:02:48,960 --> 00:02:53,920
on, the ethics of that. And some people are worried about discrimination in machine learning systems,

40
00:02:53,920 --> 00:02:57,760
that these systems we build to process people's data and make decisions about

41
00:02:57,760 --> 00:03:02,720
insurance premiums, hiring decisions, loans, all kinds of things these systems can be used for.

42
00:03:02,720 --> 00:03:07,040
They may end up being racist or sexist, things like that, which is another potential issue.

43
00:03:07,040 --> 00:03:11,360
There's also privacy concerns. People are wary about the ability of AI systems to

44
00:03:12,080 --> 00:03:16,560
deduce private information from the vast amounts of public information available to them. The

45
00:03:16,560 --> 00:03:20,960
classic example of this is the young woman who started receiving coupons for baby food and other

46
00:03:20,960 --> 00:03:25,440
baby related stuff from her supermarket. And her father stormed in there to complain, but she

47
00:03:25,440 --> 00:03:29,600
actually was pregnant. And the supermarket's product recommendation algorithms had noticed

48
00:03:29,600 --> 00:03:33,520
that before her own father. I don't even know if that story is true, but it illustrates the point.

49
00:03:33,520 --> 00:03:38,240
AI may be able to discover things about you that you didn't intend to make public.

50
00:03:38,240 --> 00:03:42,480
So all of those are problems that can happen when AI systems are working more or less as they were

51
00:03:42,480 --> 00:03:46,400
intended to work. Then you have the stuff that's more what I've been talking about in earlier

52
00:03:46,400 --> 00:03:51,440
videos, which is problems that happen when AI systems have unexpected, unintended behavior,

53
00:03:51,440 --> 00:03:56,000
which is harmful. Either during development, like in the stop button problem, where I'm talking

54
00:03:56,000 --> 00:03:59,680
about this robot that wants to make you a cup of tea and ends up running over a child who was in

55
00:03:59,680 --> 00:04:04,320
the way, that kind of accident, or problems that only happen once the system is deployed and it

56
00:04:04,320 --> 00:04:08,480
starts behaving in ways that were unexpected and have negative consequences. This can already be a

57
00:04:08,480 --> 00:04:12,720
real problem with existing machine learning systems. And as those systems get more powerful

58
00:04:12,720 --> 00:04:17,360
and more influential and get relied on more and more, uh, those problems are going to become more

59
00:04:17,360 --> 00:04:23,040
and more important. And then you have the question of general super intelligence, intelligent systems

60
00:04:23,040 --> 00:04:27,520
that dramatically outperform human intelligence across a wide range of domains. That can be a

61
00:04:27,520 --> 00:04:32,720
maximally bad problem. So when people say, Oh yes, I'm concerned about possible problems with AI.

62
00:04:32,720 --> 00:04:36,880
They're really talking about a very wide range of possible problems here. I'm going to go back

63
00:04:36,880 --> 00:04:42,000
to the nuclear analogy I've used in the past. Uh, imagine if some point around the turn of the last

64
00:04:42,000 --> 00:04:46,800
century, a load of scientists got together and all signed a letter saying we're concerned about

65
00:04:46,800 --> 00:04:51,520
the risks of nuclear material. And so thousands of people sign this thing. And then it turns out

66
00:04:51,520 --> 00:04:56,480
that some of those people are talking about like radiation poisoning, you know, uh, Marie Curie

67
00:04:56,480 --> 00:05:00,240
died young as a consequence of the radiation she was exposed to while doing her research.

68
00:05:00,240 --> 00:05:03,920
But other people are talking more about things like the demon core,

69
00:05:03,920 --> 00:05:07,600
this plutonium core, which caused a lot of problems during the Manhattan project,

70
00:05:07,600 --> 00:05:11,840
where there were accidents that resulted in a sudden powerful burst of radiation,

71
00:05:11,840 --> 00:05:15,600
which gave acute radiation poisoning to the scientists conducting the experiment

72
00:05:15,600 --> 00:05:19,120
and anyone who happened to be standing nearby. Or you have other people are more concerned

73
00:05:19,120 --> 00:05:22,960
with risks associated with nuclear power. You know, you have this nuclear waste generated

74
00:05:22,960 --> 00:05:27,760
that needs to be disposed of. That's a problem. Or you have the other problem with nuclear power,

75
00:05:27,760 --> 00:05:31,440
which is the possibility of meltdowns, which can be disastrous. And then you have other people

76
00:05:31,440 --> 00:05:36,000
saying, well, nevermind all that. What about nuclear weapons? Right? This is the big problem.

77
00:05:36,000 --> 00:05:40,160
What if people build nuclear weapons that can kill millions of people? And then if they proliferate,

78
00:05:40,160 --> 00:05:45,440
then that can cause vast problems for humanity, like global thermonuclear war. That's an issue.

79
00:05:46,080 --> 00:05:51,520
And then, you know, beyond that, you also have concerns like, okay, during the Manhattan project,

80
00:05:51,520 --> 00:05:57,760
there was concern that the Trinity nuclear test might ignite the atmosphere. The principle of this

81
00:05:57,760 --> 00:06:02,240
is quite similar to the way that a hydrogen bomb works. You have a standard atom bomb next to a

82
00:06:02,240 --> 00:06:08,560
certain amount of hydrogen. And then, uh, I'm not a physicist, but more or less the energy from the

83
00:06:08,560 --> 00:06:13,280
fission reaction, the explosion of the atom bomb kicks off a fusion reaction in the hydrogen,

84
00:06:13,280 --> 00:06:18,400
the hydrogen atoms fuse and give off a tremendous amount of energy, which is another chain reaction.

85
00:06:18,400 --> 00:06:22,160
It's the same thing that's happening in the sun, right? Turning hydrogen into helium. But of course

86
00:06:22,160 --> 00:06:27,200
in the sun, heavier elements are also fused up to and including, for example, nitrogen. So there

87
00:06:27,200 --> 00:06:31,840
was some concern when people were developing the atom bomb. What if we kickstart a fusion chain

88
00:06:31,840 --> 00:06:37,200
reaction in nitrogen? Because the atmosphere is like 78% nitrogen. There's a chance that we turn

89
00:06:37,200 --> 00:06:41,600
the entire atmosphere into a thermonuclear bomb effectively. From the first time that this issue

90
00:06:41,600 --> 00:06:45,520
was raised, it seemed pretty clear that it wasn't going to happen in the sense that the amount of

91
00:06:45,520 --> 00:06:50,240
energy given off by an atom bomb or a hydrogen bomb is just not big enough to do this. Their

92
00:06:50,240 --> 00:06:53,520
understanding of physics at the time pointed to it being nowhere near enough energy. But

93
00:06:54,480 --> 00:06:58,080
at the same time, I don't believe they'd ever actually fused nitrogen in the lab. So they

94
00:06:58,080 --> 00:07:02,240
didn't know for sure exactly what conditions cause nitrogen to fuse. And also they'd never

95
00:07:02,240 --> 00:07:06,000
set off a nuclear bomb before, so they didn't know for sure exactly how much energy they were

96
00:07:06,000 --> 00:07:10,800
going to get out of it. So there was a non-zero probability from their perspective when they set

97
00:07:10,800 --> 00:07:15,840
off the Trinity explosion that it would end all of humanity instantaneously, more or less,

98
00:07:15,840 --> 00:07:19,920
right there and then. So before they did it, they ran the numbers again in a few different ways,

99
00:07:19,920 --> 00:07:23,840
and they looked at it very carefully and made very, very sure ahead of time that they were

100
00:07:23,840 --> 00:07:27,600
very confident this wasn't going to be an issue, that they weren't going to ignite the atmosphere.

101
00:07:28,160 --> 00:07:32,720
So people have had various concerns about nuclear material, ranging from, oh, if you work with it,

102
00:07:32,720 --> 00:07:37,360
you might get radiation poisoning, to if you screw it up, you may destroy all life on the planet

103
00:07:37,360 --> 00:07:43,040
forever in a giant fire explosion. And so getting people to sign a letter that says we're concerned

104
00:07:43,040 --> 00:07:47,920
about nuclear material would cover a broad range of possibilities. So I like a nuclear analogy here

105
00:07:47,920 --> 00:07:52,080
because it helps me explain something about a paper that I'm going to go through soon,

106
00:07:52,080 --> 00:07:56,880
Concrete Problems in AI Safety, because it's concerned about accidents. Specifically,

107
00:07:56,880 --> 00:08:01,280
the paper is looking at unintended and harmful behavior that may emerge from poor design of

108
00:08:01,280 --> 00:08:06,160
real-world AI systems. And another way that this is similar to nuclear research is that

109
00:08:06,720 --> 00:08:10,480
the type of knowledge you need in order to prevent something like the Demon Core problem

110
00:08:10,480 --> 00:08:14,400
of something going supercritical and dumping out a huge amount of radiation in your lab

111
00:08:14,400 --> 00:08:20,240
is the same kind of understanding of radioactivity and fissile material in general that you need in

112
00:08:20,240 --> 00:08:24,480
order to understand how nuclear bombs work and make sure you don't set off one of those by accident,

113
00:08:24,480 --> 00:08:29,680
or to understand what storage and transportation technology is necessary for nuclear waste,

114
00:08:29,680 --> 00:08:34,560
or how to prevent meltdowns. Like, a general good understanding of nuclear physics will help you

115
00:08:34,560 --> 00:08:39,120
with protecting yourself from getting radiation poisoning, and also hopefully protect you from

116
00:08:39,120 --> 00:08:43,920
accidentally igniting the atmosphere. And I think it's the same in AI, and I think that's part of

117
00:08:43,920 --> 00:08:48,800
what Concrete Problems in AI Safety is trying to do. It's trying to bring together the people who

118
00:08:48,800 --> 00:08:53,440
are concerned about possibly igniting the atmosphere, the real apocalyptic superintelligence

119
00:08:53,440 --> 00:08:58,000
problems, and the people looking at the more run-of-the-mill, what-if-my-robot-ignores-my-stop

120
00:08:58,000 --> 00:09:02,880
button type problems. And it's trying to point out areas of research that we could look into

121
00:09:02,880 --> 00:09:06,720
that would actually provide progress on both of those things, where there's overlap,

122
00:09:06,720 --> 00:09:10,240
things that we can study that would help us with current existing AI systems,

123
00:09:10,240 --> 00:09:15,200
but that may also help avoiding these huge global-scale superintelligence-related problems.

124
00:09:15,200 --> 00:09:19,840
Problems which, like igniting the atmosphere, may or may not at this point actually turn out

125
00:09:19,840 --> 00:09:24,320
to be a real problem, but which are still definitely worth looking into because these

126
00:09:24,320 --> 00:09:30,640
stakes are so unbelievably high.

127
00:09:38,480 --> 00:09:42,160
I want to end this video with a quick thank you to my excellent Patreon supporters.

128
00:09:42,160 --> 00:09:46,800
It's these wonderful people around here. I'm always looking for ways to make good use of

129
00:09:46,800 --> 00:09:51,680
your support to improve my videos. I recently built my own autocue, which I think works really

130
00:09:51,680 --> 00:09:56,960
well. I've put up a behind-the-scenes video on Patreon of how I did that, if you want to check

131
00:09:56,960 --> 00:10:03,680
that out. But in this video, I especially want to thank Stefan Skiles, who supports me for $20 a

132
00:10:03,680 --> 00:10:09,040
month. Thank you. I've actually just made a new $20 reward level. You can go check that out and

133
00:10:09,040 --> 00:10:12,080
let me know what you think. Thanks again, and I'll see you next time.

