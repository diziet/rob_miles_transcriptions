Hi. In the previous video, I talked about this open letter, which has been signed by a lot of prestigious people, which talks about how there are risks and possible problems associated with AI. And it says we need to do more thinking about the future of AI technologies and the impact they're going to have on society. But it's worth noting that in those 8,000 or so signatories, there's quite a broad range of opinions about the specific nature of these problems, which problems are most important, what the time scales are. So to say all of these people are concerned in some sense with AI safety is not to say that they all agree with each other. And the document the open letter links to lists a whole bunch of different things. And I'm going to talk about some of those now. So the first thing is economic impact. Things like technological unemployment, you know, what if AI puts us all out of a job, or the effects on economic inequality, in the sense that if the AI technologies produce a huge amount of new wealth, not everyone is going to benefit from that wealth equally. And this can increase inequality, which can cause other problems. Also, some people are concerned that even if we manage to set up a situation where people don't need to be employed in order to get the resources that they need to live, there's a question of, in a world in which nobody has to work, what are people actually doing with their time? Is it just leisure? How do people get a feeling of achievement? And ideally, they shouldn't just have a feeling of achievement, they should have actual achievement, but it's hard to have actual achievement in a world in which you're outperformed in every way by machines. There's concern about AI ethics, things like driverless vehicles. This has been done to death, but that's just because it's an interesting problem that we're not sure how to deal with yet. So you know, you have a self-driving car, it's in some scenario where it has to hit one person or the other, and then the question is how does it make that decision? This is a philosophical question, right? It's an instance of the trolley problem in real life. So there are really two questions here. The first is the ethical one. How should we design our AI systems to make ethical decisions in these situations? The interesting thing to me about this is that humans are routinely in these situations, right? Car crashes happen regularly, but we don't have time to make ethical decisions. If you're in this type of scenario in which you're forced to hit someone and you have to choose, no one's going to blame you for choosing one person or the other because you're in the middle of a car crash, like almost by definition, you have no time to think. Whereas with a self-driving car, the decision of how you want your car to behave needs to be made beforehand with all the time in the world and no excuses. So what's new isn't the decision itself so much as it's having enough time to think. Also, side note, prediction. I'm calling it now. When self-driving cars are common, we will have a problem with morons deliberately jumping in front of them for fun. Anyway, that's one question. The other question is legal. What should the law be about this? Who's liable? The person in the car? The person who owns the car? The company that wrote the software? And practically speaking, the way the software actually gets written will be determined by the legal question, not the ethical one. There's concerns about military use of this technology, autonomous weapons systems and so on, the ethics of that. And some people are worried about discrimination in machine learning systems, that these systems we build to process people's data and make decisions about insurance premiums, hiring decisions, loans, all kinds of things these systems can be used for. They may end up being racist or sexist, things like that, which is another potential issue. There's also privacy concerns. People are wary about the ability of AI systems to deduce private information from the vast amounts of public information available to them. The classic example of this is the young woman who started receiving coupons for baby food and other baby related stuff from her supermarket. And her father stormed in there to complain, but she actually was pregnant. And the supermarket's product recommendation algorithms had noticed that before her own father. I don't even know if that story is true, but it illustrates the point. AI may be able to discover things about you that you didn't intend to make public. So all of those are problems that can happen when AI systems are working more or less as they were intended to work. Then you have the stuff that's more what I've been talking about in earlier videos, which is problems that happen when AI systems have unexpected, unintended behavior, which is harmful. Either during development, like in the stop button problem, where I'm talking about this robot that wants to make you a cup of tea and ends up running over a child who was in the way, that kind of accident, or problems that only happen once the system is deployed and it starts behaving in ways that were unexpected and have negative consequences. This can already be a real problem with existing machine learning systems. And as those systems get more powerful and more influential and get relied on more and more, uh, those problems are going to become more and more important. And then you have the question of general super intelligence, intelligent systems that dramatically outperform human intelligence across a wide range of domains. That can be a maximally bad problem. So when people say, Oh yes, I'm concerned about possible problems with AI. They're really talking about a very wide range of possible problems here. I'm going to go back to the nuclear analogy I've used in the past. Uh, imagine if some point around the turn of the last century, a load of scientists got together and all signed a letter saying we're concerned about the risks of nuclear material. And so thousands of people sign this thing. And then it turns out that some of those people are talking about like radiation poisoning, you know, uh, Marie Curie died young as a consequence of the radiation she was exposed to while doing her research. But other people are talking more about things like the demon core, this plutonium core, which caused a lot of problems during the Manhattan project, where there were accidents that resulted in a sudden powerful burst of radiation, which gave acute radiation poisoning to the scientists conducting the experiment and anyone who happened to be standing nearby. Or you have other people are more concerned with risks associated with nuclear power. You know, you have this nuclear waste generated that needs to be disposed of. That's a problem. Or you have the other problem with nuclear power, which is the possibility of meltdowns, which can be disastrous. And then you have other people saying, well, nevermind all that. What about nuclear weapons? Right? This is the big problem. What if people build nuclear weapons that can kill millions of people? And then if they proliferate, then that can cause vast problems for humanity, like global thermonuclear war. That's an issue. And then, you know, beyond that, you also have concerns like, okay, during the Manhattan project, there was concern that the Trinity nuclear test might ignite the atmosphere. The principle of this is quite similar to the way that a hydrogen bomb works. You have a standard atom bomb next to a certain amount of hydrogen. And then, uh, I'm not a physicist, but more or less the energy from the fission reaction, the explosion of the atom bomb kicks off a fusion reaction in the hydrogen, the hydrogen atoms fuse and give off a tremendous amount of energy, which is another chain reaction. It's the same thing that's happening in the sun, right? Turning hydrogen into helium. But of course in the sun, heavier elements are also fused up to and including, for example, nitrogen. So there was some concern when people were developing the atom bomb. What if we kickstart a fusion chain reaction in nitrogen? Because the atmosphere is like 78% nitrogen. There's a chance that we turn the entire atmosphere into a thermonuclear bomb effectively. From the first time that this issue was raised, it seemed pretty clear that it wasn't going to happen in the sense that the amount of energy given off by an atom bomb or a hydrogen bomb is just not big enough to do this. Their understanding of physics at the time pointed to it being nowhere near enough energy. But at the same time, I don't believe they'd ever actually fused nitrogen in the lab. So they didn't know for sure exactly what conditions cause nitrogen to fuse. And also they'd never set off a nuclear bomb before, so they didn't know for sure exactly how much energy they were going to get out of it. So there was a non-zero probability from their perspective when they set off the Trinity explosion that it would end all of humanity instantaneously, more or less, right there and then. So before they did it, they ran the numbers again in a few different ways, and they looked at it very carefully and made very, very sure ahead of time that they were very confident this wasn't going to be an issue, that they weren't going to ignite the atmosphere. So people have had various concerns about nuclear material, ranging from, oh, if you work with it, you might get radiation poisoning, to if you screw it up, you may destroy all life on the planet forever in a giant fire explosion. And so getting people to sign a letter that says we're concerned about nuclear material would cover a broad range of possibilities. So I like a nuclear analogy here because it helps me explain something about a paper that I'm going to go through soon, Concrete Problems in AI Safety, because it's concerned about accidents. Specifically, the paper is looking at unintended and harmful behavior that may emerge from poor design of real-world AI systems. And another way that this is similar to nuclear research is that the type of knowledge you need in order to prevent something like the Demon Core problem of something going supercritical and dumping out a huge amount of radiation in your lab is the same kind of understanding of radioactivity and fissile material in general that you need in order to understand how nuclear bombs work and make sure you don't set off one of those by accident, or to understand what storage and transportation technology is necessary for nuclear waste, or how to prevent meltdowns. Like, a general good understanding of nuclear physics will help you with protecting yourself from getting radiation poisoning, and also hopefully protect you from accidentally igniting the atmosphere. And I think it's the same in AI, and I think that's part of what Concrete Problems in AI Safety is trying to do. It's trying to bring together the people who are concerned about possibly igniting the atmosphere, the real apocalyptic superintelligence problems, and the people looking at the more run-of-the-mill, what-if-my-robot-ignores-my-stop button type problems. And it's trying to point out areas of research that we could look into that would actually provide progress on both of those things, where there's overlap, things that we can study that would help us with current existing AI systems, but that may also help avoiding these huge global-scale superintelligence-related problems. Problems which, like igniting the atmosphere, may or may not at this point actually turn out to be a real problem, but which are still definitely worth looking into because these stakes are so unbelievably high. I want to end this video with a quick thank you to my excellent Patreon supporters. It's these wonderful people around here. I'm always looking for ways to make good use of your support to improve my videos. I recently built my own autocue, which I think works really well. I've put up a behind-the-scenes video on Patreon of how I did that, if you want to check that out. But in this video, I especially want to thank Stefan Skiles, who supports me for $20 a month. Thank you. I've actually just made a new $20 reward level. You can go check that out and let me know what you think. Thanks again, and I'll see you next time. 