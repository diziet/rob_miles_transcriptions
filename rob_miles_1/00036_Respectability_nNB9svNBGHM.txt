This is a video I should probably have made a while ago, but better late than never. I've been talking about AI safety for quite a while now, and in a very public way on the Computerphile channel since about 2014. From the beginning a lot of AI safety advocacy has had a certain quality to it, which to be honest I'm sure I've contributed to a little bit. And it's true that might be the appropriate response to the problem. But if you want to warn people about something, it matters how you phrase things and how you frame things. Ideally the arguments would stand on their own merits. People should be able to look at the facts and decide for themselves. And one of the things I'm trying to do with this channel is to give people the information they need to form their own informed opinions about this kind of thing. But at the end of the day most people don't have the time to put in to get the required level of understanding, and they shouldn't have to. I mean it's not actually possible to study everything in enough detail to understand it. There's just too much to know. So we have experts. But this kind of sucks because then how do you choose your experts? It turns clear objective questions about science and facts into these messy ambiguous questions about status and respectability. And a lot of the time scientists don't want to lower themselves to playing that kind of game, so they don't play at all or play half-heartedly. But this is one of those games you can't win if you don't play, and we do need to win. There are problems with having to trust experts, but ultimately specialization is how we manage to swap sucky problems like we keep being eaten by predators for really cool problems like what if our technology becomes too powerful and solves our problems too effectively? That's a good problem to have. Yeah sorry Heinlein, insects are one of the most successful groups of animals on earth. So a couple of years ago people had a go at making AI safety more respectable with this open letter, which was often reported something like Stephen Hawking, Elon Musk and Bill Gates warn about artificial intelligence, usually with a picture of the Terminator. Also surprisingly often Stephen Hawking's... Why does this keep happening? Is he secretly a team of clones? Anyway the letter itself isn't very long and basically just says AI is advancing very rapidly and having more and more impact, so we need to be thinking about ways to make sure that impact is beneficial. It says we need to do more research on this and it links to a document that gives more detail about what that research might look like. So the content of the letter itself isn't much to talk about, but the core message I think is that this stuff isn't just science fiction and it's not just futurologists who are talking about it. Real serious people are concerned. This was good for respectability with the general public because everyone's heard of these people and knows them to be respectable smart people, but I've seen people who are slightly more sophisticated noticing that none of those people are AI researchers. Professor Hawking is a physicist, Bill Gates is a software developer, but Microsoft at the time he was working there was never really an AI company. Doesn't count. And Elon Musk, though seriously overpowered, is more of a business person and an engineer, so why do these people know anything in particular about AI? But there are actually more than 8,000 signatures on this letter. Top of the list here is Stuart Russell. He's actually currently working on AI safety and I plan to make some videos about some of his work later. If you've never studied AI you may not know who he is, but he's a pretty big name. I'm not going to say he wrote the book on artificial intelligence, but I am going to imply it pretty heavily. And of course his co-author on that book, Peter Norvig, he's also on the list. What's he up to these days? Director of Research at Google. The signatories of this open letter are not AI lightweights. Who else have we got? Demis Hassabis and just about everyone at DeepMind. Jan LeCun, head of AI at Facebook. Michael Wooldridge, head of the computer science department at Oxford. I mean I'm skipping over big people, but yeah, Tom Mitchell. I know him from a little book I used as a PhD student. You know this guy's an expert because the name of his book is just the name of the subject. That's not a totally reliable heuristic though. Anyway, my point in this video is just, if you're talking to people about AI safety, it's not cheating to say, oh these high status respectable people agree with me. But if you're going to do that, pay attention to who you're talking to. If it's someone who's heard of Russell and Norvig, they're likely to find that much more convincing than Elon Musk and Stephen Hawking. And don't use me for this. I am just a guy on YouTube. I just want to thank my amazing Patreon supporters, and in particular Ichiro Dohi, who skips the queue by sponsoring me $20 a month. I don't even have a $20 a month reward level. Now I've got to make one. That's a good problem to have. Anyway, thank you so much. You may have noticed that the gap between this video and the previous one is shorter than usual, and it's largely thanks to my Patreon supporters that I'm able to do that. So thanks again, and I'll see you next time. 