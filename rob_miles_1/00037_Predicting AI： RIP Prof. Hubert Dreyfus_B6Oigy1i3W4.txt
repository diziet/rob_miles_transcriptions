I recently learned that Professor Hubert Dreyfus, the philosopher and outspoken critic of the field of artificial intelligence, has died at the age of 87. He published a lot of philosophy and did a lot of teaching, but one of the things he's best known for, and most relevant to this channel, was his work on artificial intelligence and its limits. So I'm going to talk about that a little bit today, and focus on one of his arguments in particular. So in the 50s and 60s, AI was the next big thing. Computers were tackling problems they'd never been able to approach before. People were very excited about what was possible, and artificial intelligence researchers were confident that true human-level intelligence was right around the corner. Their reasoning was something like this. Thinking consists of processing facts using logic and rules, what you might call symbol manipulation. You have some things you know, you know the relationships between them, you know the rules of logic, and you can use those to reason about the world. Computers can do this kind of symbol manipulation very well, and they're getting better all the time. So computers are able to think, and they're getting better at thinking all the time. Dreyfus saw some problems with this, and one of those problems was that a lot of human thinking doesn't seem to boil down to symbol manipulation at all. It's only one of the ways we can think. It's perhaps the most salient, because it's related to language and conscious thought, which is the part of the mind that we're most aware of. But actually, a lot of the processing seems to be happening much lower down. When you look at something, you don't think, ah yes, this has four legs and a flat top and a back, so it's a chair, and this thing has four legs and a flat top and no back, so it's a table, because I've learned the rules of what a table is and what a chair is, and now I'm applying those rules. No, you just look at the thing and you know that it's a table. The object identification is done before your conscious mind is even aware of anything, and it's not like your unconscious mind is doing this kind of rules-based thinking in the background. It's much fuzzier than that. And what would the rules be for identifying chairs anyway? I mean, do you need a back? Do you need four legs? Or, like, any legs at all? I mean, this is still a chair. Is this thing? Is this? Or this? Even something as simple as a chair is really hard to pin down. You need a lot of rules, and it would take a human a long time to evaluate them, and that isn't how the brain works. You can imagine any of our ancestors who thought, hmm, well this thing has four legs, but it has a tail, so my rules say that it's an animal. The pointy ears and sharp teeth suggest it's maybe a cat, and its size suggests perhaps a lion or a tiger. Looking at the stripes on the side, I can, like, you're dead before you're done counting the legs. So Dreyfus thought there were problems with the assumptions that AI researchers were making about the mind. He saw that the model of cognition that the AI researchers were using didn't capture the complexity of human thought, and so their reasons for thinking that computers could do the things they claimed they could do weren't very good. And he went on to publish some work, including a book called What Computers Can't Do, which argued that a lot of the things that AI researchers claimed computers would soon be able to do were actually impossible. Things like pattern recognition, natural language, vision, complex games, and so on. These things didn't just boil down to symbol manipulation, so computers couldn't do them. So how did the AI researchers react to this philosopher coming along and telling them that they were foolishly attempting the impossible? Well, they did the obvious thing, which is to ignore him and to say unkind things about him. There's a wonderful paper called The Artificial Intelligence of Hubert L. Dreyfus, which I'll link to in the doobly-doo, check it out. Back in the day, before the internet, you know, people had to do their flame wars on typewriters. It was a different time. So having dismissed him completely, they carried on trying to apply the good old-fashioned AI techniques to all of these problems for decades, until they finally had to admit that, yeah, it wasn't going to work for a lot of these problems. So about some things, at least, Dreyfus was right from the start. But the interesting thing is that since then, new techniques have been developed, which have started giving pretty good results in things like pattern recognition, language translation, high-complexity games, and so on. The kind of things Dreyfus said computers flatly couldn't do. People say that we moved away from this good old-fashioned AI approach, which I don't think is really true. We didn't stop using those techniques, at least on the problems that they work well on. We just stopped calling it AI. But the point is, the new techniques were what you might call sub-symbolic. You could make a neural network and train it to recognize tables and chairs and tigers. You can look through the source code of that system, and you won't find anywhere a single symbol which means legs or ears or teeth or anything like that. It doesn't work by symbols in the way that the logic and rules-based approaches of the 60s do. So Dreyfus was right that the AI techniques of the time weren't able to tackle these problems. But the thing he didn't expect to happen was computers being able to use symbols to implement these non-symbolic systems that can tackle the problems. So to oversimplify, the AI researchers said thinking is just symbol manipulation. Computers can do symbol manipulation, therefore computers can think. And Dreyfus said thinking is not just symbol manipulation. Computers can only do symbol manipulation, therefore computers can't think. I don't think either of those is really right. One of them underestimates what the human mind can do. The other underestimates what computers can do. I think what I take away from all of this is you can come up with a simple model that seems to explain all the important aspects of some complex system. And then it's very easy to convince yourself that that model fully covers all of the complexities and capabilities of that system. But you have to be open to the possibility that you're missing something important and that things are more complex than they seem. Now, you might say, well, both sides of this disagreement made a similar kind of mistake and they're both wrong. I don't see it that way at all, though. I mean, someone who says the Earth is flat is wrong. Someone who says it's a sphere is wrong as well. It's an oblate spheroid. It's bigger around the equator. But then it's not really an oblate spheroid either. They're perfectly smooth, which the Earth is not. So you could say that all of those views are wrong, but some of them are clearly more wrong than others. So at the end of the day, can we make computers do any kind of thinking that humans can do? Some people think that now that we have all these new approaches, we have deep learning and so on, and we're able to start doing the kind of non-symbolic thinking that Dreyfus pointed out was necessary, we can add that on to the rules and logic stuff, and then we're nearly done, and we're going to ride this current wave of breakthroughs all the way up to true general intelligence. And maybe we will. But maybe we won't. Maybe there's some third thing that we also need, and it's going to take us several decades to figure out how to get computers to do that as well. Maybe there's a fourth thing, or a fifth thing. But I don't think there's a hundredth thing. I don't even think there's a tenth thing. I think we'll get there sooner or later. But I've been wrong before. So here's to Hubert Dreyfus, a great thinker and a man well ahead of his time. Was he right overall? Probably too soon to tell. But I think he's truly deserving of our admiration and respect for being, loudly and publicly, less wrong than those around him, which is probably the best any of us can hope for. Thank you. Thank you again, and I will see you next time. 