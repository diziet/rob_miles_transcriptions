Okay, so in some of the earlier Computerphile videos, I talk about utility functions, or objective functions. And we got a lot of different comments relating to that idea. One thing people said was, well surely this kind of monomaniacal following of a single utility function at the cost of everything else is really the cause of the problem in the first place. Why even use a utility function? Or maybe have several conflicting ones that interact with each other? Or something like that. Some people asked, why do we assume that an AI will have a utility function in the first place? Aren't we making a pretty strong assumption about the design of the AI, when in fact we don't know how it would be implemented? Humans don't have explicit utility functions that they consult when they're making their decisions. A lot of different AI designs people are working on now don't have utility functions coded into them explicitly. So why make that kind of unwarranted assumption? So before we get into that, let's just go over what a utility function is. Okay, so here's the Earth, or the universe. It can be in any one of several different states. So let's just look at three possible world states. In this world, I'm enjoying a pleasant cup of tea. In this world, I've run out of milk, so the tea isn't quite how I'd like it to be. And in this world, I'm being stung by millions of wasps. We want some way of expressing that I have preferences over these world states. Some of them are better for me than others. So a utility function is a function which takes as an argument a world state and outputs a number, saying, broadly speaking, how good that world is for me, how much utility I get from it. So in this example, perhaps a nice cup of tea is worth 10, a mediocre cup of tea is worth 9, and the wasps are minus 1,000. But Rob, you might say, that's way too simple. I care about all kinds of things, and what I love about the world is complex and nuanced. You can't distill everything down to just a single number on each world state. Not with that attitude. You can, and you kind of have to. But let's just forget about the numbers for now and talk about preferences. Let's make some basic assumptions about your preferences. The first one is that you do have preferences. Given any two states of the world, you could decide which one you would prefer to happen. Or you could be indifferent. But there's this basic trilemma here. For any pair of world states, A and B, either A is preferable to B, or B is preferable to A, or you're indifferent between A and B. It doesn't matter to you which one happens. Always exactly one of these things is true. Hopefully that should be obvious, but just think about what it would mean for it not to be true. Like, what would it mean to not prefer A to B, not prefer B to A, and also not be indifferent between B and A? Similarly, what would it mean to prefer A to B and simultaneously prefer B to A? If you're faced with a choice, then, between A and B, what do you do? The second basic assumption is transitivity. So you have this relation between states, is preferable to, and you assume that this is transitive, which just means that if you prefer A to B, and you prefer B to C, then you prefer A to C. Again, this seems intuitively pretty obvious. But let's look at what it would mean to have intransitive preferences. Let's say I prefer being in Amsterdam to being in Beijing, and I prefer being in Beijing to being in Cairo, and I prefer being in Cairo to being in Amsterdam. What happens if I have these preferences? Let's say I start out in Amsterdam. I prefer being in Cairo, so I get on a plane and I fly to Cairo. Now I'm in Cairo, and I find, actually, I prefer being in Beijing. So I get on a plane, I fly to Beijing. I'm now in Beijing, and I say, oh, you know, actually, I prefer to be in Amsterdam. So I fly to Amsterdam. And now I'm back where I started. And, hey, what do you know, I prefer to be in Cairo. So you can see that if your preferences aren't transitive, you can get sort of stuck in a loop where you just expend all of your resources flying between cities or in some other way changing between options. And this doesn't seem very smart. So if we accept these two pretty basic assumptions about your preferences, then we can say that your preferences are coherent. You may have noticed there's something else that has these two properties, which is the greater than relation on numbers. For any two numbers, A and B, either A is greater than B, B is greater than A, or A and B are equal. And if A is greater than B, and B is greater than C, then A is greater than C. The fact that preferences and numbers share these properties is relevant here. So if your preferences are coherent, they'll define an order over world states. That is to say, given your preferences, you could take every possible world state and arrange them in order of how good they are for you. There will be a single ordering over world states. You know there aren't any loops because your preferences are transitive. Now if you have an ordering of world states, there will exist a set of numbers for each world state that correspond to that ordering. Perhaps you could just take them all in order and give each one a number according to where it falls in the ordering. So those are your utility values. For any coherent preferences, there will be a set of utility values that exactly represents it. And if you have a utility value on every world state, well, there will be some function which takes in world states and returns their utility values. And that's your utility function. So if you have consistent preferences, you have a utility function. But Rob, you may say, I don't have consistent preferences. I'm a human being. My preferences are all over the place. That's true. Human beings do not reliably behave as though they have consistent preferences. But that's just because human intelligence is kind of badly implemented. Our inconsistencies don't make us better people. It's not some magic key to our humanity or secret to our effectiveness or whatever. It's not making us smarter or more empathetic or more ethical. It's just making us make bad decisions. Talking about utility functions is actually a way of assuming very little about the design of an AI other than assuming that it has coherent goal-directed behavior. It doesn't matter how it's implemented. If it's effective at navigating the world to get what it wants, it will behave as though it has a particular utility function. And this means if you're going to build an agent with coherent goal-directed behavior, you'd better make sure it has the right utility function. I just wanted to say thank you to my Patreon supporters, the three people who somehow managed to support me before I even mentioned in a video that I was setting up a Patreon. And I especially want to thank Chad Jones, who's pledged $10 a month. Thank you so much. It really means a lot to me that there are people out there who think what I'm doing is worth supporting. So thanks again. 