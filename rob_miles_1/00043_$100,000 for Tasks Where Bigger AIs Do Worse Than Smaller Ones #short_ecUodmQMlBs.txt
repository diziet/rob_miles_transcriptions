In AI, bigger is better, right? Large models outperform smaller ones according to scaling laws. Except not always. Sometimes bigger models can actually perform worse, and sometimes in ways that might be dangerous. So since we're going to be building really very big models very soon, we need to understand what's going on here. So the inverse scaling prize is offering $100,000 for new examples of situations where bigger models do worse. I talked about one of these in an earlier video. If you start with bad code, sometimes large code generation models, like GitHub Copilot, will deliberately introduce bugs or vulnerabilities into their output in situations where smaller models will generate the correct secure code. This is because of misalignment. You want code that's good, but the AI wants code that's likely to come next. That's just one example, but the hope is by looking at lots of them, we could come up with more general methods for detecting and dealing with this kind of misalignment. To apply, first find an example where you think inverse scaling applies, then find a data set of at least 300 examples and test it using the models in the Google Colab that can be found at inversescaling.com, where there's all the instructions that you'll need. The deadline is October 27th, and the total prize pool is $250,000. Good luck! 