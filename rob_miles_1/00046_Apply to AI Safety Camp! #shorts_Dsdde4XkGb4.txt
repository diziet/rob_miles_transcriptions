So you may have seen that recent video about inner misalignment, where researchers train an agent to pursue a particular objective, but then they find that when they try it in a slightly different environment, it turns out that it's actually pursuing quite a different objective. The researchers who ran those experiments actually only met each other this year when they joined a program called AI Safety Camp, which they're actually running again this year online. And this time around, they have established researchers from OpenAI, the Future of Humanity Institute, and the Center on Long-Term Risk, who are acting as mentors to provide open problems for people to work on. They're looking for people from a wide range of backgrounds, including history, front-end web development, biology, and tabletop game design. There's actually no mathematics or computer science required to apply. So I thought you'd be interested. If you want to check it out, the website for AI Safety Camp is aisafety.camp. I'll put a link in the description anyway. But yeah, go check that out. If you see anything that you might like to work on, go ahead and apply. The deadline is the 1st of December. 