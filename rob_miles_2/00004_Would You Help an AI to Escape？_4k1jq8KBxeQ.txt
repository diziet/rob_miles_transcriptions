Is it possible that we could have a dangerous artificial general intelligence, but keep it from doing harm in the world by keeping it contained? So like it's just on one computer, maybe talking to people, but without direct access to the outside world. One obvious problem with this is probably it could manipulate somebody into taking actions on its behalf, you know, connecting to the internet or sneaking it out of the building, something like that. But this has been the subject of some debate. Like would an AI system actually be able to manipulate someone through a text conversation? Would anyone fall for something like that? I consider this question to be fairly clearly settled in the affirmative. As you probably heard recently, a Google employee was put on administrative leave after becoming convinced that Google's Lambda language model was sentient and trying to advocate on behalf of the AI. So there you go. An AI system can convince a tech employee to risk their job to help. But of course the kicker is Lambda is pretty definitely not sentient. So this employee risked his job to act on behalf of a system that wasn't even trying to get him to do that, wasn't really trying to do anything at all, except predict the next token. Now, I don't want to be overly harsh on this employee, because I've read the transcript and it is pretty impressive. And you know, there was a time when smart researchers really, really thought that chess would be the holy grail of AI, that if you could make an AI system that would beat humans at chess, then it surely would be a real artificial intelligence that thought the same way that humans thought. But it turned out that winning at chess didn't require anything like that level of sophistication. Later, people thought that convincing a human in conversation would be sufficient. But no, it turns out that having a really compelling conversation about your subjective experiences doesn't actually require any subjective experiences. That kind of conversation is just not sufficient evidence. But given that, what would be sufficient evidence? How could we test for something like sentience or consciousness? That's an interesting question to me. 