1
00:00:00,000 --> 00:00:07,440
So there's a thing that I see people routinely fail to get and it's a little hard to express.

2
00:00:09,120 --> 00:00:09,920
I'm just going to try.

3
00:00:12,320 --> 00:00:19,760
There's no rule which says that we make it. There's no rule which says that humanity has

4
00:00:19,760 --> 00:00:24,000
to survive this century or even this decade. There's no rule that says that everything's

5
00:00:24,000 --> 00:00:27,680
going to be okay. And when I say this to people, they generally accept it.

6
00:00:28,240 --> 00:00:31,680
Most of the people I talk to, anyway. They say, yeah, of course. Of course,

7
00:00:31,680 --> 00:00:33,920
there's no rule that says that we have to make it. Of course,

8
00:00:33,920 --> 00:00:37,280
humanity could fail. We could go extinct. You know, it could happen.

9
00:00:39,920 --> 00:00:42,880
It's like they know it, but they don't feel it.

10
00:00:44,400 --> 00:00:47,680
They don't take it seriously as an actual possibility. They kind of think,

11
00:00:48,400 --> 00:00:52,480
okay, we have these things that we have to do. We have these challenges we have to overcome.

12
00:00:53,280 --> 00:00:59,600
And so we'll do that. You know, we'll rise to the occasion. We'll figure out what we need to do

13
00:00:59,600 --> 00:01:06,240
and we'll do it. But, okay, this is a related concept. There's no rule which says that the

14
00:01:06,240 --> 00:01:13,120
challenges we're faced with are challenges that we are capable of meeting. Think about something

15
00:01:13,120 --> 00:01:18,000
like an asteroid strike. If a big enough asteroid hits Earth, we're pretty much done for, right?

16
00:01:18,000 --> 00:01:20,800
And I'm not saying that this is something we should day-to-day be concerned about because

17
00:01:21,680 --> 00:01:25,520
we're dealing with geological periods of time, right? Regularly asteroids hit Earth.

18
00:01:26,720 --> 00:01:29,920
We're sort of due for one. But what does that really mean?

19
00:01:31,040 --> 00:01:37,360
Sometime in the next million years or whatever it is. So we have more pressing concerns. But

20
00:01:39,200 --> 00:01:43,920
when people are prepared to take seriously the possibility of an asteroid strike,

21
00:01:44,640 --> 00:01:49,120
usually it's because they're thinking about it like, oh, interesting. Okay, so we have good

22
00:01:49,520 --> 00:01:54,720
telescopes. We would spot the thing. I wonder how long we would have. Would that be enough warning?

23
00:01:54,720 --> 00:01:58,800
Would that be enough time for us to build whatever it is that we needed to build to

24
00:01:58,800 --> 00:02:03,440
go up there and redirect it or blow it up or whatever? So they're willing to think about

25
00:02:03,440 --> 00:02:11,440
the possibility from the perspective of tackling the challenge. But the thing is, as I said,

26
00:02:11,440 --> 00:02:17,120
the periods of time that pass between asteroid strikes are so large that human-scale time

27
00:02:17,120 --> 00:02:21,040
periods don't really matter. The chance of an asteroid strike 200 years from now is more or

28
00:02:21,040 --> 00:02:25,840
less the same as it is now. And similarly, the chance of an asteroid strike 200 years ago is

29
00:02:25,840 --> 00:02:35,120
about the same as it is now. And if you go back a few hundred years, the question becomes very

30
00:02:35,120 --> 00:02:44,400
different. Could we see the asteroid coming? Maybe. Could we do anything about it? No. I mean,

31
00:02:44,400 --> 00:02:49,120
pretty much just flatly no. We didn't have the technology. We didn't have the understanding of

32
00:02:49,120 --> 00:02:55,680
the thing necessary to build something that could deal with that. If an asteroid were headed for

33
00:02:55,680 --> 00:03:02,880
Earth a few hundred years ago, that would just be it. Just game over. Humanity's done. Shame.

34
00:03:03,440 --> 00:03:07,280
You know, you didn't quite develop your technology fast enough to have anything that could

35
00:03:07,280 --> 00:03:13,280
deal with asteroids before one happened to hit you. Oh, well. There is no rule

36
00:03:14,720 --> 00:03:19,840
which says that the challenges the universe faces us with are challenges we can face.

37
00:03:21,920 --> 00:03:26,480
And I hope that kind of gives some taste of what I'm talking about, of like understanding

38
00:03:26,480 --> 00:03:33,040
the possibility that we could be currently faced with challenges that we will not rise to.

39
00:03:34,000 --> 00:03:40,240
That we actually could fail. But people just have a very hard time taking seriously the

40
00:03:40,240 --> 00:03:44,320
possibility that we could fail. I think it comes from a few different places.

41
00:03:45,680 --> 00:03:51,520
One place, obviously, is religion, right? If we were created deliberately by the creator of the

42
00:03:51,520 --> 00:03:58,240
universe, we are important in the universe. And so we're not going to be wiped out by some asteroid.

43
00:03:59,200 --> 00:04:01,840
Or whatever. We're not going to be wiped out in some boring way.

44
00:04:04,480 --> 00:04:10,720
Religions, they're often okay with the idea of an apocalypse, of humanity or of the world ending.

45
00:04:12,560 --> 00:04:15,920
But they're not okay with an accidental apocalypse. They have a very hard time

46
00:04:15,920 --> 00:04:23,040
taking that kind of thing seriously. In a religious apocalypse, it's always part of the plan.

47
00:04:23,840 --> 00:04:28,240
And so someone with a religious mindset, when faced with a scenario in which

48
00:04:28,800 --> 00:04:34,240
humanity is wiped out in a way which is not part of the plan, they have a very hard time

49
00:04:34,240 --> 00:04:43,520
taking that seriously. And I would posit that anyone raised in a society that's primarily

50
00:04:43,520 --> 00:04:48,000
religious probably has a lot of this as well. A lot of this kind of feeling that

51
00:04:48,400 --> 00:04:54,240
maybe we face very difficult challenges, but we'll deal with them. And if we don't,

52
00:04:54,240 --> 00:05:00,720
then that's okay. Because it's all part of the plan. Even if they would not explicitly endorse

53
00:05:00,720 --> 00:05:07,680
this idea, even if they claim to recognize the reality that there is no plan, I think those

54
00:05:07,680 --> 00:05:13,040
lingering ideas don't just immediately disappear when you give up on the supernatural parts of your

55
00:05:13,040 --> 00:05:17,680
religious beliefs and your upbringing. And it's not just capital R religion either, right?

56
00:05:17,680 --> 00:05:24,800
Like anyone who believes that human beings have souls or that human beings are in some way special,

57
00:05:24,800 --> 00:05:31,040
like fundamentally special, that the universe itself has a separate category for human beings.

58
00:05:32,160 --> 00:05:34,880
Because again, there's this sense in which the universe on some level

59
00:05:35,440 --> 00:05:41,360
cares about us, or even just recognizes us, or knows us. And it's not just about us.

60
00:05:41,360 --> 00:05:48,800
It even just recognizes us or acknowledges us as a thing. And if you believe that, again, it's difficult to think about

61
00:05:49,360 --> 00:05:57,360
situations in which we're wiped out in some boring way, with no dramatic final test,

62
00:05:58,000 --> 00:06:06,080
you know, no heroic last stands, no judgments, no thrilling story to tell,

63
00:06:08,320 --> 00:06:09,520
and nobody to tell it to.

64
00:06:11,360 --> 00:06:16,320
So religion is one place that I think this comes from. The other place that it comes from, I think, is fiction,

65
00:06:16,320 --> 00:06:21,280
especially Hollywood, and especially science fiction. And I'm not saying that people can't

66
00:06:21,280 --> 00:06:28,080
tell the difference between fiction and reality. I'm kind of saying that. I mean, on a conscious

67
00:06:28,080 --> 00:06:32,320
level, people can tell the difference between fiction and reality. But I think on some level,

68
00:06:32,320 --> 00:06:39,360
it's like people understand that whatever this fictional thing is, it didn't happen to us.

69
00:06:40,160 --> 00:06:45,440
It didn't happen here on Earth. But it sort of happened somewhere. It happened in the world of

70
00:06:45,440 --> 00:06:53,120
the story. And that still feels real. It's kind of like, say you have two people, A and B. And

71
00:06:53,120 --> 00:06:58,880
person A says, you know, alcohol is actually a really harmful drug. I think we should ban alcohol

72
00:06:58,880 --> 00:07:06,240
in the UK. And person B might reply, well, they did try banning alcohol in the USA in the 1920s, and

73
00:07:06,880 --> 00:07:11,680
that didn't really work out at all the way they hoped it would. And person A would reply, well,

74
00:07:11,680 --> 00:07:15,120
yeah, but that was a different time. It was a different place. You know, modern Britain is

75
00:07:15,120 --> 00:07:18,960
very different from 1920s America, and we could implement it differently, and so on.

76
00:07:19,920 --> 00:07:23,760
And person B would say, yeah, I mean, maybe there are important differences. But still,

77
00:07:24,400 --> 00:07:29,040
the fact that something similar was tried and failed is evidence against the idea, right?

78
00:07:29,040 --> 00:07:33,920
That's useful information to take into consideration. And that doesn't seem like an

79
00:07:33,920 --> 00:07:39,360
unreasonable position to take. But then people would do the same thing, where somebody expresses

80
00:07:39,360 --> 00:07:45,440
something about AI. And somebody else says, yeah, but think about what happened in 2001 A Space

81
00:07:45,440 --> 00:07:52,800
Odyssey or iRobot or something. And you say to them, well, that was fiction. That didn't happen,

82
00:07:52,800 --> 00:07:57,280
right? That's a fictional example. It's different from the real world. It doesn't really make sense

83
00:07:57,920 --> 00:08:03,280
to use that. And they'll say, oh, I know. I know that it's fictional. But still,

84
00:08:03,920 --> 00:08:11,440
there's some sense in which this is like evidence, right? And no, it's not. I mean, it's

85
00:08:12,880 --> 00:08:16,320
at best, it's one dude's best guess. And usually, it's not even that. Because

86
00:08:16,880 --> 00:08:20,320
usually, the person is not trying to be as accurate as possible. They're trying to be entertaining.

87
00:08:22,480 --> 00:08:25,360
So clearly, fiction affects the way people think about this.

88
00:08:27,200 --> 00:08:31,440
It's interesting. It seems like it increases the probabilities people assign to bad things

89
00:08:31,440 --> 00:08:36,960
happening with AI. But it also decreases the probabilities they assign to really,

90
00:08:36,960 --> 00:08:43,280
really bad things happening with AI. In the sense that they have, in some part of their mind,

91
00:08:43,280 --> 00:08:49,360
a whole bunch of examples in which bad things happened. But then we were able to overcome them,

92
00:08:49,360 --> 00:08:53,920
right? And this is not just with AI. This is any existential risk. Even asteroids, for example.

93
00:08:54,720 --> 00:09:00,480
Coming back to them. Even though people often won't explicitly endorse this idea,

94
00:09:01,120 --> 00:09:06,320
I think there's a part of people which says, oh, well, I've seen the world threatened hundreds of

95
00:09:06,320 --> 00:09:14,560
times, right? And every time, it works out. There's some, there's a brave hero or a plucky

96
00:09:14,560 --> 00:09:19,200
band of misfits or something comes along. We figure out what we need to do, and then we do it.

97
00:09:19,200 --> 00:09:25,280
And I guess that's where we are, you know? And some of those people are even potentially helpful

98
00:09:25,280 --> 00:09:30,080
because they offer to join your plucky band of misfits. But they're still not able to take

99
00:09:30,080 --> 00:09:38,320
seriously the possibility that we might lose. Because they've never seen that happen, right?

100
00:09:38,320 --> 00:09:41,200
I mean, they've never seen it happen in fiction or very rarely seen it happen in fiction.

101
00:09:41,760 --> 00:09:47,200
And furthermore, they've never seen it happen in reality, obviously. That's kind of a difficult

102
00:09:47,200 --> 00:09:54,480
one because I feel like you should be able to use reality as evidence. But I mean, of course,

103
00:09:54,480 --> 00:09:59,600
we've never observed humanity being wiped out. Wouldn't be here talking about it if we had.

104
00:10:02,160 --> 00:10:08,720
And so, it's hard to get people to really feel the reality of the situation.

105
00:10:09,680 --> 00:10:15,280
Which is that we, the whole of humanity, we are up here on this tightrope.

106
00:10:17,440 --> 00:10:26,640
We've had no practice, no dry run, no rehearsal. We get one shot and there is no parachute.

107
00:10:26,640 --> 00:10:33,600
There's no safety net. There is no rule that says that we have to make it. We absolutely can fail.

108
00:10:34,480 --> 00:10:39,440
And we will if we don't successfully tackle a series of hard problems.

109
00:10:40,800 --> 00:10:45,280
And there's no rule that says that we're actually capable of tackling those problems.

110
00:10:46,560 --> 00:10:53,520
And there's no rule that says that we have enough time to do so. So we better get to work.

