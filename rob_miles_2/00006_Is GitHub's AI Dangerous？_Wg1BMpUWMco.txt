Hi. So I was trying this experiment of making really short videos, and I thought, oh, this is nice. I'll be able to put them out simultaneously on TikTok and YouTube Shorts, which some other creators have been doing. So I recorded this video, it's like two and a bit minutes long, and uploaded it to TikTok. And then I realized that YouTube Shorts only allows one minute video. You don't care. Here's the video. Hey, did you hear about this? This is GitHub Copilot. It's an AI system that writes code. You just give it an English description of what you want the code to do, and it writes it. It's pretty good. But what can this tell us about why future AI systems might be dangerous? Hi, I'm Rob Miles. I make videos about AI safety. So in the field of AI safety, we have this concept of alignment, which is about are the AI system's goals aligned with ours? Is it trying to do what we want it to do? See, when you have an AI system which is doing the wrong thing, there's kind of two reasons why that might happen. One of them is a capability problem. Maybe the system just isn't smart enough, isn't capable enough to know how to do the right thing. The other possibility is an alignment problem. That happens when the system knows perfectly well how to do the right thing, but just doesn't want to. Capability problems tend to sort of solve themselves in the course of normal AI research. You get more and better data, you get bigger models, and eventually the system becomes more capable. But alignment problems are a little bit more tricky. Just making the system more capable doesn't solve the problem. It actually kind of makes it worse. If you have a very capable system which is trying to do something that you don't want it to do, that could be really dangerous. So what would it mean for a system that writes code to be misaligned? Well, what we want is for it to write good code. So if we find a situation where it writes bad code, but where it does actually know how to write that code well, then that's an indication of misalignment. So by that definition, is GitHub Copilot misaligned? The answer to this and many other questions is yes. This recent paper had a look and found exactly that. See, Codex, the model on which Copilot is based, is not actually trying to write good code. It's trying to predict what comes next. And this means that if you've written some bad code, and then you ask the model to carry on and write the next part of the code, it will carry on with bad code. If you write buggy code, it will continue with buggy code. If you write insecure code, it will introduce security vulnerabilities. And we know it's not a capabilities problem, because if you write good code, it will complete with good code. So it doesn't know how to write good code. It's just choosing not to, because that's just not what it's trying to do. It's misaligned. And we shouldn't just expect this problem to go away with bigger models and more data. In fact, they tested this with a range of different model sizes, and they found that the larger models do this more. Because they're better at spotting more subtle patterns, they're able to better spot the difference between good code and bad code. They're able to spot the subtle bugs and notice that the code completions should also have subtle bugs. So this alignment problem is going to become increasingly central to the development of AI. The question stops being, does our AI system know how to do the right thing? And becomes, does our AI system want to do the right thing? 