Que la lumi√®re soit... C'est de l'or. Okay, hi everyone. This is going to be a really quick video. A Computerphile video just came out about language models and GPT-2 and stuff like that and people were asking about actually running the model and what that looks like and that was something that we had planned to do in the recording session for Computerphile, but we didn't get around to so that's what this video is. And I thought a fun thing to do would be to generate some fake YouTube comments like Mike did. So that's what we're gonna do. This I think is gonna be a video for my second channel because it's just a quick random little project. If you're interested in this kind of thing, random machine learning stuff and different projects, then do subscribe to this channel. Otherwise, if you're interested in my main like AI safety stuff, then you want to subscribe to my main channel. I'll put a link in the description to that. So, how are we gonna do it? Well, we've got the code. It's just on github. You can get it and then we downloaded the 345 million parameter model. This I've slightly modified the generate interactive samples file just so that instead of looping it just takes the raw tokens that you can specify it here. It just makes it slightly easier to specify a whole bunch of text. So now we need to give it something that will tell GPT-2 that what we want it to generate is YouTube comments. So basically we have to give it some YouTube comments. So let's just do that. Here's the video. We can come in here and just pull out the comments. Let's actually zoom in a bit to make this easier for everybody to read. Giant. So what we can do is first off, we've got to get rid of all of these and then we're going to do a go with the dollar sign, find all of those and then there. So here I'm just putting a symbol at the beginning. Something like that. This is one of those things where you could think that the thing to do would be to write a script or talk to the YouTube API or something like that. But by the time you're done doing that you can have just done it manually. There we are. We're done. Took a couple of minutes. So this is our sample text. So we've taken up taken away anything extraneous and we put this specific string here that lets it know this is a new comment each time. Oh, right. And then at the end we need to say now it's your turn, right? Generate from that. So let's give that to the model here. Now it's your turn. GPT-2, you generate some stuff. We're going to generate 150 tokens for samples. Let's have 10 samples using the 345 million parameter model and let's go. One minute seven seconds later. Ah, here we are. So sample one. What a wonderful one sentence speech that sums up the rest of the paper at over five minutes and five seconds long to be precise. Like that's a plausible comment. That could be a comment. This one is all question marks. This person is very, very confused by the video. Transformers, transforming a human face. This will be a video after this one. Are we going to have a video about how to use transformers on human faces? Could do. Here's a link to a PDF on the archive. Probably not a valid link. I know, but what of transforming a human face? Because no, stop trying to transform human faces. You're scaring me. Okay, here's a new comment. If you would like to use any of the models, the source of the scripts or how to implement these models, please contact me at usg.dave.mccormackstudios.co.uk. I'll help you with all the code you'll need for a proper application and we will send you access to the models. I mean like that could be a real comment. Why would I use it? I already did some work using it on some other parts. Doesn't quite work. We're about to start. Can you please get on the phone with Michael Scott? New sample. It's pretty amazing what this guy got up to. He really needs a better set of headphones as he had problems with the sound quality. So this is fun. This token is the special end of text token, which has been specially defined. So that means that here it now thinks that it's hit the end of the document and it's like starting a new document. A bill to allow concealed carrier firearms into schools has earned heavy criticism from some local groups and a Republican presidential candidate. Opponents of the Senate version of the bill say it would prohibit schools from providing guns to students. That sounds like a good bill. I don't want to get political, but I don't think schools should be providing guns to students. Yeah, so basically once it hits here, I've asked it to generate 150 tokens and it's run out of stuff to say. So it's now just starting afresh, completely unprimed and just generating some text in English. And it's apparently an article about a bill about guns. Here's a new YouTube comment. For most people in our age group, they cannot read or pronounce human. Is there some sort of AI that allows humans such as you to learn from human speech and give the correct answer without the need for words or a computer to do it for you? I'm really looking forward to reading more on the topic. I really like your research. Thanks. Are we really going to learn to think this way without a computer? The Internet has changed so drastically. It has. I'm fascinated by all of this and this video. I'm fascinated by all of this and this video. Are you working on AI models that are not based in an artificial environment and have the ability to learn the human language? I mean, like that. That's a legit comment. I am very tired about the computer files who are complaining about me being boring. A model of human cognition based on deep neural network with the following properties. It requires too much time. Think two hours for each argument. It doesn't need two hours. It was a while to sit there, but. Oh, God, I'm arguing with a machine. What is happening? This is more like a carnivorous matrix. I just looked over these two examples and I'm a bit of a carnivorous matrix freak. OK. I'm sure there are lots of people in the comments who are really fond of the attention and logic are two different things. What matters are the probabilities bit. I mean, that is. I never said that. That does sound like something I would say. Attention and logic are two different things. I find this stuff interesting. Have you read some of the comments on these papers? I am very interested. That's a standard computer file comment. This paper is very hard to summarize, but in general, the results that are presented are generally very interesting. Thank you. The question that is so annoying is why is so many people talking about this stuff on TV? It's YouTube, man. You don't have to watch. You can watch a different YouTube channel. Well, it's very sad to see because there are so many people talking about this stuff on TV. Well, it's very sad to see because I thought all people would care about being involved with computers, being able to do real tasks like typing out a speech or writing a new piece of text. It's such a strange topic that there's always an argument about the relative utility of computers versus humans. In any case, why do all these people think you have to be super stupid just to type up this thing? I have no doubt it would be fun to work out how it makes sense to get as accurate a reading as possible, given how the human brain compilates the information. I don't remember why you would want to use such a very clever model. Tough it. All right. My cat's got opinions. My cat's got opinions. Do you want to be involved, Tough? No. A proud parent here. My son knows his way around the house. This model is an interesting one. The first two ideas seem somewhat familiar, but we want to know the fir... Tough it. I'm filming. Fine. This model is an interesting one. The first two ideas seem somewhat familiar, but we want to know the first one or the second word. Sounds not so clever. This is a problem because it involves the concept of the attention is all you need, but the first sentence would look very complicated and strange to anyone who doesn't know a few words from a few books. I wonder if the algorithm had its origin in computer games. So the first few words are very simple and don't give you much to work with, and so on and so on. It doesn't make sense, but it... Not all comments make sense, so it's accurate. It's not a transhumanist project at all. Is this a documentary on the technology behind my robot friend? It does not mean. Don't ask me to be a producer or anything. There's nothing to add to this. I feel my brain is in a box, just like your brain in my box. I'm not a researcher. Sorry for asking. How many times do we need to say to you that you are funny? Rob, do you have a robot friend, please? I mean, I've had that comment before. Yeah, so that was GPT-2 generated YouTube comments on the most recent beautiful video with me in it. I hope you enjoyed it. Subscribe to this channel if you want more of this kind of thing, or to my main channel if you want actual AI safety content. Alright, thanks, bye. I guess I'm making a video in a huge hurry because the computerphile video just came out and I want to publish something at the same time, which is what happened this time. But I want to say again, thank you all so much for all of your support. It means a lot to me. 